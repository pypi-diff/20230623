# Comparing `tmp/gravity-toolkit-1.2.0.tar.gz` & `tmp/gravity-toolkit-1.2.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "gravity-toolkit-1.2.0.tar", last modified: Thu Mar  9 20:13:51 2023, max compression
+gzip compressed data, was "gravity-toolkit-1.2.1.tar", last modified: Fri Jun 23 16:33:13 2023, max compression
```

## Comparing `gravity-toolkit-1.2.0.tar` & `gravity-toolkit-1.2.1.tar`

### file list

```diff
@@ -1,119 +1,140 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.745316 gravity-toolkit-1.2.0/
--rw-r--r--   0 runner    (1001) docker     (122)     1074 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)       27 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)     6773 2023-03-09 20:13:51.745316 gravity-toolkit-1.2.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     5814 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.737316 gravity-toolkit-1.2.0/gravity_toolkit/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.737316 gravity-toolkit-1.2.0/gravity_toolkit/SLR/
--rw-r--r--   0 runner    (1001) docker     (122)    20649 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/C20.py
--rw-r--r--   0 runner    (1001) docker     (122)    11737 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/C30.py
--rw-r--r--   0 runner    (1001) docker     (122)     4932 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/C40.py
--rw-r--r--   0 runner    (1001) docker     (122)     8434 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/C50.py
--rw-r--r--   0 runner    (1001) docker     (122)    12626 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/CS2.py
--rw-r--r--   0 runner    (1001) docker     (122)       95 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/SLR/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2990 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14988 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/associated_legendre.py
--rw-r--r--   0 runner    (1001) docker     (122)    11627 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/clenshaw_summation.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.741316 gravity-toolkit-1.2.0/gravity_toolkit/data/
--rw-r--r--   0 runner    (1001) docker     (122)    62611 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/Load_Love2_CE.dat
--rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/PREM-LLNs-truncated.dat
--rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/PREMhard-LLNs-truncated.dat
--rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/PREMsoft-LLNs-truncated.dat
--rw-r--r--   0 runner    (1001) docker     (122)      130 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/land_fcn_300km.nc
--rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/landsea_1d.nc
--rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/landsea_hd.nc
--rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/landsea_qd.nc
--rw-r--r--   0 runner    (1001) docker     (122)    49421 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/data/love_numbers
--rwxr-xr-x   0 runner    (1001) docker     (122)     2119 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/degree_amplitude.py
--rw-r--r--   0 runner    (1001) docker     (122)    11314 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/destripe_harmonics.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    11089 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/fourier_legendre.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3608 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gauss_weights.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6303 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_averaging_kernel.py
--rw-r--r--   0 runner    (1001) docker     (122)    10554 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_disc_load.py
--rw-r--r--   0 runner    (1001) docker     (122)    15207 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)     7314 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_point_load.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    12803 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_spherical_cap.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     9103 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/gen_stokes.py
--rw-r--r--   0 runner    (1001) docker     (122)    49291 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/geocenter.py
--rw-r--r--   0 runner    (1001) docker     (122)    15265 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/grace_date.py
--rw-r--r--   0 runner    (1001) docker     (122)     5206 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/grace_find_months.py
--rw-r--r--   0 runner    (1001) docker     (122)    48252 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/grace_input_months.py
--rw-r--r--   0 runner    (1001) docker     (122)     9439 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/grace_months_index.py
--rw-r--r--   0 runner    (1001) docker     (122)     5523 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/harmonic_gradients.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    11705 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/harmonic_summation.py
--rw-r--r--   0 runner    (1001) docker     (122)    77386 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)     6119 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/legendre.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3822 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/legendre_polynomials.py
--rw-r--r--   0 runner    (1001) docker     (122)    13252 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/mascons.py
--rw-r--r--   0 runner    (1001) docker     (122)     5272 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/ocean_stokes.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    37908 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/read_GIA_model.py
--rw-r--r--   0 runner    (1001) docker     (122)    14769 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/read_GRACE_harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)    14811 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/read_SLR_harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)     9647 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/read_gfc_harmonics.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    23731 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/read_love_numbers.py
--rw-r--r--   0 runner    (1001) docker     (122)    24817 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/sea_level_equation.py
--rw-r--r--   0 runner    (1001) docker     (122)    75950 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/spatial.py
--rw-r--r--   0 runner    (1001) docker     (122)    28863 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.741316 gravity-toolkit-1.2.0/gravity_toolkit/time_series/
--rw-r--r--   0 runner    (1001) docker     (122)      125 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     1457 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/amplitude.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    15313 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/piecewise.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    14715 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/regress.py
--rw-r--r--   0 runner    (1001) docker     (122)     5597 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/savitzky_golay.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    15145 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/time_series/smooth.py
--rw-r--r--   0 runner    (1001) docker     (122)    44926 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/tools.py
--rw-r--r--   0 runner    (1001) docker     (122)     7226 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/units.py
--rw-r--r--   0 runner    (1001) docker     (122)    75303 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/utilities.py
--rw-r--r--   0 runner    (1001) docker     (122)      341 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/gravity_toolkit/version.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.737316 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)     6773 2023-03-09 20:13:51.000000 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     3454 2023-03-09 20:13:51.000000 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-03-09 20:13:51.000000 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)       47 2023-03-09 20:13:51.000000 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)       21 2023-03-09 20:13:51.000000 gravity-toolkit-1.2.0/gravity_toolkit.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.745316 gravity-toolkit-1.2.0/scripts/
--rw-r--r--   0 runner    (1001) docker     (122)    13008 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/aod1b_geocenter.py
--rw-r--r--   0 runner    (1001) docker     (122)    12300 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/aod1b_oblateness.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    74535 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/calc_degree_one.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     3638 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/calc_harmonic_resolution.py
--rw-r--r--   0 runner    (1001) docker     (122)    47431 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/calc_mascon.py
--rw-r--r--   0 runner    (1001) docker     (122)    27868 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/calc_sensitivity_kernel.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    15344 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/cnes_grace_sync.py
--rw-r--r--   0 runner    (1001) docker     (122)    17332 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/combine_harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)    11636 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/convert_harmonics.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    35407 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/dealiasing_monthly_mean.py
--rw-r--r--   0 runner    (1001) docker     (122)    11260 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/esa_costg_swarm_sync.py
--rw-r--r--   0 runner    (1001) docker     (122)    11225 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/geocenter_compare_tellus.py
--rw-r--r--   0 runner    (1001) docker     (122)     8547 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/geocenter_monte_carlo.py
--rw-r--r--   0 runner    (1001) docker     (122)     9021 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/geocenter_ocean_models.py
--rw-r--r--   0 runner    (1001) docker     (122)     9341 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/geocenter_processing_centers.py
--rw-r--r--   0 runner    (1001) docker     (122)    12076 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/gfz_icgem_costg_ftp.py
--rw-r--r--   0 runner    (1001) docker     (122)    10860 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/gfz_isdc_dealiasing_ftp.py
--rw-r--r--   0 runner    (1001) docker     (122)    19712 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/gfz_isdc_grace_ftp.py
--rw-r--r--   0 runner    (1001) docker     (122)    21754 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/grace_mean_harmonics.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    32923 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/grace_spatial_error.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    35901 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/grace_spatial_maps.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    11475 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/itsg_graz_grace_sync.py
--rw-r--r--   0 runner    (1001) docker     (122)     5590 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/make_grace_index.py
--rw-r--r--   0 runner    (1001) docker     (122)    18906 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/mascon_reconstruct.py
--rw-r--r--   0 runner    (1001) docker     (122)    65659 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/monte_carlo_degree_one.py
--rw-r--r--   0 runner    (1001) docker     (122)    19688 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/podaac_cumulus.py
--rw-r--r--   0 runner    (1001) docker     (122)    29248 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/podaac_grace_sync.py
--rw-r--r--   0 runner    (1001) docker     (122)     6445 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/podaac_webdav.py
--rwxr-xr-x   0 runner    (1001) docker     (122)    25513 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/regress_grace_maps.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6402 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/run_grace_date.py
--rw-r--r--   0 runner    (1001) docker     (122)    16118 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/run_sea_level_equation.py
--rw-r--r--   0 runner    (1001) docker     (122)    41331 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/scripts/scale_grace_maps.py
--rw-r--r--   0 runner    (1001) docker     (122)      288 2023-03-09 20:13:51.745316 gravity-toolkit-1.2.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:51.745316 gravity-toolkit-1.2.0/test/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      702 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/conftest.py
--rw-r--r--   0 runner    (1001) docker     (122)     6075 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_download_and_read.py
--rw-r--r--   0 runner    (1001) docker     (122)     3660 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_gia.py
--rwxr-xr-x   0 runner    (1001) docker     (122)     6150 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_harmonics.py
--rw-r--r--   0 runner    (1001) docker     (122)     1756 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_legendre.py
--rw-r--r--   0 runner    (1001) docker     (122)     2399 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_love_numbers.py
--rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_point_masses.py
--rw-r--r--   0 runner    (1001) docker     (122)     8729 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_time.py
--rw-r--r--   0 runner    (1001) docker     (122)     2596 2023-03-09 20:13:41.000000 gravity-toolkit-1.2.0/test/test_units.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.315363 gravity-toolkit-1.2.1/
+-rw-r--r--   0 runner    (1001) docker     (122)     1074 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)       27 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)     6408 2023-06-23 16:33:13.315363 gravity-toolkit-1.2.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)     5449 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.303363 gravity-toolkit-1.2.1/gravity_toolkit/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.303363 gravity-toolkit-1.2.1/gravity_toolkit/SLR/
+-rw-r--r--   0 runner    (1001) docker     (122)    20405 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/C20.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11659 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/C30.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5096 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/C40.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8585 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/C50.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12745 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/CS2.py
+-rw-r--r--   0 runner    (1001) docker     (122)       95 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/SLR/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3136 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15298 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/associated_legendre.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11676 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/clenshaw_summation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.307363 gravity-toolkit-1.2.1/gravity_toolkit/data/
+-rw-r--r--   0 runner    (1001) docker     (122)    62611 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/Load_Love2_CE.dat
+-rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/PREM-LLNs-truncated.dat
+-rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/PREMhard-LLNs-truncated.dat
+-rw-r--r--   0 runner    (1001) docker     (122)   465086 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/PREMsoft-LLNs-truncated.dat
+-rw-r--r--   0 runner    (1001) docker     (122)      130 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/land_fcn_300km.nc
+-rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/landsea_1d.nc
+-rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/landsea_hd.nc
+-rwxr-xr-x   0 runner    (1001) docker     (122)      130 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/landsea_qd.nc
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10666 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/leap-seconds.list
+-rw-r--r--   0 runner    (1001) docker     (122)    49421 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/data/love_numbers
+-rwxr-xr-x   0 runner    (1001) docker     (122)     2237 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/degree_amplitude.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11421 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/destripe_harmonics.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    11168 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/fourier_legendre.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3677 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gauss_weights.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6877 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_averaging_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11121 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_disc_load.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15323 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7386 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_point_load.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    13249 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_spherical_cap.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     9174 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/gen_stokes.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49328 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/geocenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15507 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/grace_date.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5753 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/grace_find_months.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48232 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/grace_input_months.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8662 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/grace_months_index.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11060 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/harmonic_gradients.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    11401 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/harmonic_summation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    78023 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6138 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/legendre.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3916 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/legendre_polynomials.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13468 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/mascons.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5721 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/ocean_stokes.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    36424 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/read_GIA_model.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15211 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/read_GRACE_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15029 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/read_SLR_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9932 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/read_gfc_harmonics.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    24490 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/read_love_numbers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25701 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/sea_level_equation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    80706 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/spatial.py
+-rw-r--r--   0 runner    (1001) docker     (122)    42294 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.307363 gravity-toolkit-1.2.1/gravity_toolkit/time_series/
+-rw-r--r--   0 runner    (1001) docker     (122)      144 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     1457 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/amplitude.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1645 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/fit.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15581 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/piecewise.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15023 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/regress.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5581 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/savitzky_golay.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15145 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/time_series/smooth.py
+-rw-r--r--   0 runner    (1001) docker     (122)    45686 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/tools.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11246 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/units.py
+-rw-r--r--   0 runner    (1001) docker     (122)    84855 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/utilities.py
+-rw-r--r--   0 runner    (1001) docker     (122)      341 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/gravity_toolkit/version.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.303363 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)     6408 2023-06-23 16:33:13.000000 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)     4154 2023-06-23 16:33:13.000000 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-06-23 16:33:13.000000 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       64 2023-06-23 16:33:13.000000 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       21 2023-06-23 16:33:13.000000 gravity-toolkit-1.2.1/gravity_toolkit.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.315363 gravity-toolkit-1.2.1/scripts/
+-rw-r--r--   0 runner    (1001) docker     (122)    13208 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/aod1b_geocenter.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12497 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/aod1b_oblateness.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    75023 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/calc_degree_one.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     3627 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/calc_harmonic_resolution.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48255 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/calc_mascon.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28749 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/calc_sensitivity_kernel.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    15408 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/cnes_grace_sync.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17942 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/combine_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12649 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/convert_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22133 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/dealiasing_global_uplift.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    36409 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/dealiasing_monthly_mean.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11353 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/esa_costg_swarm_sync.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11241 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/geocenter_compare_tellus.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8571 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/geocenter_monte_carlo.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9041 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/geocenter_ocean_models.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9365 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/geocenter_processing_centers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12107 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/gfz_icgem_costg_ftp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11053 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/gfz_isdc_dealiasing_ftp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19549 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/gfz_isdc_grace_ftp.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21916 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/grace_mean_harmonics.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    32582 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/grace_spatial_error.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    35358 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/grace_spatial_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    11554 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/itsg_graz_grace_sync.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5727 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/make_grace_index.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18873 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/mascon_reconstruct.py
+-rw-r--r--   0 runner    (1001) docker     (122)    66500 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/monte_carlo_degree_one.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    27957 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/piecewise_grace_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35601 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_GrIS_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30600 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_grid_3maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30748 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_grid_4maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    30336 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_grid_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    32117 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_grid_movie.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34333 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_regional_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    36136 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_AIS_regional_movie.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31602 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_GrIS_grid_3maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31226 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_GrIS_grid_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    33478 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_GrIS_grid_movie.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31283 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_QML_grid_3maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24505 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_3maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24657 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_4maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24924 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_5maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24701 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_9maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24151 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25064 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/plot_global_grid_movie.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20105 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/podaac_cumulus.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10293 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/quick_mascon_plot.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10834 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/quick_mascon_regress.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)    27483 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/regress_grace_maps.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6434 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/run_grace_date.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16288 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/run_sea_level_equation.py
+-rw-r--r--   0 runner    (1001) docker     (122)    41165 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/scripts/scale_grace_maps.py
+-rw-r--r--   0 runner    (1001) docker     (122)      288 2023-06-23 16:33:13.315363 gravity-toolkit-1.2.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (122)     2022 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:13.315363 gravity-toolkit-1.2.1/test/
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)      702 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7332 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_download_and_read.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3677 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_gia.py
+-rwxr-xr-x   0 runner    (1001) docker     (122)     6139 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_harmonics.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1756 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_legendre.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2399 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_love_numbers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1914 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_point_masses.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11848 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_time.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4724 2023-06-23 16:33:01.000000 gravity-toolkit-1.2.1/test/test_units.py
```

### Comparing `gravity-toolkit-1.2.0/LICENSE` & `gravity-toolkit-1.2.1/LICENSE`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/PKG-INFO` & `gravity-toolkit-1.2.1/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gravity-toolkit
-Version: 1.2.0
+Version: 1.2.1
 Summary: Python tools for obtaining and working with spherical harmoniccoefficients from the NASA/DLR GRACE and NASA/GFZ GRACE Follow-on missions
 Home-page: https://github.com/tsutterley/gravity-toolkit
 Author: Tyler Sutterley
 Author-email: tsutterl@uw.edu
 License: MIT
 Keywords: GRACE,GRACE-FO,Gravity,satellite geodesy,spherical harmonics
 Classifier: Development Status :: 3 - Alpha
@@ -24,36 +24,28 @@
 gravity-toolkit
 ===============
 
 |Language|
 |License|
 |PyPI Version|
 |Documentation Status|
-|Binder|
-|Pangeo|
 |zenodo|
 
 .. |Language| image:: https://img.shields.io/pypi/pyversions/gravity-toolkit?color=green
    :target: https://www.python.org/
 
 .. |License| image:: https://img.shields.io/github/license/tsutterley/gravity-toolkit
    :target: https://github.com/tsutterley/gravity-toolkit/blob/main/LICENSE
 
 .. |PyPI Version| image:: https://img.shields.io/pypi/v/gravity-toolkit.svg
    :target: https://pypi.python.org/pypi/gravity-toolkit/
 
 .. |Documentation Status| image:: https://readthedocs.org/projects/gravity-toolkit/badge/?version=latest
    :target: https://gravity-toolkit.readthedocs.io/en/latest/?badge=latest
 
-.. |Binder| image:: https://mybinder.org/badge_logo.svg
-   :target: https://mybinder.org/v2/gh/tsutterley/gravity-toolkit/main
-
-.. |Pangeo| image:: https://img.shields.io/static/v1.svg?logo=Jupyter&label=PangeoBinderAWS&message=us-west-2&color=orange
-   :target: https://aws-uswest2-binder.pangeo.io/v2/gh/tsutterley/gravity-toolkit/main?urlpath=lab
-
 .. |zenodo| image:: https://zenodo.org/badge/107323776.svg
    :target: https://zenodo.org/badge/latestdoi/107323776
 
 Python tools for obtaining and working with Level-2 spherical harmonic coefficients from the NASA/DLR Gravity Recovery and Climate Experiment (GRACE) and the NASA/GFZ Gravity Recovery and Climate Experiment Follow-On (GRACE-FO) missions
 
 Resources
 #########
@@ -82,19 +74,19 @@
 - `geoid-toolkit: Python utilities for calculating geoid heights from static gravity field coefficients <https://github.com/tsutterley/geoid-toolkit/>`_
 
 References
 ##########
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Self-Consistent Ice Mass Balance
     and Regional Sea Level From Time-Variable Gravity", *Earth and Space Science*, 7,
-    (2020). `doi:10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
+    (2020). `doi: 10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
 
     T. C. Sutterley and I. Velicogna, "Improved estimates of geocenter variability
     from time-variable gravity and ocean model outputs", *Remote Sensing*, 11(18),
-    2108, (2019). `doi:10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
+    2108, (2019). `doi: 10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
 
     J. Wahr, S. C. Swenson, and I. Velicogna, "Accuracy of GRACE mass estimates",
     *Geophysical Research Letters*, 33(6), L06401, (2006).
     `doi: 10.1029/2005GL025305 <https://doi.org/10.1029/2005GL025305>`_
 
     J. Wahr, M. Molenaar, and F. Bryan, "Time variability of the Earth's gravity
     field: Hydrological and oceanic effects and their possible detection using
@@ -107,19 +99,19 @@
     `doi: 10.1111/j.1365-246X.1995.tb01819.x <https://doi.org/10.1111/j.1365-246X.1995.tb01819.x>`_
 
 Data Repositories
 #################
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Ice Mass and Regional Sea Level
     Estimates from Time-Variable Gravity", (2020).
-    `doi:10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
+    `doi: 10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
 
     T. C. Sutterley and I. Velicogna, "Geocenter Estimates from Time-Variable
     Gravity and Ocean Model Outputs", (2019).
-    `doi:10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
+    `doi: 10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
 
 Download
 ########
 
 | The program homepage is:
 | https://github.com/tsutterley/gravity-toolkit
 | A zip archive of the latest version is available directly at:
```

### Comparing `gravity-toolkit-1.2.0/README.rst` & `gravity-toolkit-1.2.1/README.rst`

 * *Files 6% similar despite different names*

```diff
@@ -2,36 +2,28 @@
 gravity-toolkit
 ===============
 
 |Language|
 |License|
 |PyPI Version|
 |Documentation Status|
-|Binder|
-|Pangeo|
 |zenodo|
 
 .. |Language| image:: https://img.shields.io/pypi/pyversions/gravity-toolkit?color=green
    :target: https://www.python.org/
 
 .. |License| image:: https://img.shields.io/github/license/tsutterley/gravity-toolkit
    :target: https://github.com/tsutterley/gravity-toolkit/blob/main/LICENSE
 
 .. |PyPI Version| image:: https://img.shields.io/pypi/v/gravity-toolkit.svg
    :target: https://pypi.python.org/pypi/gravity-toolkit/
 
 .. |Documentation Status| image:: https://readthedocs.org/projects/gravity-toolkit/badge/?version=latest
    :target: https://gravity-toolkit.readthedocs.io/en/latest/?badge=latest
 
-.. |Binder| image:: https://mybinder.org/badge_logo.svg
-   :target: https://mybinder.org/v2/gh/tsutterley/gravity-toolkit/main
-
-.. |Pangeo| image:: https://img.shields.io/static/v1.svg?logo=Jupyter&label=PangeoBinderAWS&message=us-west-2&color=orange
-   :target: https://aws-uswest2-binder.pangeo.io/v2/gh/tsutterley/gravity-toolkit/main?urlpath=lab
-
 .. |zenodo| image:: https://zenodo.org/badge/107323776.svg
    :target: https://zenodo.org/badge/latestdoi/107323776
 
 Python tools for obtaining and working with Level-2 spherical harmonic coefficients from the NASA/DLR Gravity Recovery and Climate Experiment (GRACE) and the NASA/GFZ Gravity Recovery and Climate Experiment Follow-On (GRACE-FO) missions
 
 Resources
 #########
@@ -60,19 +52,19 @@
 - `geoid-toolkit: Python utilities for calculating geoid heights from static gravity field coefficients <https://github.com/tsutterley/geoid-toolkit/>`_
 
 References
 ##########
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Self-Consistent Ice Mass Balance
     and Regional Sea Level From Time-Variable Gravity", *Earth and Space Science*, 7,
-    (2020). `doi:10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
+    (2020). `doi: 10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
 
     T. C. Sutterley and I. Velicogna, "Improved estimates of geocenter variability
     from time-variable gravity and ocean model outputs", *Remote Sensing*, 11(18),
-    2108, (2019). `doi:10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
+    2108, (2019). `doi: 10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
 
     J. Wahr, S. C. Swenson, and I. Velicogna, "Accuracy of GRACE mass estimates",
     *Geophysical Research Letters*, 33(6), L06401, (2006).
     `doi: 10.1029/2005GL025305 <https://doi.org/10.1029/2005GL025305>`_
 
     J. Wahr, M. Molenaar, and F. Bryan, "Time variability of the Earth's gravity
     field: Hydrological and oceanic effects and their possible detection using
@@ -85,19 +77,19 @@
     `doi: 10.1111/j.1365-246X.1995.tb01819.x <https://doi.org/10.1111/j.1365-246X.1995.tb01819.x>`_
 
 Data Repositories
 #################
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Ice Mass and Regional Sea Level
     Estimates from Time-Variable Gravity", (2020).
-    `doi:10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
+    `doi: 10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
 
     T. C. Sutterley and I. Velicogna, "Geocenter Estimates from Time-Variable
     Gravity and Ocean Model Outputs", (2019).
-    `doi:10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
+    `doi: 10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
 
 Download
 ########
 
 | The program homepage is:
 | https://github.com/tsutterley/gravity-toolkit
 | A zip archive of the latest version is available directly at:
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/SLR/C20.py` & `gravity-toolkit-1.2.1/gravity_toolkit/SLR/C20.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,27 +1,24 @@
 #!/usr/bin/env python
 u"""
 C20.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (05/2023)
 
 Reads in C20 spherical harmonic coefficients derived from SLR measurements
 
 Dataset distributed by NASA PO.DAAC
-    https://podaac-tools.jpl.nasa.gov/drive/files/GeodeticsGravity/grace/docs
-        TN-05_C20_SLR.txt
-        TN-07_C20_SLR.txt
-        TN-11_C20_SLR.txt
-        TN-14_C30_C30_GSFC_SLR.txt
+    TN-05_C20_SLR.txt
+    TN-07_C20_SLR.txt
+    TN-11_C20_SLR.txt
+    TN-14_C30_C30_GSFC_SLR.txt
 Dataset distributed by UTCSR
-    ftp://ftp.csr.utexas.edu/pub/slr/degree_2/C20_RL05.txt
+    C20_RL05.txt
 Datasets distributed by GFZ
-    ftp://isdcftp.gfz-potsdam.de/grace/Level-2/GFZ/RL06_SLR_C20/
-        GFZ_RL06_C20_SLR.dat
-    ftp://isdcftp.gfz-potsdam.de/grace/GravIS/GFZ/Level-2B/aux_data/
-        GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat
+    GFZ_RL06_C20_SLR.dat
+    GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat
 
 CALLING SEQUENCE:
     SLR_C20 = gravity_toolkit.SLR.C20(SLR_file)
 
 INPUTS:
     SLR_file:
         RL04: TN-05_C20_SLR.txt
@@ -61,14 +58,15 @@
         https://doi.org/10.1029/2019GL082929
     Koenig, R., Schreiner, P, and Dahle, C. "Monthly estimates of C(2,0)
         generated by GFZ from SLR satellites based on GFZ GRACE/GRACE-FO
         RL06 background models." V. 1.0. GFZ Data Services, (2019).
         https://doi.org/10.5880/GFZ.GRAVIS_06_C20_SLR
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 01/2023: refactored satellite laser ranging read functions
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 09/2021: use functions for converting to and from GRACE months
     Updated 05/2021: added GFZ SLR and GravIS oblateness solutions
         define int/float precision to prevent deprecation warning
     Updated 02/2021: use adjust_months function to fix special months cases
@@ -103,16 +101,16 @@
         For these months the SLR file has a second dataline for the modified period
         Will use these marked (*) data to replace the GRACE C2,0
         ALSO converted the mon and slrdate inputs into options
     Updated 01/2012: Updated to feed in SLR file from outside
         Will accommodate upcoming GRACE RL05, which will use different SLR files
     Written 12/2011
 """
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 
 # PURPOSE: read oblateness data from Satellite Laser Ranging (SLR)
 def C20(SLR_file, AOD=True, HEADER=True):
     """
     Reads *C*\ :sub:`20` spherical harmonic coefficients from SLR measurements
@@ -136,57 +134,58 @@
     month: int
         GRACE/GRACE-FO month of measurement
     date: float
         date of SLR measurement
     """
 
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
-        raise FileNotFoundError('SLR file not found in file system')
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
+        raise FileNotFoundError(f'{str(SLR_file)} not found in file system')
 
     # output dictionary with data variables
     dinput = {}
     # determine if imported file is from PO.DAAC or CSR
-    if bool(re.search(r'C20_RL\d+',SLR_file,re.I)):
+    if bool(re.search(r'C20_RL\d+', SLR_file.name ,re.I)):
         # SLR C20 file from CSR
         # Just for checking new months when TN series isn't up to date as the
         # SLR estimates always use the full set of days in each calendar month.
         # format of the input file (note 64 bit floating point for C20)
         # Column 1: Approximate mid-point of monthly solution (years)
         # Column 2: C20 from SLR (normalized)
         # Column 3: Delta C20 relative to a mean value (1E-10)
         # Column 4: Solution sigma (1E-10)
         # Column 5: Mean value of Atmosphere-Ocean De-aliasing model (1E-10)
         # Columns 6-7: Start and end dates of data used in solution
         dtype = {}
         dtype['names'] = ('time','C20','delta','sigma','AOD','start','end')
         dtype['formats'] = ('f','f8','f','f','f','f','f')
         # header text is commented and won't be read
-        file_input = np.loadtxt(os.path.expanduser(SLR_file),dtype=dtype)
+        file_input = np.loadtxt(SLR_file, dtype=dtype)
         # date and GRACE/GRACE-FO month
         dinput['time'] = file_input['time']
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
         # monthly spherical harmonic replacement solutions
         dinput['data'] = file_input['C20'].copy()
         # monthly spherical harmonic formal standard deviations
         dinput['error'] = file_input['sigma']*1e-10
         # Background gravity model includes solid earth and ocean tides, solid
         # earth and ocean pole tides, and the Atmosphere-Ocean De-aliasing
         # product. The monthly mean of the AOD model has been restored.
         if AOD:
             # Removing AOD product that was restored in the solution
             dinput['data'] -= file_input['AOD']*1e-10
-    elif bool(re.search(r'GFZ_(RL\d+)_C20_SLR',SLR_file,re.I)):
+    elif bool(re.search(r'GFZ_(RL\d+)_C20_SLR', SLR_file.name, re.I)):
         # SLR C20 file from GFZ
         # Column 1: MJD of BEGINNING of solution span
         # Column 2: Year and fraction of year of BEGINNING of solution span
         # Column 3: Replacement C(2,0)
         # Column 4: Replacement C(2,0) - mean C(2,0) (1.0E-10)
         # Column 5: C(2,0) formal error (1.0E-10)
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
         # counts the number of lines in the header
         count = 0
         # Reading over header text
         while HEADER:
@@ -225,22 +224,22 @@
                     dinput['time'][t], around=np.round)
                 # add to t count
                 t += 1
         # truncate variables if necessary
         for key,val in dinput.items():
             dinput[key] = val[:t]
 
-    elif bool(re.search(r'GRAVIS-2B_GFZOP',SLR_file,re.I)):
+    elif bool(re.search(r'GRAVIS-2B_GFZOP', SLR_file.name, re.I)):
         # Combined GRACE/SLR solution file produced by GFZ
         # Column  1: MJD of BEGINNING of solution data span
         # Column  2: Year and fraction of year of BEGINNING of solution span
         # Column  3: Replacement C(2,0)
         # Column  4: Replacement C(2,0) - mean C(2,0) (1.0E-10)
         # Column  5: C(2,0) formal standard deviation (1.0E-12)
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -252,15 +251,15 @@
             # add 1 to counter
             count += 1
 
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['data'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['error'] = np.zeros((n_mon))
         # time count
         t = 0
         # for every other line:
@@ -281,17 +280,17 @@
                     dinput['time'][t], around=np.round)
                 # add to t count
                 t += 1
         # truncate variables if necessary
         for key,val in dinput.items():
             dinput[key] = val[:t]
 
-    elif bool(re.search(r'TN-(11|14)',SLR_file,re.I)):
+    elif bool(re.search(r'TN-(11|14)', SLR_file.name, re.I)):
         # SLR C20 RL06 file from PO.DAAC
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -342,15 +341,15 @@
                 # add to t count
                 t += 1
         # truncate variables if necessary
         for key,val in dinput.items():
             dinput[key] = val[:t]
     else:
         # SLR C20 file from PO.DAAC
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/SLR/C30.py` & `gravity-toolkit-1.2.1/gravity_toolkit/SLR/C30.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,19 @@
 #!/usr/bin/env python
 u"""
 C30.py
-Written by Yara Mohajerani and Tyler Sutterley (01/2023)
+Written by Yara Mohajerani and Tyler Sutterley (05/2023)
 
 Reads monthly degree 3 zonal spherical harmonic data files from SLR
 
 Dataset distributed by NASA PO.DAAC
-    https://podaac-tools.jpl.nasa.gov/drive/files/GeodeticsGravity/gracefo/docs
-        TN-14_C30_C30_GSFC_SLR.txt
-    ftp://ftp.csr.utexas.edu/pub/slr/degree_5/
-        CSR_Monthly_5x5_Gravity_Harmonics.txt
+    TN-14_C30_C30_GSFC_SLR.txt
+    CSR_Monthly_5x5_Gravity_Harmonics.txt
 Dataset distributed by GFZ
-    ftp://isdcftp.gfz-potsdam.de/grace/GravIS/GFZ/Level-2B/aux_data/
-        GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat
+    GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat
 
 CALLING SEQUENCE:
     SLR_C30 = gravity_toolkit.SLR.C30(SLR_file)
 
 INPUTS:
     SLR_file:
         CSR: CSR_Monthly_5x5_Gravity_Harmonics.txt
@@ -56,14 +53,15 @@
         https://doi.org/10.1029/2019GL085488
     Dahle and Murboeck, "Post-processed GRACE/GRACE-FO Geopotential
         GSM Coefficients GFZ RL06 (Level-2B Product)."
         V. 0002. GFZ Data Services, (2019).
         https://doi.org/10.5880/GFZ.GRAVIS_06_L2B
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 01/2023: refactored satellite laser ranging read functions
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 09/2021: use functions for converting to and from GRACE months
     Updated 05/2021: added GFZ GravIS GRACE/SLR low degree solutions
         define int/float precision to prevent deprecation warning
     Updated 04/2021: renamed SLR monthly 5x5 function from CSR
@@ -75,16 +73,16 @@
         add catch to verify input SLR file exists
         added LARES filtered C30 files from John Ries (C30_LARES_filtered.txt)
         add C30 mean (9.5717395773300e-07) to LARES solutions
     Updated 07/2019: added SLR C3,0 files from PO.DAAC (GSFC)
         read CSR monthly 5x5 file and extract C3,0 coefficients
     Written 05/2019
 """
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 import gravity_toolkit.read_SLR_harmonics
 
 # PURPOSE: read Degree 3 zonal data from Satellite Laser Ranging (SLR)
 def C30(SLR_file, C30_MEAN=9.5717395773300e-07, HEADER=True):
     """
@@ -108,23 +106,25 @@
     month: int
         GRACE/GRACE-FO month of measurement
     time: float
         date of SLR measurement
     """
 
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
-        raise FileNotFoundError('SLR file not found in file system')
-    # output dictionary with input data
-    dinput = {}
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
+        raise FileNotFoundError(f'{str(SLR_file)} not found in file system')
 
-    if bool(re.search(r'TN-(14)',SLR_file,re.I)):
+    # output dictionary with data variables
+    dinput = {}
+    # determine source of input file
+    if bool(re.search(r'TN-(14)', SLR_file.name, re.I)):
 
         # SLR C30 RL06 file from PO.DAAC produced by GSFC
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -136,15 +136,15 @@
             # add 1 to counter
             count += 1
 
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['data'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['error'] = np.zeros((n_mon))
         # time count
         t = 0
         # for every other line:
@@ -174,32 +174,32 @@
         # verify that there imported C30 solutions
         # (TN-14 data format has changed in the past)
         if (t == 0):
             raise Exception('No GSFC C30 data imported')
         # truncate variables if necessary
         for key,val in dinput.items():
             dinput[key] = val[:t]
-    elif bool(re.search(r'C30_LARES',SLR_file,re.I)):
+    elif bool(re.search(r'C30_LARES', SLR_file.name, re.I)):
         # read LARES filtered values
         LARES_input = np.loadtxt(SLR_file,skiprows=1)
         dinput['time'] = LARES_input[:,0].copy()
         # convert C30 from anomalies to absolute
         dinput['data'] = 1e-10*LARES_input[:,1] + C30_MEAN
         # filtered data does not have errors
         dinput['error'] = np.zeros_like(LARES_input[:,1])
         # calculate GRACE/GRACE-FO month
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
-    elif bool(re.search(r'GRAVIS-2B_GFZOP',SLR_file,re.I)):
+    elif bool(re.search(r'GRAVIS-2B_GFZOP', SLR_file.name, re.I)):
         # Combined GRACE/SLR solution file produced by GFZ
         # Column  1: MJD of BEGINNING of solution data span
         # Column  2: Year and fraction of year of BEGINNING of solution span
         # Column  6: Replacement C(3,0)
         # Column  7: Replacement C(3,0) - mean C(3,0) (1.0E-10)
         # Column  8: C(3,0) formal standard deviation (1.0E-12)
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -211,15 +211,15 @@
             # add 1 to counter
             count += 1
 
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['data'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['error'] = np.zeros((n_mon))
         # time count
         t = 0
         # for every other line:
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/SLR/C40.py` & `gravity-toolkit-1.2.1/gravity_toolkit/SLR/C40.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 C40.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (05/2023)
 
 Reads monthly degree 4 zonal spherical harmonic data files from SLR
 
 Dataset distributed by CSR
     ftp://ftp.csr.utexas.edu/pub/slr/degree_5/
         CSR_Monthly_5x5_Gravity_Harmonics.txt
 Dataset distributed by GSFC
@@ -39,19 +39,20 @@
         https://dateutil.readthedocs.io/en/stable/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
     read_SLR_harmonics.py: low-degree spherical harmonic coefficients from SLR
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 01/2023: refactored satellite laser ranging read functions
     Written 09/2022
 """
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 import gravity_toolkit.read_SLR_harmonics
 
 # PURPOSE: read Degree 4 zonal data from Satellite Laser Ranging (SLR)
 def C40(SLR_file, C40_MEAN=0.0, DATE=None, **kwargs):
     """
@@ -75,30 +76,32 @@
     month: int
         GRACE/GRACE-FO month of measurement
     time: float
         date of SLR measurement
     """
 
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
-        raise FileNotFoundError('SLR file not found in file system')
-    # output dictionary with input data
-    dinput = {}
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
+        raise FileNotFoundError(f'{str(SLR_file)} not found in file system')
 
-    if bool(re.search(r'gsfc_slr_5x5c61s61',SLR_file,re.I)):
+    # output dictionary with data variables
+    dinput = {}
+    # determine source of input file
+    if bool(re.search(r'gsfc_slr_5x5c61s61', SLR_file.name, re.I)):
         # read 5x5 + 6,1 file from GSFC and extract coefficients
         Ylms = gravity_toolkit.read_SLR_harmonics(SLR_file, HEADER=True)
         # calculate 28-day moving-average solution from 7-day arcs
         dinput.update(gravity_toolkit.convert_weekly(Ylms['time'],
             Ylms['clm'][4,0,:], DATE=DATE, NEIGHBORS=28))
         # no estimated spherical harmonic errors
         dinput['error'] = np.zeros_like(DATE,dtype='f8')
-    elif bool(re.search(r'C40_LARES',SLR_file,re.I)):
+    elif bool(re.search(r'C40_LARES', SLR_file.name, re.I)):
         # read LARES filtered values
-        LARES_input = np.loadtxt(SLR_file,skiprows=1)
+        LARES_input = np.loadtxt(SLR_file, skiprows=1)
         dinput['time'] = LARES_input[:,0].copy()
         # convert C40 from anomalies to absolute
         dinput['data'] = 1e-10*LARES_input[:,1] + C40_MEAN
         # filtered data does not have errors
         dinput['error'] = np.zeros_like(LARES_input[:,1])
         # calculate GRACE/GRACE-FO month
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/SLR/C50.py` & `gravity-toolkit-1.2.1/gravity_toolkit/SLR/C50.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 C50.py
-Written by Yara Mohajerani and Tyler Sutterley (01/2023)
+Written by Yara Mohajerani and Tyler Sutterley (05/2023)
 
 Reads monthly degree 5 zonal spherical harmonic data files from SLR
 
 Dataset distributed by CSR
     ftp://ftp.csr.utexas.edu/pub/slr/degree_5/
         CSR_Monthly_5x5_Gravity_Harmonics.txt
 Dataset distributed by GSFC
@@ -39,29 +39,30 @@
         https://dateutil.readthedocs.io/en/stable/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
     read_SLR_harmonics.py: low-degree spherical harmonic coefficients from SLR
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 01/2023: refactored satellite laser ranging read functions
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 12/2021: use function for converting from 7-day arcs
     Updated 11/2021: reader for new weekly 5x5+6,1 fields from NASA GSFC
     Updated 09/2021: use functions for converting to and from GRACE months
     Updated 05/2021: simplified program similar to other SLR readers
         define int/float precision to prevent deprecation warning
     Updated 04/2021: using utilities from time module
     Updated 08/2020: flake8 compatible regular expression strings
     Updated 07/2020: added function docstrings
     Written 11/2019
 """
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 import gravity_toolkit.read_SLR_harmonics
 
 # PURPOSE: read Degree 5 zonal data from Satellite Laser Ranging (SLR)
 def C50(SLR_file, C50_MEAN=0.0, DATE=None, HEADER=True):
     """
@@ -87,23 +88,25 @@
     month: int
         GRACE/GRACE-FO month of measurement
     time: float
         date of SLR measurement
     """
 
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
-        raise FileNotFoundError('SLR file not found in file system')
-    # output dictionary with input data
-    dinput = {}
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
+        raise FileNotFoundError(f'{str(SLR_file)} not found in file system')
 
-    if bool(re.search(r'GSFC_SLR_C(20)_C(30)_C(50)',SLR_file,re.I)):
+    # output dictionary with data variables
+    dinput = {}
+    # determine source of input file
+    if bool(re.search(r'GSFC_SLR_C(20)_C(30)_C(50)', SLR_file.name, re.I)):
 
         # SLR C50 RL06 file from GSFC
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -115,15 +118,15 @@
             # add 1 to counter
             count += 1
 
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['data'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['error'] = np.zeros((n_mon))
         # time count
         t = 0
         # for every other line:
@@ -152,25 +155,25 @@
                 t += 1
         # verify that there imported C50 solutions
         if (t == 0):
             raise Exception('No GSFC C50 data imported')
         # truncate variables if necessary
         for key,val in dinput.items():
             dinput[key] = val[:t]
-    elif bool(re.search(r'gsfc_slr_5x5c61s61',SLR_file,re.I)):
+    elif bool(re.search(r'gsfc_slr_5x5c61s61', SLR_file.name, re.I)):
         # read 5x5 + 6,1 file from GSFC and extract coefficients
         Ylms = gravity_toolkit.read_SLR_harmonics(SLR_file, HEADER=True)
         # calculate 28-day moving-average solution from 7-day arcs
         dinput.update(gravity_toolkit.convert_weekly(Ylms['time'],
             Ylms['clm'][5,0,:], DATE=DATE, NEIGHBORS=28))
         # no estimated spherical harmonic errors
         dinput['error'] = np.zeros_like(DATE,dtype='f8')
-    elif bool(re.search(r'C50_LARES',SLR_file,re.I)):
+    elif bool(re.search(r'C50_LARES', SLR_file.name, re.I)):
         # read LARES filtered values
-        LARES_input = np.loadtxt(SLR_file,skiprows=1)
+        LARES_input = np.loadtxt(SLR_file, skiprows=1)
         dinput['time'] = LARES_input[:,0].copy()
         # convert C50 from anomalies to absolute
         dinput['data'] = 1e-10*LARES_input[:,1] + C50_MEAN
         # filtered data does not have errors
         dinput['error'] = np.zeros_like(LARES_input[:,1])
         # calculate GRACE/GRACE-FO month
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/SLR/CS2.py` & `gravity-toolkit-1.2.1/gravity_toolkit/SLR/CS2.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 CS2.py
-Written by Hugo Lecomte and Tyler Sutterley (01/2023)
+Written by Hugo Lecomte and Tyler Sutterley (05/2023)
 
 Reads monthly degree 2,m (figure axis and azimuthal dependence)
     spherical harmonic data files from satellite laser ranging (SLR)
 
 Dataset distributed by CSR
     http://download.csr.utexas.edu/pub/slr/degree_2/
         C21_S21_RL06.txt or C22_S22_RL06.txt
@@ -61,27 +61,28 @@
         https://doi.org/10.5880/GFZ.GRAVIS_06_L2B
     Chen el al., "Assessment of degree-2 order-1 gravitational changes
         from GRACE and GRACE Follow-on, Earth rotation, satellite laser
         ranging, and models", Journal of Geodesy, 95(38), (2021).
         https://doi.org/10.1007/s00190-021-01492-x
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 01/2023: refactored satellite laser ranging read functions
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 11/2021: reader for new weekly 5x5+6,1 fields from NASA GSFC
     Updated 09/2021: use functions for converting to and from GRACE months
     Updated 08/2021: output empty spherical harmonic errors for GSFC
     Updated 06/2021: added GSFC 7-day SLR figure axis solutions
     Updated 05/2021: added GFZ GravIS GRACE/SLR low degree solutions
     Updated 04/2021: use adjust_months function to fix special months cases
     Written 11/2020
 """
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 import gravity_toolkit.read_SLR_harmonics
 
 # PURPOSE: read Degree 2,m data from Satellite Laser Ranging (SLR)
 def CS2(SLR_file, ORDER=1, DATE=None, HEADER=True):
     """
@@ -112,30 +113,32 @@
     month: int
         GRACE/GRACE-FO month of measurement
     time: float
         date of SLR measurement
     """
 
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
-        raise FileNotFoundError('SLR file not found in file system')
-    # output dictionary with input data
-    dinput = {}
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
+        raise FileNotFoundError(f'{str(SLR_file)} not found in file system')
 
-    if bool(re.search(r'GSFC_C2(\d)_S2(\d)',SLR_file,re.I)):
+    # output dictionary with data variables
+    dinput = {}
+    # determine source of input file
+    if bool(re.search(r'GSFC_C2(\d)_S2(\d)', SLR_file.name, re.I)):
         # 7-day arc SLR file produced by GSFC
         # input variable names and types
         dtype = {}
         dtype['names'] = ('time','C2','S2')
         dtype['formats'] = ('f','f8','f8')
         # read SLR 2,1 file from GSFC
         # Column 1: Approximate mid-point of 7-day solution (years)
         # Column 2: Solution from SLR (normalized)
         # Column 3: Solution from SLR (normalized)
-        content = np.loadtxt(os.path.expanduser(SLR_file),dtype=dtype)
+        content = np.loadtxt(SLR_file, dtype=dtype)
         # duplicate time and harmonics
         tdec = np.repeat(content['time'],7)
         c2m = np.repeat(content['C2'],7)
         s2m = np.repeat(content['S2'],7)
         # calculate daily dates to use in centered moving average
         tdec += (np.mod(np.arange(len(tdec)),7) - 3.5)/365.25
         # number of dates to use in average
@@ -150,15 +153,15 @@
         for i,D in enumerate(DATE):
             isort = np.argsort((tdec - D)**2)[:n_neighbors]
             dinput['time'][i] = np.mean(tdec[isort])
             dinput['C2m'][i] = np.mean(c2m[isort])
             dinput['S2m'][i] = np.mean(s2m[isort])
         # GRACE/GRACE-FO month
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
-    elif bool(re.search(r'gsfc_slr_5x5c61s61',SLR_file,re.I)):
+    elif bool(re.search(r'gsfc_slr_5x5c61s61', SLR_file.name, re.I)):
         # read 5x5 + 6,1 file from GSFC and extract coefficients
         Ylms = gravity_toolkit.read_SLR_harmonics(SLR_file, HEADER=True)
         # duplicate time and harmonics
         tdec = np.repeat(Ylms['time'],7)
         c2m = np.repeat(Ylms['clm'][2,ORDER],7)
         s2m = np.repeat(Ylms['slm'][2,ORDER],7)
         # calculate daily dates to use in centered moving average
@@ -175,15 +178,15 @@
         for i,D in enumerate(DATE):
             isort = np.argsort((tdec - D)**2)[:n_neighbors]
             dinput['time'][i] = np.mean(tdec[isort])
             dinput['C2m'][i] = np.mean(c2m[isort])
             dinput['S2m'][i] = np.mean(s2m[isort])
         # GRACE/GRACE-FO month
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
-    elif bool(re.search(r'C2(\d)_S2(\d)_(RL\d{2})',SLR_file,re.I)):
+    elif bool(re.search(r'C2(\d)_S2(\d)_(RL\d{2})', SLR_file.name, re.I)):
         # SLR RL06 file produced by CSR
         # input variable names and types
         dtype = {}
         dtype['names'] = ('time','C2','S2','eC2','eS2',
             'C2aod','S2aod','start','end')
         dtype['formats'] = ('f','f8','f8','f','f','f','f','f','f')
         # read SLR 2,1 or 2,2 RL06 file from CSR
@@ -192,35 +195,35 @@
         # Column 2: Solution from SLR (normalized)
         # Column 3: Solution from SLR (normalized)
         # Column 4: Solution sigma (1E-10)
         # Column 5: Solution sigma (1E-10)
         # Column 6: Mean value of Atmosphere-Ocean De-aliasing model (1E-10)
         # Column 7: Mean value of Atmosphere-Ocean De-aliasing model (1E-10)
         # Columns 8-9: Start and end dates of data used in solution
-        content = np.loadtxt(os.path.expanduser(SLR_file),dtype=dtype)
+        content = np.loadtxt(SLR_file, dtype=dtype)
         # date and GRACE/GRACE-FO month
         dinput['time'] = content['time'].copy()
         dinput['month'] = gravity_toolkit.time.calendar_to_grace(dinput['time'])
         # remove the monthly mean of the AOD model
         dinput['C2m'] = content['C2'] - content['C2aod']*10**-10
         dinput['S2m'] = content['S2'] - content['S2aod']*10**-10
         # scale SLR solution sigmas
         dinput['eC2m'] = content['eC2']*10**-10
         dinput['eS2m'] = content['eS2']*10**-10
-    elif bool(re.search(r'GRAVIS-2B_GFZOP',SLR_file,re.I)):
+    elif bool(re.search(r'GRAVIS-2B_GFZOP', SLR_file.name, re.I)):
         # Combined GRACE/SLR solution file produced by GFZ
         # Column  1: MJD of BEGINNING of solution data span
         # Column  2: Year and fraction of year of BEGINNING of solution span
         # Column  9: Replacement C(2,1)
         # Column 10: Replacement C(2,1) - mean C(2,1) (1.0E-10)
         # Column 11: C(2,1) formal standard deviation (1.0E-12)
         # Column 12: Replacement S(2,1)
         # Column 13: Replacement S(2,1) - mean S(2,1) (1.0E-10)
         # Column 14: S(2,1) formal standard deviation (1.0E-12)
-        with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+        with SLR_file.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -232,15 +235,15 @@
             # add 1 to counter
             count += 1
 
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['C2m'] = np.zeros((n_mon))
         dinput['S2m'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['eC2m'] = np.zeros((n_mon))
         dinput['eS2m'] = np.zeros((n_mon))
         # time count
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/__init__.py` & `gravity-toolkit-1.2.1/gravity_toolkit/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -19,40 +19,71 @@
 import gravity_toolkit.mascons
 import gravity_toolkit.time
 import gravity_toolkit.tools
 import gravity_toolkit.utilities
 import gravity_toolkit.version
 from gravity_toolkit import SLR
 from gravity_toolkit import time_series
-from gravity_toolkit.associated_legendre import associated_legendre, plm_colombo, plm_holmes, plm_mohlenkamp
+from gravity_toolkit.associated_legendre import (
+    associated_legendre,
+    plm_colombo,
+    plm_holmes,
+    plm_mohlenkamp
+)
 from gravity_toolkit.clenshaw_summation import clenshaw_summation
 from gravity_toolkit.degree_amplitude import degree_amplitude
 from gravity_toolkit.destripe_harmonics import destripe_harmonics
-from gravity_toolkit.fourier_legendre import fourier_legendre, legendre_gradient
+from gravity_toolkit.fourier_legendre import (
+    fourier_legendre,
+    legendre_gradient
+)
 from gravity_toolkit.gauss_weights import gauss_weights
 from gravity_toolkit.gen_averaging_kernel import gen_averaging_kernel
 from gravity_toolkit.gen_disc_load import gen_disc_load
 from gravity_toolkit.gen_harmonics import gen_harmonics
 from gravity_toolkit.gen_point_load import gen_point_load
 from gravity_toolkit.gen_spherical_cap import gen_spherical_cap
 from gravity_toolkit.gen_stokes import gen_stokes
 from gravity_toolkit.geocenter import geocenter
 from gravity_toolkit.grace_date import grace_date
 from gravity_toolkit.grace_find_months import grace_find_months
-from gravity_toolkit.grace_input_months import grace_input_months, read_ecmwf_corrections
+from gravity_toolkit.grace_input_months import (
+    grace_input_months,
+    read_ecmwf_corrections
+)
 from gravity_toolkit.grace_months_index import grace_months_index
 from gravity_toolkit.harmonics import harmonics
-from gravity_toolkit.harmonic_gradients import harmonic_gradients
-from gravity_toolkit.harmonic_summation import harmonic_summation, harmonic_transform, stokes_summation
+from gravity_toolkit.harmonic_gradients import (
+    harmonic_gradients,
+    geostrophic_currents
+)
+from gravity_toolkit.harmonic_summation import (
+    harmonic_summation,
+    harmonic_transform,
+    stokes_summation
+)
 from gravity_toolkit.legendre_polynomials import legendre_polynomials
 from gravity_toolkit.legendre import legendre
 from gravity_toolkit.ocean_stokes import ocean_stokes
 from gravity_toolkit.read_gfc_harmonics import read_gfc_harmonics
-from gravity_toolkit.read_GIA_model import read_GIA_model, gia
+from gravity_toolkit.read_GIA_model import (
+    read_GIA_model,
+    gia
+)
 from gravity_toolkit.read_GRACE_harmonics import read_GRACE_harmonics
-from gravity_toolkit.read_love_numbers import read_love_numbers, load_love_numbers, love_numbers
-from gravity_toolkit.read_SLR_harmonics import read_SLR_harmonics, convert_weekly
+from gravity_toolkit.read_love_numbers import (
+    read_love_numbers,
+    load_love_numbers,
+    love_numbers
+)
+from gravity_toolkit.read_SLR_harmonics import (
+    read_SLR_harmonics,
+    convert_weekly
+)
 from gravity_toolkit.sea_level_equation import sea_level_equation
-from gravity_toolkit.spatial import spatial, scaling_factors
+from gravity_toolkit.spatial import (
+    spatial,
+    scaling_factors
+)
 from gravity_toolkit.units import units
 # get version number
 __version__ = gravity_toolkit.version.version
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/associated_legendre.py` & `gravity-toolkit-1.2.1/gravity_toolkit/associated_legendre.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 #!/usr/bin/env python
 u"""
 associated_legendre.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (03/2023)
 
 Computes fully-normalized associated Legendre Polynomials
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 01/2023: refactored associated legendre polynomials
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 09/2020: verify dimensions of input x variable
     Updated 08/2020: prevent zero divisions by changing u==0 to eps of data type
     Updated 07/2020: added function docstrings
     Updated 10/2018: using future division for python3 Compatibility
@@ -17,76 +18,83 @@
     Updated 05/2015: added parameter MMAX for MMAX != LMAX
     Updated 09/2013: new format for file headers
     Written 03/2013
 """
 from __future__ import division
 import numpy as np
 
-def associated_legendre(LMAX, x, method='holmes', MMAX=None, astype=np.float64):
+def associated_legendre(LMAX, x,
+        method='holmes',
+        MMAX=None,
+        astype=np.float64
+    ):
     """
     Computes fully-normalized associated Legendre Polynomials and their
     first derivative
 
     Parameters
     ----------
     LMAX: int
         maximum degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
     method: str, default 'holmes'
         Method for computing the associated Legrendre polynomials
 
             - ``'columbo'``
             - ``'holmes'``
             - ``'mohlenkamp'``
     MMAX: int or NoneType, default None
         maximum order of Associated Legrendre polynomials
-    astype: obj, default np.float64
+    astype: np.dtype, default np.float64
         output variable data type
 
     Returns
     -------
-    plms: float
+    plms: np.ndarray
         fully-normalized Legendre polynomials
-    dplms: float
+    dplms: np.ndarray
         first derivative of Legendre polynomials
     """
     if (method.lower() == 'colombo'):
         return plm_colombo(LMAX, x, MMAX=MMAX, astype=astype)
     elif (method.lower() == 'holmes'):
         return plm_holmes(LMAX, x, MMAX=MMAX, astype=astype)
     elif (method.lower() == 'mohlenkamp'):
         return plm_mohlenkamp(LMAX, x, MMAX=MMAX, astype=astype)
     raise ValueError(f'Unknown method {method}')
 
-def plm_colombo(LMAX, x, MMAX=None, astype=np.float64):
+def plm_colombo(LMAX, x,
+        MMAX=None,
+        astype=np.float64
+    ):
     """
     Computes fully-normalized associated Legendre Polynomials and their
     first derivative using a Standard forward column method [Colombo1981]_
 
     Parameters
     ----------
     LMAX: int
         maximum degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
     MMAX: int or NoneType, default None
         maximum order of Associated Legrendre polynomials
-    astype: obj, default np.float64
+    astype: np.dtype, default np.float64
         output variable data type
 
     Returns
     -------
-    plms: float
+    plms: np.ndarray
         fully-normalized Legendre polynomials
-    dplms: float
+    dplms: np.ndarray
         first derivative of Legendre polynomials
 
     References
     ----------
     .. [Colombo1981] O. L. Colombo,
         "Numerical Methods for Harmonic Analysis on the Sphere",
         Air Force Contract No. F19628-79-C-0027,
@@ -150,37 +158,40 @@
         # calculate first derivatives for sectorial harmonics
         dplm[l,l,:] = np.longdouble(l)*(x/u)*plm[l,l,:]
 
     # return the legendre polynomials and their first derivative
     # truncating orders to MMAX
     return plm[:,:MMAX+1,:], dplm[:,:MMAX+1,:]
 
-def plm_holmes(LMAX, x, MMAX=None, astype=np.float64):
+def plm_holmes(LMAX, x,
+        MMAX=None,
+        astype=np.float64
+    ):
     """
     Computes fully-normalized associated Legendre Polynomials and their
     first derivative using the recursion relation from [Holmes2002]_
 
     Parameters
     ----------
     LMAX: int
         maximum degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
     MMAX: int or NoneType, default None
         maximum order of Associated Legrendre polynomials
-    astype: obj, default np.float64
+    astype: np.dtype, default np.float64
         output variable data type
 
     Returns
     -------
-    plms: float
+    plms: np.ndarray
         fully-normalized Legendre polynomials
-    dplms: float
+    dplms: np.ndarray
         first derivative of Legendre polynomials
 
     References
     ----------
     .. [Losch2003] M. Losch and V. Seufer,
         "How to Compute Geoid Undulations (Geoid Height Relative
         to a Given Reference Ellipsoid) from Spherical Harmonic
@@ -281,37 +292,42 @@
                 flm = np.sqrt(((l**2.0 - m**2.0)*(2.0*l + 1.0))/(2.0*l - 1.0))
                 dplm[l,m,:]= (1.0/u)*(l*x*plm[l,m,:] - flm*plm[l-1,m,:])
 
     # return the legendre polynomials and their first derivative
     # truncating orders to MMAX
     return plm[:,:MMAX+1,:], dplm[:,:MMAX+1,:]
 
-def plm_mohlenkamp(LMAX, x, MMAX=None, astype=np.float64):
+def plm_mohlenkamp(LMAX, x,
+        MMAX=None,
+        astype=np.float64
+    ):
     """
     Computes fully-normalized associated Legendre Polynomials and their
     first derivative using the recursion relation from [Mohlenkamp2016]_
 
     Derived from [Szego1939]_ recurrence formula for Jacobi Polynomials
 
     Parameters
     ----------
     LMAX: int
         maximum degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
     MMAX: int or NoneType, default None
         maximum order of Associated Legrendre polynomials
+    astype: np.dtype, default np.float64
+        output variable data type
 
     Returns
     -------
-    plms: float
+    plms: np.ndarray
         fully-normalized Legendre polynomials
-    dplms: float
+    dplms: np.ndarray
         first derivative of Legendre polynomials
 
     References
     ----------
     .. [Mohlenkamp2016] M. J. Mohlenkamp,
         "A User's Guide to Spherical Harmonics", (2016).
         `[pdf] <http://www.ohiouniversityfaculty.com/mohlenka/research/uguide.pdf>`_
@@ -329,18 +345,18 @@
 
     # removing singleton dimensions of x
     x = np.atleast_1d(x).flatten()
     # length of the x array
     sx = len(x)
 
     # Initialize the output Legendre polynomials
-    plm = np.zeros((LMAX+1,MMAX+1,sx), dtype=astype)
+    plm = np.zeros((LMAX+1, MMAX+1, sx), dtype=astype)
     dplm = np.zeros((LMAX+1,LMAX+1,sx), dtype=astype)
     # Jacobi polynomial for the recurrence relation
-    jlmm = np.zeros((LMAX+1,MMAX+1,sx))
+    jlmm = np.zeros((LMAX+1, MMAX+1, sx))
     # for x=cos(th): u= sin(th)
     u = np.sqrt(1.0 - x**2)
     # update where u==0 to eps of data type to prevent invalid divisions
     u[u == 0] = np.finfo(u.dtype).eps
 
     # for all spherical harmonic orders of interest
     for mm in range(0,MMAX+1):# equivalent to 0:MMAX
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/clenshaw_summation.py` & `gravity-toolkit-1.2.1/gravity_toolkit/clenshaw_summation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 clenshaw_summation.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (04/2023)
 Calculates the spatial field for a series of spherical harmonics for a
     sequence of ungridded points
 
 CALLING SEQUENCE:
     spatial = clenshaw_summation(clm, slm, lon, lat, UNITS=1,
         LMAX=60, LOVE=(hl,kl,ll))
 
@@ -45,14 +45,16 @@
         the Recursive Computation of Very High Degree and Order Normalised
         Associated Legendre Functions", Journal of Geodesy (2002)
         https://doi.org/10.1007/s00190-002-0216-2
     Tscherning and Poder, "Some Geodetic Applications of Clenshaw Summation",
         Bollettino di Geodesia e Scienze (1982)
 
 UPDATE HISTORY:
+    Updated 04/2023: allow love numbers to be None for custom units case
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: added UNITS list option for converting to custom units
     Updated 09/2021: fix passing SCALE keyword argument to clenshaw_s_m
     Updated 06/2021: output equivalent pressure in pascals
     Updated 08/2020: parameterize float precision to improve computational time
@@ -63,54 +65,60 @@
         simplified love number extrapolation if LMAX is greater than 696
     Written 08/2017
 """
 import numpy as np
 from gravity_toolkit.gauss_weights import gauss_weights
 from gravity_toolkit.units import units
 
-def clenshaw_summation(clm, slm, lon, lat, RAD=0, UNITS=0, LMAX=0, LOVE=None,
-    ASTYPE=np.longdouble, SCALE=1e-280):
+def clenshaw_summation(clm, slm, lon, lat,
+        RAD=0,
+        UNITS=0,
+        LMAX=0,
+        LOVE=None,
+        ASTYPE=np.longdouble,
+        SCALE=1e-280
+    ):
     """
     Calculates the spatial field for a series of spherical harmonics for a
     sequence of ungridded points
 
     Parameters
     ----------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    lon: float
+    lon: np.ndarray
         longitude of points
-    lat: float
+    lat: np.ndarray
         latitude of points
-    RAD: float, default 0.0
+    RAD: int or float, default 0
         Gaussian smoothing radius (km)
-    UNITS: int, default 0
+    UNITS: int, str, list or np.ndarray, default 0
         Output data units
 
             - ``1``: cm water equivalent thickness (cm w.e., g/cm\ :sup:`2`)
             - ``2``: mm geoid height
             - ``3``: mm elastic crustal deformation
             - ``4``: microGal gravitational perturbation
             - ``5``: mbar equivalent surface pressure
             - ``6``: cm viscoelastic crustal uplift (GIA)
             - list: custom degree-dependent unit conversion factor
     LMAX: int, default 0
         Upper bound of Spherical Harmonic Degrees
     LOVE: tuple or NoneType, default None
         Load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
-    ASTYPE: obj, default np.longdouble
+    ASTYPE: np.dtype, default np.longdouble
         floating point precision for calculating Clenshaw summation
     SCALE: float, default 1e-280
         scaling factor to prevent underflow in Clenshaw summation
 
     Returns
     -------
-    spatial: float
+    spatial: np.ndarray
         calculated spatial field for latitude and longitude
 
     References
     ----------
     .. [Davis2004] J. L. Davis et al.,
         "Climate-driven deformation of the solid Earth from GRACE and GPS",
         *Geophysical Research Letters*, 31(L24605), (2004).
@@ -151,41 +159,25 @@
     if (RAD != 0):
         wl = 2.0*np.pi*gauss_weights(RAD,LMAX)
     else:
         # else = 1
         wl = np.ones((LMAX+1))
 
     # Setting units factor for output
-    # extract arrays of kl, hl, and ll Love Numbers
-    factors = units(lmax=LMAX).harmonic(*LOVE)
-    # dfactor computes the degree dependent coefficients
-    if isinstance(UNITS, (list, np.ndarray)):
+    # dfactor is the degree dependent coefficients
+    factors = units(lmax=LMAX)
+    if isinstance(UNITS, (list,np.ndarray)):
         # custom units
         dfactor = np.copy(UNITS)
-    elif (UNITS == 0):
-        # 0: keep original scale
-        dfactor = factors.norm
-    elif (UNITS == 1):
-        # 1: cmH2O, centimeters water equivalent
-        dfactor = factors.cmwe
-    elif (UNITS == 2):
-        # 2: mmGH, mm geoid height
-        dfactor = factors.mmGH
-    elif (UNITS == 3):
-        # 3: mmCU, mm elastic crustal deformation
-        dfactor = factors.mmCU
-    elif (UNITS == 4):
-        # 4: micGal, microGal gravity perturbations
-        dfactor = factors.microGal
-    elif (UNITS == 5):
-        # 5: mbar, equivalent surface pressure
-        dfactor = factors.mbar
-    elif (UNITS == 6):
-        # 6: cmVCU, cm viscoelastic  crustal uplift (GIA ONLY)
-        dfactor = factors.cmVCU
+    elif isinstance(UNITS, str):
+        # named units
+        dfactor = factors.harmonic(*LOVE).get(UNITS)
+    elif isinstance(UNITS, int):
+        # use named unit codes
+        dfactor = factors.harmonic(*LOVE).get(units.bycode(UNITS))
     else:
         raise ValueError(f'Unknown units {UNITS}')
 
     # calculate arrays for clenshaw summations over colatitudes
     s_m_c = np.zeros((npts,LMAX*2+2))
     for m in range(LMAX, -1, -1):
         # convolve harmonics with unit factors and smoothing
@@ -214,31 +206,45 @@
     # calculate spatial field
     spatial = np.sqrt(3.0)*u*s_m + s_m_c[:,0]
     # return the calculated spatial field
     return spatial
 
 # PURPOSE: compute conditioned arrays for Clenshaw summation from the
 # fully-normalized associated Legendre's function for an order m
-def clenshaw_s_m(t, f, m, clm1, slm1, lmax, ASTYPE=np.longdouble, SCALE=1e-280):
+def clenshaw_s_m(t, f, m, clm1, slm1, lmax,
+        ASTYPE=np.longdouble,
+        SCALE=1e-280
+    ):
     """
     Compute conditioned arrays for Clenshaw summation from the fully-normalized
     associated Legendre's function for an order m
 
     Parameters
     ----------
-    t: elements ranging from -1 to 1, typically cos(th)
-    f: degree dependent factors
-    m: spherical harmonic order
-    clm1: cosine spherical harmonics
-    slm1: sine spherical harmonics
-    lmax: maximum spherical harmonic degree
+    t: np.ndarray
+        elements ranging from -1 to 1, typically cos(th)
+    f: np.ndarray
+        degree dependent factors
+    m: int
+        spherical harmonic order
+    clm1: np.ndarray
+        cosine spherical harmonics
+    slm1: np.ndarray
+        sine spherical harmonics
+    lmax: int
+        maximum spherical harmonic degree
+    ASTYPE: np.dtype, default np.longdouble
+        floating point precision for calculating Clenshaw summation
+    SCALE: float, default 1e-280
+        scaling factor to prevent underflow in Clenshaw summation
 
     Returns
     -------
-    s_m_c: conditioned array for clenshaw summation
+    s_m_c: np.ndarray
+        conditioned array for clenshaw summation
     """
     # allocate for output matrix
     N = len(t)
     s_m = np.zeros((N,2),dtype=ASTYPE)
     # scaling to prevent overflow
     clm = SCALE*clm1.astype(ASTYPE)
     slm = SCALE*slm1.astype(ASTYPE)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/data/Load_Love2_CE.dat` & `gravity-toolkit-1.2.1/gravity_toolkit/data/Load_Love2_CE.dat`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/data/PREM-LLNs-truncated.dat` & `gravity-toolkit-1.2.1/gravity_toolkit/data/PREM-LLNs-truncated.dat`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/data/PREMhard-LLNs-truncated.dat` & `gravity-toolkit-1.2.1/gravity_toolkit/data/PREMhard-LLNs-truncated.dat`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/data/PREMsoft-LLNs-truncated.dat` & `gravity-toolkit-1.2.1/gravity_toolkit/data/PREMsoft-LLNs-truncated.dat`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/data/love_numbers` & `gravity-toolkit-1.2.1/gravity_toolkit/data/love_numbers`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/degree_amplitude.py` & `gravity-toolkit-1.2.1/gravity_toolkit/degree_amplitude.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 degree_amplitude.py
-Written Tyler Sutterley (05/2022)
+Written Tyler Sutterley (03/2023)
 
 Calculates the amplitude of each spherical harmonic degree
 
 CALLING SEQUENCE:
     amp = degree_amplitude(clm,slm)
 
 INPUTS:
@@ -16,41 +16,47 @@
     LMAX: Upper bound of Spherical Harmonic Degrees
     MMAX: Upper bound of Spherical Harmonic Orders
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 05/2022: use numpy atleast_3d to add singleton dimensions
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 07/2020: added function docstrings
     Updated 05/2020: add singleton dimension to calculate time series amplitudes
     Updated 05/2015: added parameter MMAX for MMAX != LMAX
     Written 07/2013
 """
 import numpy as np
 
-def degree_amplitude(clm, slm, LMAX=None, MMAX=None):
+def degree_amplitude(
+        clm,
+        slm,
+        LMAX=None,
+        MMAX=None,
+    ):
     """
     Calculates the amplitude of each spherical harmonic degree
 
     Parameters
     ----------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
     LMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
 
     Returns
     -------
-    amp: float
+    amp: np.ndarray
         degree amplitude
     """
     # add a singleton dimension to input harmonics
     clm = np.atleast_3d(clm)
     slm = np.atleast_3d(slm)
     # check shape
     LMp1,MMp1,nt = np.shape(clm)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/destripe_harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/destripe_harmonics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 #!/usr/bin/env python
 u"""
 destripe_harmonics.py
 Original Fortran program remove_errors.f written by Isabella Velicogna
 Adapted by Chia-Wei Hsu (05/2018)
-Updated by Tyler Sutterley (04/2022)
+Updated by Tyler Sutterley (03/2023)
 
 Filters spherical harmonic coefficients for correlated "striping" errors
 
 ALGORITHM:
     clm1, slm1 are the spherical harmonic coefficients
         after the mean field has been removed
     Smooth values over l, for l=even and l=odd separately
@@ -41,54 +41,61 @@
 
 REFERENCES:
     S C Swenson and J Wahr, "Post-processing removal of correlated errors in
         GRACE data", Geophysical Research Letters, 33(L08402), 2006
         https://doi.org/10.1029/2005GL025285
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 02/2021: replaced numpy bool to prevent deprecation warning
     Updated 07/2020: added function docstrings
     Updated 03/2020: Updated for public release
     Updated 05/2018: using __future__ print and updated flags comments
     Updated 08/2015: changed from sys.exit to raise ValueError
     Updated 05/2015: added parameter MMAX for MMAX != LMAX
     Updated 02/2014: generalization for GRACE GUI and other routines
 """
-from __future__ import print_function
 import numpy as np
 
-def destripe_harmonics(clm1, slm1, LMIN=2, LMAX=60, MMAX=None,
-    ROUND=True, NARROW=False):
+def destripe_harmonics(
+        clm1,
+        slm1,
+        LMIN=2,
+        LMAX=60,
+        MMAX=None,
+        ROUND=True,
+        NARROW=False,
+    ):
     """
     Filters spherical harmonic coefficients for correlated striping errors
 
     Parameters
     ----------
-    clm1: float
+    clm1: np.ndarray
         cosine spherical harmonic coefficients
-    slm1: float
+    slm1: np.ndarray
         sine spherical harmonic coefficients
     LMIN: int, default 2
         Lower bound of Spherical Harmonic Degrees
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
     ROUND: bool, default True
         use round to find nearest even
     NARROW: bool, default True
         set harmonics to 0 if less than window size
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         filtered cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         filtered sine spherical harmonic coefficients
 
     References
     ----------
     .. [Swenson2006] S. Swenson and J. Wahr,
         "Post-processing removal of correlated errors in GRACE data",
         *Geophysical Research Letters*, 33(L08402), (2006).
@@ -107,16 +114,16 @@
     Wclm = clm1.copy()
     Wslm = slm1.copy()
     # matrix size declarations
     clmeven = np.zeros((LMAX), dtype=np.float64)
     slmeven = np.zeros((LMAX), dtype=np.float64)
     clmodd = np.zeros((LMAX+1), dtype=np.float64)
     slmodd = np.zeros((LMAX+1), dtype=np.float64)
-    clmsm = np.zeros((LMAX+1,MMAX+1), dtype=np.float64)
-    slmsm = np.zeros((LMAX+1,MMAX+1), dtype=np.float64)
+    clmsm = np.zeros((LMAX+1, MMAX+1), dtype=np.float64)
+    slmsm = np.zeros((LMAX+1, MMAX+1), dtype=np.float64)
 
     # start of the smoothing over orders (m)
     for m in range(int(MMAX+1)):
         smooth = np.exp(-np.float64(m)/10.0)*15.0
         if ROUND:
             # round(smooth) to nearest even instead of int(smooth)
             nsmooth = np.around(smooth)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/fourier_legendre.py` & `gravity-toolkit-1.2.1/gravity_toolkit/fourier_legendre.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 #!/usr/bin/env python
 u"""
 fourier_legendre.py
 Original IDL code gen_plms.pro written by Sean Swenson
-Adapted by Tyler Sutterley (10/2022)
+Adapted by Tyler Sutterley (03/2023)
 
 Computes Fourier coefficients of the associated Legendre functions
 
 CALLING SEQUENCE:
     plm = fourier_legendre(lmax,mmax)
 
 INPUTS:
@@ -16,14 +16,15 @@
 OUTPUTS:
     plm: Fourier coefficients
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 10/2022: add polynomial function for calculating gradients
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 09/2021: cleaned up program for public release
     Updated 07/2020: added function docstrings
     Updated 06/2019: using Python3 compatible division
     Written 04/2013
 """
@@ -39,15 +40,15 @@
     lmax: int
         Upper bound of Spherical Harmonic Degrees
     mmax: int
         Upper bound of Spherical Harmonic Orders
 
     Returns
     -------
-    plm: float
+    plm: np.ndarray
         Fourier coefficients
     """
 
     # allocate for output fourier coefficients
     plm = np.zeros((lmax+1,lmax+1,lmax+1))
     l_even = np.arange(0,lmax+1,2)
     l_odd = np.arange(1,lmax,2)
@@ -220,17 +221,17 @@
     lmax: int
         Upper bound of Spherical Harmonic Degrees
     mmax: int
         Upper bound of Spherical Harmonic Orders
 
     Returns
     -------
-    vlm: float
+    vlm: np.ndarray
         Fourier coefficients for meridional gradients
-    wlm: float
+    wlm: np.ndarray
         Fourier coefficients for zonal gradients
     """
 
     plm = fourier_legendre(lmax, mmax)
     vlm = np.zeros((lmax+1,lmax+1,lmax+1))
     wlm = np.zeros((lmax+1,lmax+1,lmax+1))
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gauss_weights.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gauss_weights.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 #!/usr/bin/env python
 u"""
 gauss_weights.py
 Original IDL code gauss_weights.pro written by Sean Swenson
-Adapted by Tyler Sutterley (04/2022)
+Adapted by Tyler Sutterley (03/2023)
 
 Computes the Gaussian weights as a function of degree
 A normalized version of Jekeli's Gaussian averaging function
 
 Christopher Jekeli (1981)
 Alternative Methods to Smooth the Earth's Gravity Field
 http://www.geology.osu.edu/~jekeli.1/OSUReports/reports/report_327.pdf
@@ -34,14 +34,15 @@
         weighting from gauss_weights is normalized outside of the function
             wt = 2.0*pi*gauss_weights(rad,LMAX)
         weighting from recurs is normalized inside of the function
         call recurs(alpha,bcoef) calculates bcoef up to LMAX 150 (=wt[0:150])
             alpha = alog(2.)/(1.-cos(rad/6371.))
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 09/2021: added option for setting minimum value threshold
     Updated 07/2020: added function docstrings
     Updated 06/2015: adjusted threshold from 1e-9 to 1e-10
     Updated 12/2014: updated comments and header text updating full reference
     Updated 02/2014: changed variables from ints to floats to prevent truncation
     Written 05/2013
@@ -60,15 +61,15 @@
     LMAX: int
         Maximum degree of spherical harmonic coefficients
     CUTOFF: float, default 1e-10
         minimum value for tail of Gaussian averaging function
 
     Returns
     -------
-    wl: float
+    wl: np.ndarray
         degree dependent weighting function
 
     References
     ----------
     .. [Jekeli1981] C. Jekeli, "Alternative Methods to Smooth
         the Earth's Gravity Field", NASA Grant No. NGR 36-008-161,
         OSURF Proj. No. 783210, 48 pp., (1981).
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_averaging_kernel.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_averaging_kernel.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 #!/usr/bin/env python
 u"""
 gen_averaging_kernel.py
 Original IDL code gen_wclms_me.pro written by Sean Swenson
-Adapted by Tyler Sutterley (04/2022)
+Adapted by Tyler Sutterley (06/2023)
 
 Generates averaging kernel coefficients which minimize the total error
 
 CALLING SEQUENCE:
     Wlms = gen_averaging_kernel(gclm,gslm,eclm,eslm,sigma,hw,
         LMIN=0, LMAX=60, UNITS=0, LOVE=(hl,kl,ll))
 
@@ -30,72 +30,79 @@
     clm: cosine coefficients of the averaging kernel
     slm: sine coefficients of the averaging kernel
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 PROGRAM DEPENDENCIES:
+    harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     units.py: class for converting spherical harmonic data to specific units
 
 REFERENCES:
     Swenson and Wahr, "Methods for inferring regional surface-mass anomalies
         from Gravity Recovery and Climate Experiment (GRACE) measurements of
         time-variable gravity," Journal of Geophysical Research: Solid Earth,
         107(B9), (2002). https://doi.org/10.1029/2001JB000576
 
 UPDATE HISTORY:
+    Updated 06/2023: added option for setting minimum value threshold
+        use harmonics class for spherical harmonic operations
+    Updated 04/2023: allow love numbers to be None for mass units case
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 08/2021: using units module for Earth parameters
     Updated 04/2020: reading load love numbers outside of this function
     Updated 05/2015: added parameter MMAX for MMAX != LMAX
     Written 05/2013
 """
 import numpy as np
 import gravity_toolkit.units
 
 def gen_averaging_kernel(gclm, gslm, eclm, eslm, sigma, hw,
-    LMAX=60, MMAX=None, UNITS=0, LOVE=None):
+    LMAX=60, MMAX=None, CUTOFF=1e-15, UNITS=0, LOVE=None):
     """
     Generates averaging kernel coefficients which minimize the
     total error following [Swenson2002]_
 
     Uses a normalized form of the Gaussian averaging function
     from [Jekeli1981]_
 
     Parameters
     ----------
-    gclm: float
+    gclm: np.ndarray
         cosine spherical harmonics of exact averaging kernel
-    gslm: float
+    gslm: np.ndarray
         sine spherical harmonics of exact averaging kernel
-    eclm: float
+    eclm: np.ndarray
         measurement error in the cosine harmonics
-    eslm: float
+    eslm: np.ndarray
         measurement error in the sine harmonics
     sigma: float
         variance of the surface mass signal
     hw: float
         Gaussian radius of the kernel in kilometers
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
+    CUTOFF: float, default 1e-15
+        minimum value for tail of Gaussian averaging function
     UNITS: int, default 0
         Input data units
 
             - ``0``: fully-normalized
             - ``1``: mass coefficients (cm w.e., g/cm\ :sup:`2`)
     LOVE: tuple or NoneType, default None
         Load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine coefficients of the averaging kernel
-    slm: float
+    slm: np.ndarray
         sine coefficients of the averaging kernel
 
     References
     ----------
     .. [Jekeli1981] C. Jekeli, "Alternative Methods to Smooth
         the Earth's Gravity Field", NASA Grant No. NGR 36-008-161,
         OSURF Proj. No. 783210, 48 pp., (1981).
@@ -107,18 +114,24 @@
         `doi: 10.1029/2001JB000576 <https://doi.org/10.1029/2001JB000576>`_
     """
     # upper bound of spherical harmonic orders (default = LMAX)
     if MMAX is None:
         MMAX = np.copy(LMAX)
 
     # Earth Parameters
+    factors = gravity_toolkit.units(lmax=LMAX)
     # extract arrays of kl, hl, and ll Love Numbers
-    dfactor = gravity_toolkit.units(lmax=LMAX).harmonic(*LOVE)
+    if (UNITS == 0):
+        # Input coefficients are fully-normalized
+        dfactor = factors.harmonic(*LOVE).cmwe
+    elif (UNITS == 1):
+        # Inputs coefficients are mass (cmwe)
+        dfactor = np.ones((LMAX+1))
     # average radius of the earth (km)
-    rad_e = dfactor.rad_e/1e5
+    rad_e = factors.rad_e/1e5
 
     # allocate for gaussian function
     gl = np.zeros((LMAX+1))
     # calculate gaussian weights using recursion
     b = np.log(2.0)/(1.0-np.cos(hw/rad_e))
     # weight for degree 0
     gl[0] = (1.0-np.exp(-2.0*b))/b
@@ -128,16 +141,16 @@
     valid = True
     # spherical harmonic degree
     l = 2
     # generate Legendre coefficients of Gaussian correlation function
     while (valid and (l <= LMAX)):
         gl[l] = (1.0 - 2.0*l)/b*gl[l-1] + gl[l-2]
         # check validity
-        if (np.abs(gl[l]) < 1.0e-15):
-            gl[l:LMAX+1] = 1.0e-15
+        if (gl[l] < CUTOFF):
+            gl[l:LMAX+1] = CUTOFF
             valid = False
         # add to counter for spherical harmonic degree
         l += 1
 
     # Convert sigma to correlation function amplitude
     area = np.copy(gclm[0,0])
     temp_0 = np.zeros((LMAX+1))
@@ -148,28 +161,24 @@
 
     # divide by the square of the area under the kernel
     temp = np.sum(temp_0)/area**2
     # signal variance
     sigma_0 = sigma/np.sqrt(temp)
 
     # Compute averaging kernel coefficients
-    wclm = np.zeros((LMAX+1,MMAX+1))
-    wslm = np.zeros((LMAX+1,MMAX+1))
+    Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     # for each spherical harmonic degree
     for l in range(0,LMAX+1):# equivalent to 0:lmax
-        if (UNITS == 0):
-            # Input coefficients are fully-normalized
-            cmwe = dfactor.cmwe[l]
-            ldivg = (cmwe**2)/(gl[l]*sigma_0**2)
-        elif (UNITS == 1):
-            # Inputs coefficients are mass (cmwe)
-            ldivg = 1.0/(gl[l]*sigma_0**2)
+        # inverse of smoothed signal variance in output units
+        ldivg = (dfactor[l]**2)/(gl[l]*sigma_0**2)
         # for each valid spherical harmonic order
         mm = np.min([MMAX,l])
         for m in range(0,mm+1):
             temp = 1.0 + 2.0*ldivg*eclm[l,m]**2
-            wclm[l,m] = gclm[l,m]/temp
+            Ylms.clm[l,m] = gclm[l,m]/temp
             temp = 1.0 + 2.0*ldivg*eslm[l,m]**2
-            wslm[l,m] = gslm[l,m]/temp
+            Ylms.slm[l,m] = gslm[l,m]/temp
 
     # return kernels divided by the area under the kernel
-    return {'clm':wclm/area, 'slm':wslm/area}
+    return Ylms.scale(1.0/area)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_disc_load.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_disc_load.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gen_disc_load.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (06/2023)
 Calculates gravitational spherical harmonic coefficients for a uniform disc load
 
 CALLING SEQUENCE:
     Ylms = gen_disc_load(data, lon, lat, area, LMAX=60, MMAX=None)
 
 INPUTS:
     data: data magnitude (Gt)
@@ -51,15 +51,17 @@
         https://doi.org/10.1029/RG010i003p00761
     H. N. Pollack, Journal of Geophysical Research, 78(11), 1973
         https://doi.org/10.1029/JB078i011p01760
     T. Jacob et al., Journal of Geodesy, 86, 337-358, 2012
         https://doi.org/10.1007/s00190-011-0522-7
 
 UPDATE HISTORY:
+    Updated 06/2023: modified custom units case to not convert to cmwe
     Updated 03/2023: simplified unit degree factors using units class
+        improve typing for variables in docstrings
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 01/2023: refactored associated legendre polynomials
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: added UNITS option for converting from different inputs
     Updated 01/2021: use harmonics class for spherical harmonic operations
     Updated 07/2020: added function docstrings
@@ -81,47 +83,47 @@
 def gen_disc_load(data, lon, lat, area, LMAX=60, MMAX=None, UNITS=2,
     PLM=None, LOVE=None):
     """
     Calculates spherical harmonic coefficients for a uniform disc load
 
     Parameters
     ----------
-    data: float
+    data: np.ndarray
         data magnitude (Gt)
-    lon: float
+    lon: np.ndarray
         longitude of disc center
-    lat: float
+    lat: np.ndarray
         latitude of disc center
     area: float
         area of disc (km\ :sup:`2`)
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
     UNITS: int, default 2
         Input data units
 
             - ``1``: cm water equivalent thickness (cm w.e., g/cm\ :sup:`2`)
             - ``2``: gigatonnes of mass (Gt)
             - ``3``:  mm water equivalent thickness (mm w.e., kg/m\ :sup:`2`)
             - list: custom unit conversion factor
-    PLM: float or NoneType, default None
+    PLM: np.ndarray or NoneType, default None
         Legendre polynomials for ``cos(theta)`` (disc center)
     LOVE: tuple or NoneType, default None
         Load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients (geodesy normalization)
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients (geodesy normalization)
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
 
     References
     ----------
     .. [Holmes2002] S. A. Holmes and W. E. Featherstone,
         "A unified approach to the Clenshaw summation and the recursive
         computation of very high degree and order normalised associated
@@ -148,43 +150,49 @@
         MMAX = np.copy(LMAX)
 
     # convert lon and lat to radians
     phi = lon*np.pi/180.0# Longitude in radians
     th = (90.0 - lat)*np.pi/180.0# Colatitude in radians
 
     # Earth Parameters
-    factors = gravity_toolkit.units(lmax=LMAX).spatial(*LOVE)
+    factors = gravity_toolkit.units(lmax=LMAX)
 
     # convert input area into cm^2 and then divide by area of a half sphere
     # alpha will be 1 - the ratio of the input area with the half sphere
     alpha = (1.0 - 1e10*area/(2.0*np.pi*factors.rad_e**2))
 
     # Calculate factor to convert from input units into g/cm^2
     if isinstance(UNITS, (list, np.ndarray)):
         # custom units
-        unit_conv = np.copy(UNITS)
+        unit_conv = 1.0
+        dfactor = np.copy(UNITS)
     elif (UNITS == 1):
-        # Input data is in cm water equivalent (cmH2O)
+        # Input data is in cm water equivalent (cmwe)
         unit_conv = 1.0
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     elif (UNITS == 2):
         # Input data is in gigatonnes (Gt)
         # 1e15 converts from Gt to grams, 1e10 converts from km^2 to cm^2
         unit_conv = 1e15/(1e10*area)
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     elif (UNITS == 3):
         # Input data is in kg/m^2
         # 1 kg = 1000 g
         # 1 m^2 = 100*100 cm^2 = 1e4 cm^2
         unit_conv = 0.1
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     else:
         raise ValueError(f'Unknown units {UNITS}')
 
-    # calculate SH degree dependent factors to convert from coefficients
-    # of mass into normalized geoid coefficients
-    dfactor = 4.0*np.pi*factors.cmwe/(1.0 + 2.0*factors.l)
-
     # Calculating plms of the disc
     # allocating for constructed array
     pl_alpha = np.zeros((LMAX+1))
     # l=0 is a special case (P(-1) = 1, P(1) = cos(alpha))
     pl_alpha[0] = (1.0 - alpha)/2.0
     # for all other degrees: calculate the legendre polynomials up to LMAX+1
     pl_matrix,_ = legendre_polynomials(LMAX+1,alpha)
@@ -208,31 +216,31 @@
         # truncate precomputed plms to degree and order
         plmout = PLM[:LMAX+1,:MMAX+1]
 
     # calculate array of m values ranging from 0 to MMAX (harmonic orders)
     # MMAX+1 as there are MMAX+1 elements between 0 and MMAX
     m = np.arange(MMAX+1)
     # Multiplying by the units conversion factor (unit_conv) to
-    # convert from the input units into cmH2O equivalent
-    # Multiplying point mass data (converted to cmH2O) with sin/cos of m*phis
+    # convert from the input units into cmwe
+    # Multiplying point mass data (converted to cmwe) with sin/cos of m*phis
     # data normally is 1 for a uniform 1cm water equivalent layer
     # but can be a mass point if reconstructing a spherical harmonic field
     # NOTE: NOT a matrix multiplication as data (and phi) is a single point
     dcos = unit_conv*data*np.cos(m*phi)
     dsin = unit_conv*data*np.sin(m*phi)
 
     # Multiplying by plm_alpha (F_l from Jacob 2012)
-    plm = np.zeros((LMAX+1,MMAX+1))
+    plm = np.zeros((LMAX+1, MMAX+1))
     # Initializing preliminary spherical harmonic matrices
-    yclm = np.zeros((LMAX+1,MMAX+1))
-    yslm = np.zeros((LMAX+1,MMAX+1))
+    yclm = np.zeros((LMAX+1, MMAX+1))
+    yslm = np.zeros((LMAX+1, MMAX+1))
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     for m in range(0,MMAX+1):# MMAX+1 to include MMAX
         l = np.arange(m,LMAX+1)# LMAX+1 to include LMAX
         # rotate disc load to be centered at lat/lon
         plm[l,m] = plmout[l,m]*pl_alpha[l]
         # multiplying clm by cos(m*phi) and slm by sin(m*phi)
         # to get a field of spherical harmonics
         yclm[l,m] = plm[l,m]*dcos[m]
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_harmonics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gen_harmonics.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (03/2023)
 Converts data from the spatial domain to spherical harmonic coefficients
 Does not compute the solid Earth elastic response or convert units
 
 CALLING SEQUENCE:
     Ylms = gen_harmonics(data, lon, lat, LMIN=0, LMAX=60)
 
 INPUTS:
@@ -43,14 +43,15 @@
 
 REFERENCES:
     Holmes and Featherstone, "A Unified Approach to the Clenshaw Summation and
         the Recursive Computation of Very High Degree and Order Normalised
         Associated Legendre Functions", Journal of Geodesy (2002)
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 01/2023: refactored associated legendre polynomials
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 09/2021: merged integration and fourier harmonics programs
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 01/2021: use harmonics class for spherical harmonic operations
     Updated 07/2020: added function docstrings
     Updated 04/2020: include degrees and orders in output dictionary
@@ -67,42 +68,42 @@
 
 def gen_harmonics(data, lon, lat, **kwargs):
     """
     Converts data from the spatial domain to spherical harmonic coefficients
 
     Parameters
     ----------
-    data: float
+    data: np.ndarray
         data magnitude
-    lon: float
+    lon: np.ndarray
         longitude array
-    lat: float
+    lat: np.ndarray
         latitude array
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
-    PLM: float, default 0
+    PLM: np.ndarray, default 0
         Fully normalized associated Legendre polynomials or
         Fourier coefficients of Legendre polynomials
     METHOD: str, default 'integration'
         Conversion method for calculating harmonics
 
             - ``'integration'``: for global grids
             - ``'fourier'``: for regional or global grids
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients (4-pi normalized)
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients (4-pi normalized)
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
     """
     # set default keyword arguments
     kwargs.setdefault('LMAX',60)
     kwargs.setdefault('MMAX',None)
     kwargs.setdefault('PLM',0)
     kwargs.setdefault('METHOD','integration')
@@ -180,35 +181,35 @@
 
     # Multiplying sin(th) with differentials of theta and phi
     # to calculate the integration factor at each latitude
     int_fact = np.sin(th)*dphi*dth
     coeff = 1.0/(4.0*np.pi)
 
     # Calculate polynomials using Holmes and Featherstone (2002) relation
-    plm = np.zeros((LMAX+1,MMAX+1,nlat))
+    plm = np.zeros((LMAX+1, MMAX+1, nlat))
     if (np.ndim(PLM) == 0):
         plmout,dplm = plm_holmes(LMAX, np.cos(th))
     else:
         # use precomputed plms to improve computational speed
         # or to use a different recursion relation for polynomials
         plmout = PLM
 
     # Multiply plms by integration factors [sin(theta)*dtheta*dphi]
     # truncate plms to maximum spherical harmonic order if MMAX < LMAX
     m = np.arange(MMAX+1)
     for j in range(0,nlat):
         plm[:,m,j] = plmout[:,m,j]*int_fact[j]
 
     # Initializing preliminary spherical harmonic matrices
-    yclm = np.zeros((LMAX+1,MMAX+1))
-    yslm = np.zeros((LMAX+1,MMAX+1))
+    yclm = np.zeros((LMAX+1, MMAX+1))
+    yslm = np.zeros((LMAX+1, MMAX+1))
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     # Multiplying gridded data with sin/cos of m#phis (output [m,theta])
     # This will sum through all phis in the dot product
     dcos = np.dot(ccos,data)
     dsin = np.dot(ssin,data)
     for l in range(0,LMAX+1):
         mm = np.min([MMAX,l])# truncate to MMAX if specified (if l > MMAX)
         m = np.arange(0,mm+1)# mm+1 elements between 0 and mm
@@ -334,16 +335,16 @@
         plm = fourier_legendre(LMAX,MMAX)
     else:
         # use precomputed plms to improve computational speed
         plm = PLM
 
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
 
     # Sum theta fourier coefficients
     # temp is the integral of cos(n theta) cos(k theta) dcos(theta)
     # over the interval 0 to pi
     # n and k must have like parities
 
     # m = even terms
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_point_load.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_point_load.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gen_point_load.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (04/2023)
 Calculates gravitational spherical harmonic coefficients for point masses
 
 CALLING SEQUENCE:
     Ylms = gen_point_load(data, lon, lat, LMAX=LMAX)
 
 INPUTS:
     data: data magnitude
@@ -43,14 +43,16 @@
         https://doi.org/10.1029/JZ067i002p00845
     W. E. Farrell, Reviews of Geophysics and Space Physics, 10(3), 1972
         https://doi.org/10.1029/RG010i003p00761
     H. N. Pollack, Journal of Geophysical Research, 78(11), 1973
         https://doi.org/10.1029/JB078i011p01760
 
 UPDATE HISTORY:
+    Updated 04/2023: allow love numbers to be None for custom units case
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: added UNITS list option for converting from custom units
     Updated 01/2021: use harmonics class for spherical harmonic operations
     Updated 07/2020: added function docstrings
     Written 05/2020
@@ -62,19 +64,19 @@
 
 def gen_point_load(data, lon, lat, LMAX=60, MMAX=None, UNITS=1, LOVE=None):
     """
     Calculates spherical harmonic coefficients for point masses
 
     Parameters
     ----------
-    data: float
+    data: np.ndarray
         data magnitude
-    lon: float
+    lon: np.ndarray
         longitude of points
-    lat: float
+    lat: np.ndarray
         latitude of points
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
     UNITS: int, default 1
         Input data units
@@ -83,21 +85,21 @@
             - ``2``: gigatonnes of mass (Gt)
             - list: custom degree-dependent unit conversion factor
     LOVE: tuple or NoneType, default None
         Input load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
 
     References
     ----------
     .. [Longman1962] I. M. Longman, "A Green's function for determining
         the deformation of the Earth under surface mass loads: 1. Theory",
         *Journal of Geophysical Research*, 67(2), (1962).
@@ -117,40 +119,38 @@
 
     # number of input data points
     npts = len(data.flatten())
     # convert output longitude and latitude into radians
     phi = np.pi*lon.flatten()/180.0
     theta = np.pi*(90.0 - lat.flatten())/180.0
 
-    # SH Degree dependent factors to convert into fully normalized SH's
-    # use splat operator to extract arrays of kl, hl, and ll Love Numbers
-    factors = gravity_toolkit.units(lmax=LMAX).spatial(*LOVE)
     # extract degree dependent factor for specific units
+    factors = gravity_toolkit.units(lmax=LMAX)
     int_fact = np.zeros((npts))
     if isinstance(UNITS, (list, np.ndarray)):
         # custom units
         dfactor = np.copy(UNITS)
         int_fact[:] = 1.0
     elif (UNITS == 1):
         # Default Parameter: Input in grams (g)
-        dfactor = factors.cmwe/(factors.rad_e**2)
+        dfactor = factors.spatial(*LOVE).cmwe/(factors.rad_e**2)
         int_fact[:] = 1.0
     elif (UNITS == 2):
         # Input in gigatonnes (Gt)
-        dfactor = factors.cmwe/(factors.rad_e**2)
+        dfactor = factors.spatial(*LOVE).cmwe/(factors.rad_e**2)
         int_fact[:] = 1e15
     else:
         raise ValueError(f'Unknown units {UNITS}')
     # flattened form of data converted to units
     D = int_fact*data.flatten()
 
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     # for each degree l
     for l in range(LMAX+1):
         m1 = np.min([l,MMAX]) + 1
         SPH = spherical_harmonic_matrix(l, D, phi, theta, dfactor[l])
         # truncate to spherical harmonic order and save to output
         Ylms.clm[l,:m1] = SPH.real[:m1]
         Ylms.slm[l,:m1] = SPH.imag[:m1]
@@ -163,26 +163,26 @@
     Calculates the spherical harmonics for a particular degree
     evaluated from data at coordinates
 
     Parameters
     ----------
     l: int
         spherical harmonic degree
-    data: float
+    data: np.ndarray
         data magnitude in grams
-    phi: float
+    phi: np.ndarray
         longitude of points in radians
-    theta: float
+    theta: np.ndarray
         colatitude of points in radians
-    coeff: float
+    coeff: np.ndarray
         degree-dependent factor for converting units
 
     Returns
     -------
-    Ylms: float
+    Ylms: np.ndarray
         spherical harmonic coefficients in Eulerian form
     """
     # calculate normalized legendre polynomials (points, order)
     Pl = legendre(l, np.cos(theta), NORMALIZE=True).T
     # spherical harmonic orders up to degree l
     m = np.arange(0, l+1)
     # calculate Euler's of spherical harmonic order multiplied by azimuth phi
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_spherical_cap.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_spherical_cap.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,13 @@
 #!/usr/bin/env python
 u"""
 gen_spherical_cap.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (06/2023)
 Calculates gravitational spherical harmonic coefficients for a spherical cap
 
-Spherical cap derivation from Longman (1962), Farrell (1972), Pollack (1973)
-    and Jacob (2012)
-
 Creating a spherical cap with generating angle alpha is a 2 step process:
     1) obtain harmonics when cap is located at the north pole
     2) rotate the cap to an arbitrary latitude and longitude
 
 CALLING SEQUENCE:
     Ylms = gen_spherical_cap(data, lon, lat, LMAX=LMAX, RAD_CAP=RAD_CAP)
 
@@ -60,15 +57,17 @@
         https://doi.org/10.1029/RG010i003p00761
     H. N. Pollack, Journal of Geophysical Research, 78(11), 1973
         https://doi.org/10.1029/JB078i011p01760
     T. Jacob et al., Journal of Geodesy, 86, 337-358, 2012
         https://doi.org/10.1007/s00190-011-0522-7
 
 UPDATE HISTORY:
+    Updated 06/2023: modified custom units case to not convert to cmwe
     Updated 03/2023: simplified unit degree factors using units class
+        improve typing for variables in docstrings
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 01/2023: refactored associated legendre polynomials
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: added UNITS list option for converting from custom units
     Updated 07/2020: added function docstrings
     Updated 05/2020: vectorize calculation over degrees to improve compute time
@@ -102,47 +101,47 @@
 def gen_spherical_cap(data, lon, lat, LMAX=60, MMAX=None,
     AREA=0, RAD_CAP=0, RAD_KM=0, UNITS=1, PLM=None, LOVE=None):
     """
     Calculates spherical harmonic coefficients for a spherical cap
 
     Parameters
     ----------
-    data: float
+    data: np.ndarray
         data magnitude
-    lon: float
+    lon: np.ndarray
         longitude of spherical cap center
-    lat: float
+    lat: np.ndarray
         latitude of spherical cap center
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
     AREA: Float, default 0
         Area of spherical cap (cm\ :sup:`2`)
     UNITS: int, default 1
         Input data units
 
             - ``1``: cm water equivalent thickness (cm w.e., g/cm\ :sup:`2`)
             - ``2``: gigatonnes of mass (Gt)
             - ``3``: mm water equivalent thickness (mm w.e., kg/m\ :sup:`2`)
             - list: custom unit conversion factor
-    PLM: float, default 0
+    PLM: np.ndarray, default 0
         Input Legendre polynomials
     LOVE: tuple or NoneType, default None
         Input load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
 
     References
     ----------
     .. [Holmes2002] S. A. Holmes and W. E. Featherstone,
         "A unified approach to the Clenshaw summation and the recursive
         computation of very high degree and order normalised associated
@@ -169,15 +168,15 @@
         MMAX = np.copy(LMAX)
 
     # convert lon and lat to radians
     phi = lon*np.pi/180.0# Longitude in radians
     th = (90.0 - lat)*np.pi/180.0# Colatitude in radians
 
     # Earth Parameters
-    factors = gravity_toolkit.units(lmax=LMAX).spatial(*LOVE)
+    factors = gravity_toolkit.units(lmax=LMAX)
 
     # Converting input area into an equivalent spherical cap radius
     # Following Jacob et al. (2012) Equation 4 and 5
     # alpha is the vertical semi-angle subtending a cone at the
     # center of the earth
     if (RAD_CAP != 0):
         # if given spherical cap radius in degrees
@@ -192,41 +191,47 @@
     elif (RAD_KM != 0):
         # if given spherical cap radius in kilometers
         # Calculating angular radius of spherical cap
         alpha = (1e5*RAD_KM)/factors.rad_e
     else:
         raise ValueError('Input RAD_CAP, AREA or RAD_KM of spherical cap')
 
-    # Calculate factor to convert from input units into cmH2O equivalent
+    # Calculate factor to convert from input units
     if isinstance(UNITS, (list, np.ndarray)):
         # custom units
-        unit_conv = np.copy(UNITS)
+        unit_conv = 1.0
+        dfactor = np.copy(UNITS)
     elif (UNITS == 1):
-        # Input data is in cm water equivalent (cmH2O)
+        # Input data is in cm water equivalent (cmwe)
         unit_conv = 1.0
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     elif (UNITS == 2):
         # Input data is in gigatonnes (Gt)
         # calculate spherical cap area from angular radius
         area = np.pi*(alpha*factors.rad_e)**2
         # the 1.e15 converts from gigatons/cm^2 to cm of water
         # 1 g/cm^3 = 1000 kg/m^3 = density water
         # 1 Gt = 1 Pg = 1.e15 g
         unit_conv = 1.e15/area
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     elif (UNITS == 3):
         # Input data is in kg/m^2
         # 1 kg = 1000 g
         # 1 m^2 = 100*100 cm^2 = 1e4 cm^2
         unit_conv = 0.1
+        # degree dependent factors to convert from coefficients
+        # of mass into normalized geoid coefficients
+        dfactor = 4.0*np.pi*factors.spatial(*LOVE).cmwe/(1.0 + 2.0*factors.l)
     else:
         raise ValueError(f'Unknown units {UNITS}')
 
-    # calculate SH degree dependent factors to convert from coefficients
-    # of mass into normalized geoid coefficients
-    dfactor = 4.0*np.pi*factors.cmwe/(1.0 + 2.0*factors.l)
-
     # Calculating plms of the spherical caps
     # From Longman et al. (1962)
     # pl_alpha = F(alpha) from Jacob 2011
     # pl_alpha is purely zonal and depends only on the size of the cap
     # allocating for constructed array
     pl_alpha = np.zeros((LMAX+1))
     # l=0 is a special case (P(-1) = 1, P(1) = cos(alpha))
@@ -254,31 +259,31 @@
         # truncate precomputed plms to degree and order
         plmout = PLM[:LMAX+1,:MMAX+1]
 
     # calculate array of m values ranging from 0 to MMAX (harmonic orders)
     # MMAX+1 as there are MMAX+1 elements between 0 and MMAX
     m = np.arange(MMAX+1)
     # Multiplying by the units conversion factor (unit_conv) to
-    # convert from the input units into cmH2O equivalent
-    # Multiplying point mass data (converted to cmH2O) with sin/cos of m*phis
+    # convert from the input units into cmwe
+    # Multiplying point mass data (converted to cmwe) with sin/cos of m*phis
     # data normally is 1 for a uniform 1cm water equivalent layer
     # but can be a mass point if reconstructing a spherical harmonic field
     # NOTE: NOT a matrix multiplication as data (and phi) is a single point
     dcos = unit_conv*data*np.cos(m*phi)
     dsin = unit_conv*data*np.sin(m*phi)
 
     # Multiplying by plm_alpha (F_l from Jacob 2012)
-    plm = np.zeros((LMAX+1,MMAX+1))
+    plm = np.zeros((LMAX+1, MMAX+1))
     # Initializing preliminary spherical harmonic matrices
-    yclm = np.zeros((LMAX+1,MMAX+1))
-    yslm = np.zeros((LMAX+1,MMAX+1))
+    yclm = np.zeros((LMAX+1, MMAX+1))
+    yslm = np.zeros((LMAX+1, MMAX+1))
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     for m in range(0,MMAX+1):# MMAX+1 to include MMAX
         l = np.arange(m,LMAX+1)# LMAX+1 to include LMAX
         # rotate spherical cap to be centered at lat/lon
         plm[l,m] = plmout[l,m]*pl_alpha[l]
         # multiplying clm by cos(m*phi) and slm by sin(m*phi)
         # to get a field of spherical harmonics
         yclm[l,m] = plm[l,m]*dcos[m]
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/gen_stokes.py` & `gravity-toolkit-1.2.1/gravity_toolkit/gen_stokes.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gen_stokes.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (04/2023)
 
 Converts data from the spatial domain to spherical harmonic coefficients
 
 CALLING SEQUENCE:
     Ylms = gen_stokes(data, lon, lat, UNITS=1, LMIN=0, LMAX=60, LOVE=(hl,kl,ll))
 
 INPUTS:
@@ -39,14 +39,16 @@
     associated_legendre.py: computes fully-normalized associated Legendre polynomials
     units.py: class for converting spherical harmonic data to specific units
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
 
 UPDATE HISTORY:
+    Updated 04/2023: allow love numbers to be None for custom units case
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 01/2023: refactored associated legendre polynomials
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: added UNITS list option for converting from custom units
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 01/2021: use harmonics class for spherical harmonic operations
@@ -77,53 +79,53 @@
 def gen_stokes(data, lon, lat, LMIN=0, LMAX=60, MMAX=None, UNITS=1,
     PLM=None, LOVE=None):
     """
     Converts data from the spatial domain to spherical harmonic coefficients
 
     Parameters
     ----------
-    data: float
+    data: np.ndarray
         data matrix
-    lon: float
+    lon: np.ndarray
         longitude array
-    lat: float
+    lat: np.ndarray
         latitude array
     LMIN: int, default 0
         Lower bound of Spherical Harmonic Degrees
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
     UNITS: int, default 1
         Input data units
 
             - ``1``: cm water equivalent thickness (cm w.e., g/cm\ :sup:`2`)
             - ``2``: gigatonnes of mass (Gt)
             - ``3``:  mm water equivalent thickness (mm w.e., kg/m\ :sup:`2`)
             - list: custom degree-dependent unit conversion factor
-    PLM: float or NoneType, default None
+    PLM: np.ndarray or NoneType, default None
         Input Legendre polynomials
     LOVE: tuple or NoneType, default None
         Input load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
 
     References
     ----------
     .. [Wahr1998] J. Wahr, M. Molenaar, and F. Bryan, "Time
-        variabilityof the Earth's gravity field: Hydrological
+        variability of the Earth's gravity field: Hydrological
         and oceanic effects and their possible detection using GRACE",
         *Journal of Geophysical Research*, 103(B12), 30205-30229, (1998).
         `doi: 10.1029/98JB02844 <https://doi.org/10.1029/98JB02844>`_
     """
 
     # converting LMIN and LMAX to integer
     LMIN = np.int64(LMIN)
@@ -151,69 +153,66 @@
     # Colatitude in radians
     th = (90.0 - np.squeeze(lat.copy()))*np.pi/180.0
 
     # reforming data to lonXlat if input latXlon
     sz = np.shape(data)
     data = data.T if (sz[0] == nlat) else np.copy(data)
 
-    # SH Degree dependent factors to convert into fully normalized SH's
-    # use splat operator to extract arrays of kl, hl, and ll Love Numbers
-    factors = gravity_toolkit.units(lmax=LMAX).spatial(*LOVE)
-
     # extract degree dependent factor for specific units
     # calculate integration factors for theta and phi
     # Multiplying sin(th) with differentials of theta and phi
     # to calculate the integration factor at each latitude
+    factors = gravity_toolkit.units(lmax=LMAX)
     int_fact = np.zeros((nlat))
     if isinstance(UNITS, (list, np.ndarray)):
         # custom units
         dfactor = np.copy(UNITS)
         int_fact[:] = np.sin(th)*dphi*dth
     elif (UNITS == 1):
         # Default Parameter: Input in cm w.e. (g/cm^2)
-        dfactor = factors.cmwe
+        dfactor = factors.spatial(*LOVE).cmwe
         int_fact[:] = np.sin(th)*dphi*dth
     elif (UNITS == 2):
         # Input in gigatonnes (Gt)
-        dfactor = factors.cmwe
+        dfactor = factors.spatial(*LOVE).cmwe
         # rad_e: Average Radius of the Earth [cm]
         int_fact[:] = 1e15/(factors.rad_e**2)
     elif (UNITS == 3):
         # Input in kg/m^2 (mm w.e.)
-        dfactor = factors.mmwe
+        dfactor = factors.spatial(*LOVE).mmwe
         int_fact[:] = np.sin(th)*dphi*dth
     else:
         raise ValueError(f'Unknown units {UNITS}')
 
     # Calculating cos/sin of phi arrays
     # output [m,phi]
     m = np.arange(MMAX+1)
     ccos = np.cos(np.dot(m[:,np.newaxis],phi))
     ssin = np.sin(np.dot(m[:,np.newaxis],phi))
 
     # Calculating fully-normalized Legendre Polynomials
     # Output is plm[l,m,th]
-    plm = np.zeros((LMAX+1,MMAX+1,nlat))
+    plm = np.zeros((LMAX+1, MMAX+1, nlat))
     # added option to precompute plms to improve computational speed
     if PLM is None:
         # if plms are not pre-computed: calculate Legendre polynomials
         PLM, dPLM = plm_holmes(LMAX, np.cos(th))
 
     # Multiplying by integration factors [sin(theta)*dtheta*dphi]
     # truncate legendre polynomials to spherical harmonic order MMAX
     for j in range(0,nlat):
         plm[:,m,j] = PLM[:,m,j]*int_fact[j]
 
     # Initializing preliminary spherical harmonic matrices
-    yclm = np.zeros((LMAX+1,MMAX+1))
-    yslm = np.zeros((LMAX+1,MMAX+1))
+    yclm = np.zeros((LMAX+1, MMAX+1))
+    yslm = np.zeros((LMAX+1, MMAX+1))
     # Initializing output spherical harmonic matrices
     Ylms = gravity_toolkit.harmonics(lmax=LMAX, mmax=MMAX)
-    Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-    Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+    Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+    Ylms.slm = np.zeros((LMAX+1, MMAX+1))
     # Multiplying gridded data with sin/cos of m#phis
     # This will sum through all phis in the dot product
     # output [m,theta]
     dcos = np.dot(ccos,data)
     dsin = np.dot(ssin,data)
     for l in range(LMIN,LMAX+1):# equivalent to LMIN:LMAX
         mm = np.min([MMAX,l])# truncate to MMAX if specified (if l > MMAX)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/geocenter.py` & `gravity-toolkit-1.2.1/gravity_toolkit/geocenter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,28 @@
 #!/usr/bin/env python
 u"""
 geocenter.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 Data class for reading and processing geocenter data
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     PyYAML: YAML parser and emitter for Python
         https://github.com/yaml/pyyaml
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: convert shape and ndim to harmonic class properties
+        improve typing for variables in docstrings
+        set case insensitive filename to None if filename is empty
     Updated 02/2023: use monospaced text for geocenter objects in docstrings
     Updated 12/2022: make geocenter objects iterable and with length
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 06/2022: drop external reader dependency for UCI format
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 03/2022: add try/except for read_GRACE_geocenter
@@ -29,23 +33,23 @@
     Updated 07/2020: added function docstrings
     Updated 06/2019: added option RADIUS to manually set the Earth's radius
     Updated 04/2017: changed option from INV to INVERSE and made True/False
     Updated 04/2015: calculate radius of the Earth directly in program
     Updated 02/2014: minor update to if statement
     Updated 03/2013: converted to python
 """
-import os
 import re
 import io
 import copy
 import gzip
 import time
 import uuid
 import yaml
 import logging
+import pathlib
 import warnings
 import numpy as np
 import gravity_toolkit.time
 
 # attempt imports
 try:
     import netCDF4
@@ -57,29 +61,29 @@
 
 class geocenter(object):
     """
     Data class for reading and processing geocenter data
 
     Attributes
     ----------
-    C10: float
+    C10: np.ndarray
         cosine spherical harmonics of degree 1 and order 0
-    C11: float
+    C11: np.ndarray
         cosine spherical harmonics of degree 1 and order 1
-    S11: float
+    S11: np.ndarray
         sine spherical harmonics of degree 1 and order 1
-    X: float
+    X: np.ndarray
         X-component of Cartesian geocenter coordinates
-    Y: float
+    Y: np.ndarray
         Y-component of Cartesian geocenter coordinates
-    Z: float
+    Z: np.ndarray
         Z-component of Cartesian geocenter coordinates
-    time: float
+    time: np.ndarray
         time variable of the spherical harmonics
-    month: int
+    month: np.ndarray
         GRACE/GRACE-FO months variable of the spherical harmonics
     radius: float, default 6371000.790009159
         Average Radius of the Earth [mm]
     """
     np.seterr(invalid='ignore')
     def __init__(self, **kwargs):
         # WGS84 ellipsoid parameters
@@ -102,85 +106,85 @@
         self.month=None
         self.filename=None
         # Average Radius of the Earth [mm]
         self.radius=copy.copy(kwargs['radius'])
         # iterator
         self.__index__ = 0
 
-    def case_insensitive_filename(self,filename):
+    def case_insensitive_filename(self, filename):
         """
         Searches a directory for a filename without case dependence
 
         Parameters
         ----------
-        filename: str
+        filename: str, io.IOBase, pathlib.Path or None
             input filename
         """
         # check if filename is open file object
         if isinstance(filename, io.IOBase):
             self.filename = copy.copy(filename)
-        elif isinstance(filename, type(None)):
+        elif isinstance(filename, type(None)) or not bool(filename):
             self.filename = None
         else:
             # tilde-expand input filename
-            self.filename = os.path.expanduser(filename)
+            self.filename = pathlib.Path(filename).expanduser().absolute()
             # check if file presently exists with input case
-            if not os.access(self.filename,os.F_OK):
+            if not self.filename.exists():
                 # search for filename without case dependence
-                basename = os.path.basename(filename)
-                directory = os.path.dirname(os.path.expanduser(filename))
-                f = [f for f in os.listdir(directory) if re.match(basename,f,re.I)]
-                # check that geocenter file exists
+                f = [f.name for f in self.filename.parent.iterdir() if
+                    re.match(self.filename.name, f.name, re.I)]
                 if not f:
                     errmsg = f'{filename} not found in file system'
                     raise FileNotFoundError(errmsg)
-                self.filename = os.path.join(directory,f.pop())
+                self.filename = self.filename.with_name(f.pop())
         # print filename
         logging.debug(self.filename)
         return self
 
     # PURPOSE: read AOD1b geocenter for month and calculate the mean harmonics
     # need to run aod1b_geocenter.py to write these monthly geocenter files
-    def from_AOD1B(self, release, calendar_year, calendar_month):
+    def from_AOD1B(self, release, year, month, product='glo'):
         """
         Reads monthly non-tidal ocean and atmospheric variation geocenter files
 
         Parameters
         ----------
         release: str
             GRACE/GRACE-FO/Swarm data release for dealiasing product
-        calendar_year: int
+        year: int
             calendar year of data
-        calendar_month: int
+        month: int
             calendar month of data
+        product: str, default 'glo'
+            GRACE/GRACE-FO/Swarm dealiasing product
         """
 
         # full path to AOD geocenter for month (using glo coefficients)
-        args = (release,'glo',calendar_year,calendar_month)
-        AOD1B_file = 'AOD1B_{0}_{1}_{2:4.0f}_{3:02.0f}.txt'.format(*args)
+        granule = f'AOD1B_{release} {product}_{year:4.0f}_{month:02.0f}.txt'
+        AOD1B_file = self.directory.joinpath(granule)
         # check that file exists
-        if not os.access(os.path.join(self.directory,AOD1B_file), os.F_OK):
+        if not AOD1B_file.exists():
             errmsg = f'AOD1B File {AOD1B_file} not in File System'
             raise FileNotFoundError(errmsg)
         # read AOD1b geocenter skipping over commented header text
-        with open(os.path.join(self.directory,AOD1B_file), mode='r', encoding='utf8') as f:
+        with AOD1B_file.open(mode='r', encoding='utf8') as f:
             file_contents=[i for i in f.read().splitlines() if not re.match(r'#',i)]
         # extract X,Y,Z from each line in the file
         n_lines = len(file_contents)
         temp = geocenter()
         temp.X = np.zeros((n_lines))
         temp.Y = np.zeros((n_lines))
         temp.Z = np.zeros((n_lines))
         for i,line in enumerate(file_contents):
             line_contents = line.split()
             # first column: ISO-formatted date and time
             cal_date = time.strptime(line_contents[0],r'%Y-%m-%dT%H:%M:%S')
             # verify that dates are within year and month
-            assert (cal_date.tm_year == calendar_year)
-            assert (cal_date.tm_mon == calendar_month)
+            assert (cal_date.tm_year == year)
+            assert (cal_date.tm_mon == month)
             # second-fourth columns: X, Y and Z geocenter variations
             temp.X[i],temp.Y[i],temp.Z[i] = np.array(line_contents[1:],dtype='f')
         # convert X,Y,Z into spherical harmonics
         temp.from_cartesian()
         # return the spherical harmonic coefficients
         return temp
 
@@ -221,15 +225,15 @@
         # Column  6: Coefficient C(1,1)
         # Column  7: Coefficient C(1,1) - mean C(1,1) (1.0E-10)
         # Column  8: C(1,1) uncertainty (1.0E-10)
         # Column  9: Coefficient S(1,1)
         # Column 10: Coefficient S(1,1) - mean S(1,1) (1.0E-10)
         # Column 11: S(1,1) uncertainty (1.0E-10)
 
-        with open(self.filename, mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -243,15 +247,15 @@
 
         # output dictionary with spherical harmonic solutions
         dinput = {}
         # number of months within the file
         n_mon = file_lines - count
         # date and GRACE/GRACE-FO month
         dinput['time'] = np.zeros((n_mon))
-        dinput['month'] = np.zeros((n_mon),dtype=int)
+        dinput['month'] = np.zeros((n_mon), dtype=int)
         # monthly spherical harmonic replacement solutions
         dinput['C10'] = np.zeros((n_mon))
         dinput['C11'] = np.zeros((n_mon))
         dinput['S11'] = np.zeros((n_mon))
         # monthly spherical harmonic formal standard deviations
         dinput['eC10'] = np.zeros((n_mon))
         dinput['eC11'] = np.zeros((n_mon))
@@ -293,15 +297,14 @@
         # For all: May 2015 (161) is centered in Apr 2015 (160)
         # For GSFC: Oct 2018 (202) is centered in Nov 2018 (203)
         dinput['month'] = gravity_toolkit.time.adjust_months(dinput['month'])
 
         # return the GFZ GravIS geocenter solutions
         return self.from_dict(dinput)
 
-
     def from_SLR(self, geocenter_file, **kwargs):
         """
         Reads monthly geocenter files from satellite laser ranging corrected
         for non-tidal ocean and atmospheric variation
 
         Reads monthly geocenter files from `satellite laser ranging
         provided by CSR <http://download.csr.utexas.edu/pub/slr/geocenter/>`_
@@ -348,24 +351,23 @@
         kwargs.setdefault('release',None)
         # copy keyword arguments to variables
         COLUMNS = copy.copy(kwargs['columns'])
         HEADER = copy.copy(kwargs['header'])
 
         # directory setup for AOD1b data starting with input degree 1 file
         # this will verify that the input paths work
-        base_dir = os.path.join(os.path.dirname(self.filename),os.path.pardir)
-        self.directory = os.path.abspath(os.path.join(base_dir,'AOD1B',
-            kwargs['release'],'geocenter'))
+        base_dir = self.filename.parent.parent
+        self.directory = base_dir.joinpath('AOD1B', kwargs['release'], 'geocenter')
         # check that AOD1B directory exists
-        if not os.access(self.directory, os.F_OK):
-            errmsg = f'{self.directory} not found in file system'
+        if not self.directory.exists():
+            errmsg = f'{str(self.directory)} not found in file system'
             raise FileNotFoundError(errmsg)
 
         # Input geocenter file and split lines
-        with open(os.path.expanduser(geocenter_file), mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         ndate = len(file_contents) - HEADER
 
         # compile regular expression operator to find numerical instances
         regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?'
         rx = re.compile(regex_pattern, re.VERBOSE)
 
@@ -412,19 +414,19 @@
             self.eC10[t] = np.copy(temp.C10)
             self.eC11[t] = np.copy(temp.C11)
             self.eS11[t] = np.copy(temp.S11)
 
             # Calculation of the Julian date from calendar date
             JD[t] = gravity_toolkit.time.calendar_to_julian(self.time[t])
             # convert the julian date into calendar dates
-            YY,MM,DD,hh,mm,ss = gravity_toolkit.time.convert_julian(JD[t],
+            YY, MM, DD, hh, mm, ss = gravity_toolkit.time.convert_julian(JD[t],
                 FORMAT='tuple')
             # calculate the GRACE/GRACE-FO month (Apr02 == 004)
             # https://grace.jpl.nasa.gov/data/grace-months/
-            self.month[t] = gravity_toolkit.time.calendar_to_grace(YY,month=MM)
+            self.month[t] = gravity_toolkit.time.calendar_to_grace(YY, month=MM)
 
             # if removing the Atmospheric and Oceanic dealiasing
             if kwargs['AOD']:
                 # read the AOD1B file for the month and year
                 temp = self.from_AOD1B(kwargs['release'], YY, MM)
                 # remove the monthly mean AOD
                 self.C10[t] -= np.mean(temp.C10)
@@ -463,15 +465,15 @@
             estimates of geocenter variability from time-variable gravity
             and ocean model outputs", *Remote Sensing*, 11(18), 2108, (2019).
             `doi: 10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
         """
         # set filename
         self.case_insensitive_filename(geocenter_file)
         # read geocenter file and get contents
-        with open(os.path.expanduser(geocenter_file), mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         HEADER = False
         count = 0
@@ -564,15 +566,15 @@
         """
         # set filename
         self.case_insensitive_filename(geocenter_file)
         # set default keyword arguments
         kwargs.setdefault('header',True)
 
         # read degree 1 file and get contents
-        with open(self.filename, mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -644,16 +646,15 @@
 
     def from_tellus(self, geocenter_file, **kwargs):
         """
         Reads monthly geocenter spherical harmonic data files from GRACE Tellus
         Technical Notes (TN-13) calculated using GRACE/GRACE-FO measurements and
         Ocean Models of Degree 1
 
-        `Datasets distributed by NASA PO.DAAC
-        <https://podaac-tools.jpl.nasa.gov/drive/files/allData/tellus/L2/degree_1>`_
+        Datasets distributed by NASA PO.DAAC
 
         Parameters
         ----------
         geocenter_file: str
             degree 1 file
 
                 - ``CSR``: TN-13_GEOC_CSR_RL06.txt
@@ -686,15 +687,15 @@
         # set filename
         self.case_insensitive_filename(geocenter_file)
         # set default keyword arguments
         kwargs.setdefault('header',True)
         kwargs.setdefault('JPL',True)
 
         # read degree 1 file and get contents
-        with open(self.filename, mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_contents = f.read().splitlines()
         # number of lines contained in the file
         file_lines = len(file_contents)
 
         # counts the number of lines in the header
         count = 0
         # Reading over header text
@@ -808,23 +809,23 @@
         """
         kwargs.setdefault('compression',None)
         # set filename
         self.case_insensitive_filename(geocenter_file)
         # Open the netCDF4 file for reading
         if (kwargs['compression'] == 'gzip'):
             # read gzipped file as in-memory (diskless) netCDF4 dataset
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 fileID = netCDF4.Dataset(uuid.uuid4().hex,
                     memory=f.read())
         elif (kwargs['compression'] == 'bytes'):
             # read as in-memory (diskless) netCDF4 dataset
             fileID = netCDF4.Dataset(uuid.uuid4().hex,
                 memory=self.filename.read())
         else:
-            fileID = netCDF4.Dataset(self.filename, 'r')
+            fileID = netCDF4.Dataset(self.filename, mode='r')
         # Getting the data from each netCDF4 variable
         DEG1 = {}
         # converting netCDF4 objects into numpy arrays
         for key,val in fileID.variables.items():
             DEG1[key] = val[:].copy()
         # close the netCDF4 file
         fileID.close()
@@ -856,15 +857,15 @@
 
     def from_dict(self, temp, **kwargs):
         """
         Convert a dictionary object to a ``geocenter`` object
 
         Parameters
         ----------
-        temp: obj
+        temp: dict
             dictionary object to be converted
         fields: list
             default keys in dictionary
         """
         # set default keyword arguments
         kwargs.setdefault('fields',['time','month',
             'C10','C11','S11','eC10','eC11','eS11',
@@ -884,15 +885,15 @@
         Parameters
         ----------
         temp: obj
             ``harmonics`` object to be converted
         fields: list
             default keys in ``harmonics`` object
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # set default keyword arguments
         kwargs.setdefault('fields',['time','month','filename'])
         # try to assign variables to self
         for key in kwargs['fields']:
             try:
                 val = getattr(temp, key)
@@ -913,17 +914,17 @@
 
     def from_matrix(self, clm, slm):
         """
         Converts spherical harmonic matrices to a ``geocenter`` object
 
         Parameters
         ----------
-        clm: float
+        clm: np.ndarray
             cosine spherical harmonics of degree 1
-        slm: float
+        slm: np.ndarray
             sine spherical harmonics of degree 1
         """
         # verify dimensions
         clm = np.atleast_3d(clm)
         slm = np.atleast_3d(slm)
         # output geocenter object
         self.C10 = np.copy(clm[1,0,:])
@@ -933,15 +934,15 @@
 
     def to_dict(self, **kwargs):
         """
         Convert a ``geocenter`` object to a dictionary object
 
         Parameters
         ----------
-        fields: obj
+        fields: list
             default attributes in ``geocenter`` object
         """
         # output dictionary
         temp = {}
         # set default keyword arguments
         kwargs.setdefault('fields',['time','month',
             'C10','C11','S11','eC10','eC11','eS11',
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/grace_date.py` & `gravity-toolkit-1.2.1/gravity_toolkit/grace_date.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 #!/usr/bin/env python
 u"""
 grace_date.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte and Yara Mohajerani
 
-Reads index file from podaac_grace_sync.py or gfz_isdc_grace_ftp.py
+Reads index file from podaac_cumulus.py or gfz_isdc_grace_ftp.py
 Parses dates of each GRACE/GRACE-FO file and assigns the month number
 Creates an index of dates for GRACE/GRACE-FO files
 
 INPUTS:
     base_dir: Working data directory for GRACE/GRACE-FO data
 
 OPTIONS:
@@ -42,15 +42,18 @@
     future: Compatibility layer between Python 2 and Python 3
         https://python-future.org/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
-    Updated 11/2022: use f-strings for formatting verbose or ascii output
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: use f-strings for formatting output date lines
+        added regex formatting for CNES GRGS harmonics
+    Updated 11/2022: use f-strings for formatting verbose output
     Updated 09/2022: raise exception if index file cannot be found
         use logging for debugging level verbose output
     Updated 08/2022: moved file parsing functions to time module
     Updated 05/2022: use argparse descriptions within documentation
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 09/2021: adjust regular expression operators for Swarm and GRAZ
@@ -97,16 +100,16 @@
     Updated 06/2012: fixes missing dates to be automatic
         Also added options to enter the processing center, the data release and
         the dataset from an external main level program
     Updated 04/2012: changes for RL05 data
 """
 from __future__ import print_function
 
-import os
 import logging
+import pathlib
 import argparse
 import numpy as np
 import gravity_toolkit.time
 
 # PURPOSE: parses GRACE/GRACE-FO data files and assigns month numbers
 def grace_date(base_dir, PROC='', DREL='', DSET='', OUTPUT=True, MODE=0o775):
     """
@@ -126,14 +129,15 @@
             - ``'CSR'``: University of Texas Center for Space Research
             - ``'GFZ'``: German Research Centre for Geosciences (GeoForschungsZentrum)
             - ``'JPL'``: Jet Propulsion Laboratory
             - ``'CNES'``: French Centre National D'Etudes Spatiales
             - ``'GRAZ'``: Institute of Geodesy from GRAZ University of Technology
             - ``'COSTG'``: Combination Service for Time-variable Gravity Fields
             - ``'Swarm'``: Time-variable gravity data from Swarm satellites
+            - ``'GRGS'``: CNES Groupe de Recherche de Geodesie Spatiale
     DREL: str, default ''
         GRACE/GRACE-FO/Swarm data release
     DSET: str, default ''
         GRACE/GRACE-FO/Swarm dataset
 
             - ``'GAA'``: non-tidal atmospheric correction
             - ``'GAB'``: non-tidal oceanic correction
@@ -148,38 +152,39 @@
     Returns
     -------
     output_files: dict
         dictionary of GRACE/GRACE-FO files indexed by month
     """
 
     #  Directory of exact product
-    grace_dir = os.path.join(base_dir, PROC, DREL, DSET)
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath(PROC, DREL, DSET)
     # index file containing GRACE/GRACE-FO data filenames
-    index_file = os.path.join(grace_dir, 'index.txt')
+    index_file = grace_dir.joinpath('index.txt')
     # check that index file exists
-    if not os.access(index_file, os.F_OK):
-        raise FileNotFoundError(f'{index_file} not found')
+    if not index_file.exists():
+        raise FileNotFoundError(f'{str(index_file)} not found')
     # log index file if debugging
-    logging.debug(f'Reading index file: {index_file}')
+    logging.debug(f'Reading index file: {str(index_file)}')
     # read index file for GRACE/GRACE-FO filenames
-    with open(index_file, mode='r', encoding='utf8') as f:
+    with index_file.open(mode='r', encoding='utf8') as f:
         input_files = f.read().splitlines()
 
     #  number of lines in input_files
     n_files = len(input_files)
 
     # define date variables
     start_yr = np.zeros((n_files))# year start date
     end_yr = np.zeros((n_files))# year end date
     start_day = np.zeros((n_files))# day number start date
     end_day = np.zeros((n_files))# day number end date
     mid_day = np.zeros((n_files))# mid-month day
     tot_days = np.zeros((n_files))# number of days since Jan 2002
-    tdec = np.zeros((n_files))# tdec is the date in decimal form
-    mon = np.zeros((n_files,),dtype=np.int64)# GRACE/GRACE-FO month number
+    tdec = np.zeros((n_files))# date in decimal form
+    mon = np.zeros((n_files,), dtype=np.int64)# GRACE/GRACE-FO month number
 
     # for each data file
     for t,infile in enumerate(input_files):
         if PROC in ('GRAZ','Swarm',):
             # get date lists for the start and end of fields
             start_date,end_date = gravity_toolkit.time.parse_gfc_file(
                 infile, PROC, DSET)
@@ -249,53 +254,53 @@
     # For CSR and GFZ: Nov 2011 (119) is centered in Oct 2011 (118)
     # For JPL: Dec 2011 (120) is centered in Jan 2012 (121)
     # For all: May 2015 (161) is centered in Apr 2015 (160)
     mon = gravity_toolkit.time.adjust_months(mon)
 
     # Output GRACE/GRACE-FO date ascii file
     if OUTPUT:
-        date_file = f'{PROC}_{DREL}_DATES.txt'
-        fid = open(os.path.join(grace_dir,date_file), mode='w', encoding='utf8')
+        grace_date_file = grace_dir.joinpath(f'{PROC}_{DREL}_DATES.txt')
+        fid = grace_date_file.open(mode='w', encoding='utf8')
         # date file header information
         args = ('Mid-date','Month','Start_Day','End_Day','Total_Days')
         print('{0} {1:>10} {2:>11} {3:>10} {4:>13}'.format(*args),file=fid)
 
     # create python dictionary mapping input file names with GRACE months
     grace_files = {}
     # for each data file
     for t, infile in enumerate(input_files):
         # add file to python dictionary mapped to GRACE/GRACE-FO month
-        grace_files[mon[t]] = os.path.join(grace_dir,infile)
+        grace_files[mon[t]] = grace_dir.joinpath(infile)
         # print to GRACE dates ascii file (NOTE: tot_days will be rounded)
         if OUTPUT:
-            print(('{0:13.8f} {1:03d} {2:8.0f} {3:03.0f} {4:8.0f} {5:03.0f} '
-                '{6:8.0f}').format(tdec[t],mon[t],start_yr[t],start_day[t],
-                end_yr[t],end_day[t],tot_days[t]), file=fid)
+            print((f'{tdec[t]:13.8f} {mon[t]:03d} '
+                f'{start_yr[t]:8.0f} {start_day[t]:03.0f} '
+                f'{end_yr[t]:8.0f} {end_day[t]:03.0f} '
+                f'{tot_days[t]:8.0f}'), file=fid)
 
     # close date file
     # set permissions level of output date file
     if OUTPUT:
         fid.close()
-        os.chmod(os.path.join(grace_dir, date_file), MODE)
+        grace_date_file.chmod(mode=MODE)
 
     # return the python dictionary that maps GRACE months with GRACE files
     return grace_files
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Parses dates of each GRACE/GRACE-FO file and
             assigns the month number.
             Creates an index of dates for GRACE/GRACE-FO files.
             """
     )
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/grace_find_months.py` & `gravity-toolkit-1.2.1/gravity_toolkit/grace_find_months.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 grace_find_months.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (05/2023)
 
 Parses date index file from grace_date program
 Finds the months available for a GRACE/GRACE-FO/Swarm product
 Finds the all months missing from the product
 
 INPUTS:
     base_dir: Working data directory for GRACE/GRACE-FO data
@@ -32,30 +32,30 @@
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 PROGRAM DEPENDENCIES:
     grace_date.py: reads GRACE index file and calculates dates for each month
 
 UPDATE HISTORY:
+    Updated 05/2023: use formatting for reading from date file
+        use pathlib to define and operate on paths
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 07/2020: added function docstrings
     Updated 03/2020: check that GRACE/GRACE-FO date file exists
     Updated 10/2019: using local() function to set subdirectories
     Updated 08/2018: using full release string (RL05 instead of 5)
     Updated 05/2016: using __future__ print function
     Updated 11/2015: simplified to use sets for find missing months
     Updated 09/2014: add CNES versions as RL03 is monthly data
     Updated 09/2013: missing periods for for CNES
     Written 05/2013
 """
-from __future__ import print_function
-
-import os
+import pathlib
 import numpy as np
 from gravity_toolkit.grace_date import grace_date
 
 def grace_find_months(base_dir, PROC, DREL, DSET='GSM'):
     """
     Parses date index file
 
@@ -100,33 +100,42 @@
     months: list
         all available months in a GRACE/GRACE-FO dataset
     time: list
         center dates of all available months in a GRACE/GRACE-FO dataset
     """
 
     #  Directory of exact product (using date index from GSM)
-    grace_dir = os.path.join(base_dir, PROC, DREL, DSET)
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath(PROC, DREL, DSET)
 
     # check that GRACE/GRACE-FO date file exists
-    date_file = os.path.join(grace_dir, f'{PROC}_{DREL}_DATES.txt')
-    if not os.access(date_file, os.F_OK):
+    grace_date_file = grace_dir.joinpath(f'{PROC}_{DREL}_DATES.txt')
+    if not grace_date_file.exists():
         grace_date(base_dir, PROC=PROC, DREL=DREL, DSET=DSET, OUTPUT=True)
 
-    # read GRACE/GRACE-FO date ascii file from grace_date.py
+    # names and formats of GRACE/GRACE-FO date ascii file
+    names = ('t','mon','styr','stday','endyr','endday','total')
+    formats = ('f','i','i','i','i','i','i')
+    dtype = np.dtype({'names':names, 'formats':formats})
+    # read GRACE/GRACE-FO date ascii file
     # skip the header row and extract dates (decimal format) and months
-    date_input = np.loadtxt(date_file, skiprows=1)
-    tdec = date_input[:,0]
-    months = date_input[:,1].astype(np.int64)
+    date_input = np.loadtxt(grace_date_file, skiprows=1, dtype=dtype)
+    # date info dictionary
+    var_info = {}
+    var_info['time'] = date_input['t']
+    var_info['months'] = date_input['mon']
+    var_info['start'] = np.min(date_input['mon'])
+    var_info['end'] = np.max(date_input['mon'])
 
     # array of all possible months (or in case of CNES RL01/2: 10-day sets)
-    all_months = np.arange(1,months.max(),dtype=np.int64)
+    all_months = np.arange(1, var_info['end'], dtype=np.int64)
     # missing months (values in all_months but not in months)
-    missing = sorted(set(all_months)-set(months))
+    var_info['missing'] = sorted(set(all_months) - set(date_input['mon']))
     # If CNES RL01/2: simply convert into numpy array
     # else: remove months 1-3 and convert into numpy array
     if ((PROC == 'CNES') & (DREL in ('RL01','RL02'))):
-        missing = np.array(missing,dtype=np.int64)
+        var_info['missing'] = np.array(var_info['missing'], dtype=np.int64)
     else:
-        missing = np.array(missing[3:],dtype=np.int64)
+        var_info['missing'] = np.array(var_info['missing'][3:], dtype=np.int64)
 
-    return {'time':tdec, 'start':months[0], 'end':months[-1], 'months':months,
-        'missing':missing}
+    # return the date information dictionary
+    return var_info
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/grace_input_months.py` & `gravity-toolkit-1.2.1/gravity_toolkit/grace_input_months.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 grace_input_months.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte and Yara Mohajerani
 
 Reads GRACE/GRACE-FO files for a specified spherical harmonic degree and order
     and for a specified date range
 Includes degree 1 with with input values (if specified)
 Replaces C20 with SLR values (if specified)
 Replaces C21/S21/C22/S22/C30/C50 with SLR values for months 179+ (if specified)
@@ -105,15 +105,18 @@
     SLR.C40.py: reads C40 files from satellite laser ranging (CSR or GSFC)
     SLR.C50.py: reads C50 files from satellite laser ranging (CSR or GSFC)
     geocenter.py: data class for reading and processing geocenter data
     read_GRACE_harmonics.py: read spherical harmonic data from SHM files
     read_gfc_harmonics.py: reads spherical harmonic data from gfc files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: use release-03 GFZ GravIS SLR and geocenter files
     Updated 03/2023: added attributes for input files and corrections
+        improve typing for variables in docstrings
     Updated 01/2023: refactored satellite laser ranging read functions
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 10/2022: tilde-expansion of input working data directory
     Updated 09/2022: use logging for debugging level verbose output
         add option to replace degree 4 zonal harmonics with SLR
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 12/2021: option to specify a specific geocenter correction file
@@ -163,19 +166,19 @@
     Updated 09/2013: added option to least-squares model the degree 1
         coefficients for missing months
     Updated 07/2013: can use different geocenter solutions
     Written 05/2013
 """
 from __future__ import print_function, division
 
-import os
 import re
 import gzip
 import copy
 import logging
+import pathlib
 import collections
 import numpy as np
 import gravity_toolkit.geocenter
 import gravity_toolkit.SLR
 from gravity_toolkit.grace_date import grace_date
 from gravity_toolkit.read_GRACE_harmonics import read_GRACE_harmonics
 from gravity_toolkit.read_gfc_harmonics import read_gfc_harmonics
@@ -289,29 +292,29 @@
     DEG1_FILE: str or NoneType, default None
         full path to degree 1 coefficients file
     MODEL_DEG1: bool, default False
         least-squares model missing degree 1 coefficients
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonics to degree/order ``LMAX`` and ``MMAX``
-    slm: float
+    slm: np.ndarray
         sine spherical harmonics to degree/order ``LMAX`` and ``MMAX``
-    eclm: float
+    eclm: np.ndarray
         uncalibrated cosine spherical harmonic errors
-    eslm: float
+    eslm: np.ndarray
         uncalibrated sine spherical harmonic errors
-    time: float
+    time: np.ndarray
         time of each measurement (mid-month)
-    month: int
+    month: np.ndarray
         GRACE/GRACE-FO months of input datasets
-    l: int
+    l: np.ndarray
         spherical harmonic degree to ``LMAX``
-    m: int
+    m: np.ndarray
         spherical harmonic order to ``MMAX``
     title: str
         Processing string denoting low degree zonals
         replacement, geocenter usage and corrections
     directory: str
         Directory of exact GRACE/GRACE-FO/Swarm product
     attributes: dict
@@ -359,36 +362,37 @@
     kwargs.setdefault('SLR_C50','')
     kwargs.setdefault('DEG1_FILE',None)
     kwargs.setdefault('MODEL_DEG1',False)
     kwargs.setdefault('ATM',False)
     kwargs.setdefault('POLE_TIDE',False)
 
     # directory of exact GRACE/GRACE-FO product
-    grace_dir = os.path.join(os.path.expanduser(base_dir), PROC, DREL, DSET)
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath(PROC, DREL, DSET)
     # check that GRACE/GRACE-FO product directory exists
-    if not os.access(grace_dir, os.F_OK):
-        raise FileNotFoundError(grace_dir)
+    if not grace_dir.exists():
+        raise FileNotFoundError(str(grace_dir))
 
     # upper bound of spherical harmonic orders (default = LMAX)
     MMAX = kwargs.get('MMAX') or np.copy(LMAX)
 
     # Range of months from start_mon to end_mon (end_mon+1 to include end_mon)
     # Removing the missing months and months not to consider
-    months = sorted(set(np.arange(start_mon,end_mon+1)) - set(missing))
+    months = sorted(set(np.arange(start_mon, end_mon+1)) - set(missing))
     # number of months to consider in analysis
     n_cons = len(months)
 
     # Initializing input data matrices
     grace_Ylms = {}
-    grace_Ylms['clm'] = np.zeros((LMAX+1,MMAX+1,n_cons))
-    grace_Ylms['slm'] = np.zeros((LMAX+1,MMAX+1,n_cons))
-    grace_Ylms['eclm'] = np.zeros((LMAX+1,MMAX+1,n_cons))
-    grace_Ylms['eslm'] = np.zeros((LMAX+1,MMAX+1,n_cons))
+    grace_Ylms['clm'] = np.zeros((LMAX+1, MMAX+1, n_cons))
+    grace_Ylms['slm'] = np.zeros((LMAX+1, MMAX+1, n_cons))
+    grace_Ylms['eclm'] = np.zeros((LMAX+1, MMAX+1, n_cons))
+    grace_Ylms['eslm'] = np.zeros((LMAX+1, MMAX+1, n_cons))
     grace_Ylms['time'] = np.zeros((n_cons))
-    grace_Ylms['month'] = np.zeros((n_cons),dtype=np.int64)
+    grace_Ylms['month'] = np.zeros((n_cons), dtype=np.int64)
     # output dimensions
     grace_Ylms['l'] = np.arange(LMAX+1)
     grace_Ylms['m'] = np.arange(MMAX+1)
 
     # attributes for processing run
     attributes = collections.OrderedDict()
     # input GRACE/GRACE-FO and correction files
@@ -403,298 +407,298 @@
     )
 
     # importing data from GRACE/GRACE-FO files
     for i,grace_month in enumerate(months):
         # read spherical harmonic data products
         infile = grace_files[grace_month]
         # log input file if debugging
-        logging.debug(f'Reading file {i:d}: {infile}')
+        logging.debug(f'Reading file {i:d}: {str(infile)}')
         # read GRACE/GRACE-FO/Swarm file
         if PROC in ('GRAZ','Swarm'):
             # Degree 2 zonals will be converted to a tide free state
             Ylms = read_gfc_harmonics(infile, TIDE='tide_free')
         else:
             # Effects of Pole tide drift will be compensated if specified
             Ylms = read_GRACE_harmonics(infile, LMAX, MMAX=MMAX,
                 POLE_TIDE=kwargs['POLE_TIDE'])
         # truncate harmonics to degree and order
-        grace_Ylms['clm'][:,:,i] = Ylms['clm'][0:LMAX+1,0:MMAX+1]
-        grace_Ylms['slm'][:,:,i] = Ylms['slm'][0:LMAX+1,0:MMAX+1]
+        grace_Ylms['clm'][:,:,i] = Ylms['clm'][0:LMAX+1, 0:MMAX+1]
+        grace_Ylms['slm'][:,:,i] = Ylms['slm'][0:LMAX+1, 0:MMAX+1]
         # truncate harmonic errors to degree and order
-        grace_Ylms['eclm'][:,:,i] = Ylms['eclm'][0:LMAX+1,0:MMAX+1]
-        grace_Ylms['eslm'][:,:,i] = Ylms['eslm'][0:LMAX+1,0:MMAX+1]
+        grace_Ylms['eclm'][:,:,i] = Ylms['eclm'][0:LMAX+1, 0:MMAX+1]
+        grace_Ylms['eslm'][:,:,i] = Ylms['eslm'][0:LMAX+1, 0:MMAX+1]
         # copy date variables
         grace_Ylms['time'][i] = np.copy(Ylms['time'])
         grace_Ylms['month'][i] = np.int64(grace_month)
         # copy input file basename
-        attributes['lineage'][i] = os.path.basename(infile)
+        attributes['lineage'][i] = pathlib.Path(infile).stem
 
     # single accelerometer months
     single_acc_months = np.copy(grace_Ylms['month'][grace_Ylms['month'] > 176])
 
     # SLR low-degree harmonic, geocenter and correction flags
     FLAGS = []
 
     # Replacing C2,0 with SLR values
     if (SLR_C20 == 'CSR'):
         if (DREL == 'RL04'):
-            SLR_file = os.path.join(base_dir,'TN-05_C20_SLR.txt')
+            SLR_file = base_dir.joinpath('TN-05_C20_SLR.txt')
         elif (DREL == 'RL05'):
-            SLR_file = os.path.join(base_dir,'TN-07_C20_SLR.txt')
+            SLR_file = base_dir.joinpath('TN-07_C20_SLR.txt')
         elif (DREL == 'RL06'):
-            # SLR_file = os.path.join(base_dir,'TN-11_C20_SLR.txt')
-            SLR_file = os.path.join(base_dir,'C20_RL06.txt')
+            # SLR_file = base_dir.joinpath('TN-11_C20_SLR.txt')
+            SLR_file = base_dir.joinpath('C20_RL06.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C20 file: {SLR_file}')
+        logging.debug(f'Reading SLR C20 file: {str(SLR_file)}')
         # read SLR file
         C20_input = gravity_toolkit.SLR.C20(SLR_file)
         FLAGS.append('_wCSR_C20')
-        attributes['SLR C20'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR C20'] = ('CSR', SLR_file.name)
     elif (SLR_C20 == 'GFZ'):
-        SLR_file = os.path.join(base_dir,f'GFZ_{DREL}_C20_SLR.dat')
+        SLR_file = base_dir.joinpath(f'GFZ_{DREL}_C20_SLR.dat')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C20 file: {SLR_file}')
+        logging.debug(f'Reading SLR C20 file: {str(SLR_file)}')
         # read SLR file
         C20_input = gravity_toolkit.SLR.C20(SLR_file)
         FLAGS.append('_wGFZ_C20')
-        attributes['SLR C20'] = ('GFZ',os.path.basename(SLR_file))
+        attributes['SLR C20'] = ('GFZ', SLR_file.name)
     elif (SLR_C20 == 'GSFC'):
-        SLR_file = os.path.join(base_dir,'TN-14_C30_C20_GSFC_SLR.txt')
+        SLR_file = base_dir.joinpath('TN-14_C30_C20_GSFC_SLR.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C20 file: {SLR_file}')
+        logging.debug(f'Reading SLR C20 file: {str(SLR_file)}')
         # read SLR file
         C20_input = gravity_toolkit.SLR.C20(SLR_file)
         FLAGS.append('_wGSFC_C20')
-        attributes['SLR C20'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR C20'] = ('GSFC', SLR_file.name)
 
     # Replacing C2,1/S2,1 with SLR values
     if (kwargs['SLR_21'] == 'CSR'):
-        SLR_file = os.path.join(base_dir,f'C21_S21_{DREL}.txt')
+        SLR_file = base_dir.joinpath(f'C21_S21_{DREL}.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C21/S21 file: {SLR_file}')
+        logging.debug(f'Reading SLR C21/S21 file: {str(SLR_file)}')
         # read SLR file
         C21_input = gravity_toolkit.SLR.CS2(SLR_file)
         FLAGS.append('_wCSR_21')
-        attributes['SLR 21'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR 21'] = ('CSR', SLR_file.name)
     elif (kwargs['SLR_21'] == 'GFZ'):
-        GravIS_file = 'GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat'
-        SLR_file = os.path.join(base_dir,GravIS_file)
+        GravIS_file = 'GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0003.dat'
+        SLR_file = base_dir.joinpath(GravIS_file)
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C21/S21 file: {SLR_file}')
+        logging.debug(f'Reading SLR C21/S21 file: {str(SLR_file)}')
         # read SLR file
         C21_input = gravity_toolkit.SLR.CS2(SLR_file)
         FLAGS.append('_wGFZ_21')
-        attributes['SLR 21'] = ('GFZ GravIS',os.path.basename(SLR_file))
+        attributes['SLR 21'] = ('GFZ GravIS', SLR_file.name)
     elif (kwargs['SLR_21'] == 'GSFC'):
         # calculate monthly averages from 7-day arcs
-        SLR_file = os.path.join(base_dir,'gsfc_slr_5x5c61s61.txt')
+        SLR_file = base_dir.joinpath('gsfc_slr_5x5c61s61.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C21/S21 file: {SLR_file}')
+        logging.debug(f'Reading SLR C21/S21 file: {str(SLR_file)}')
         # read SLR file
         C21_input = gravity_toolkit.SLR.CS2(SLR_file,
             DATE=grace_Ylms['time'], ORDER=1)
         FLAGS.append('_wGSFC_21')
-        attributes['SLR 21'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR 21'] = ('GSFC', SLR_file.name)
 
     # Replacing C2,2/S2,2 with SLR values
     if (kwargs['SLR_22'] == 'CSR'):
-        SLR_file = os.path.join(base_dir,f'C22_S22_{DREL}.txt')
+        SLR_file = base_dir.joinpath(f'C22_S22_{DREL}.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C22/S22 file: {SLR_file}')
+        logging.debug(f'Reading SLR C22/S22 file: {str(SLR_file)}')
         # read SLR file
         C22_input = gravity_toolkit.SLR.CS2(SLR_file)
         FLAGS.append('_wCSR_22')
-        attributes['SLR 22'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR 22'] = ('CSR', SLR_file.name)
     elif (kwargs['SLR_22'] == 'GSFC'):
-        SLR_file = os.path.join(base_dir,'gsfc_slr_5x5c61s61.txt')
+        SLR_file = base_dir.joinpath('gsfc_slr_5x5c61s61.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C22/S22 file: {SLR_file}')
+        logging.debug(f'Reading SLR C22/S22 file: {str(SLR_file)}')
         # read SLR file
         C22_input = gravity_toolkit.SLR.CS2(SLR_file,
             DATE=grace_Ylms['time'], ORDER=2)
         FLAGS.append('_wGSFC_22')
-        attributes['SLR 22'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR 22'] = ('GSFC', SLR_file.name)
 
     # Replacing C3,0 with SLR values
     if (kwargs['SLR_C30'] == 'CSR'):
-        SLR_file = os.path.join(base_dir,'CSR_Monthly_5x5_Gravity_Harmonics.txt')
+        SLR_file = base_dir.joinpath('CSR_Monthly_5x5_Gravity_Harmonics.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C30 file: {SLR_file}')
+        logging.debug(f'Reading SLR C30 file: {str(SLR_file)}')
         # read SLR file
         C30_input = gravity_toolkit.SLR.C30(SLR_file)
         FLAGS.append('_wCSR_C30')
-        attributes['SLR C30'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR C30'] = ('CSR', SLR_file.name)
     elif (kwargs['SLR_C30'] == 'LARES'):
-        SLR_file = os.path.join(base_dir,'C30_LARES_filtered.txt')
+        SLR_file = base_dir.joinpath('C30_LARES_filtered.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C30 file: {SLR_file}')
+        logging.debug(f'Reading SLR C30 file: {str(SLR_file)}')
         # read SLR file
         C30_input = gravity_toolkit.SLR.C30(SLR_file)
         FLAGS.append('_wLARES_C30')
-        attributes['SLR_C30'] = ('CSR LARES',os.path.basename(SLR_file))
+        attributes['SLR_C30'] = ('CSR LARES', SLR_file.name)
     elif (kwargs['SLR_C30'] == 'GFZ'):
-        GravIS_file = 'GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat'
-        SLR_file = os.path.join(base_dir,GravIS_file)
+        GravIS_file = 'GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0003.dat'
+        SLR_file = base_dir.joinpath(GravIS_file)
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C30 file: {SLR_file}')
+        logging.debug(f'Reading SLR C30 file: {str(SLR_file)}')
         # read SLR file
         C30_input = gravity_toolkit.SLR.C30(SLR_file)
         FLAGS.append('_wGFZ_C30')
-        attributes['SLR C30'] = ('GFZ GravIS',os.path.basename(SLR_file))
+        attributes['SLR C30'] = ('GFZ GravIS', SLR_file.name)
     elif (kwargs['SLR_C30'] == 'GSFC'):
-        SLR_file = os.path.join(base_dir,'TN-14_C30_C20_GSFC_SLR.txt')
+        SLR_file = base_dir.joinpath('TN-14_C30_C20_GSFC_SLR.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C30 file: {SLR_file}')
+        logging.debug(f'Reading SLR C30 file: {str(SLR_file)}')
         # read SLR file
         C30_input = gravity_toolkit.SLR.C30(SLR_file)
         FLAGS.append('_wGSFC_C30')
-        attributes['SLR C30'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR C30'] = ('GSFC', SLR_file.name)
 
     # Replacing C4,0 with SLR values
     if (kwargs['SLR_C40'] == 'CSR'):
-        SLR_file = os.path.join(base_dir,'CSR_Monthly_5x5_Gravity_Harmonics.txt')
+        SLR_file = base_dir.joinpath('CSR_Monthly_5x5_Gravity_Harmonics.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C40 file: {SLR_file}')
+        logging.debug(f'Reading SLR C40 file: {str(SLR_file)}')
         # read SLR file
         C40_input = gravity_toolkit.SLR.C40(SLR_file)
         FLAGS.append('_wCSR_C40')
-        attributes['SLR C40'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR C40'] = ('CSR', SLR_file.name)
     elif (kwargs['SLR_C40'] == 'LARES'):
-        SLR_file = os.path.join(base_dir,'C40_LARES_filtered.txt')
+        SLR_file = base_dir.joinpath('C40_LARES_filtered.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C40 file: {SLR_file}')
+        logging.debug(f'Reading SLR C40 file: {str(SLR_file)}')
         # read SLR file
         C40_input = gravity_toolkit.SLR.C40(SLR_file)
         FLAGS.append('_wLARES_C40')
-        attributes['SLR C40'] = ('CSR LARES',os.path.basename(SLR_file))
+        attributes['SLR C40'] = ('CSR LARES', SLR_file.name)
     elif (kwargs['SLR_C40'] == 'GSFC'):
-        SLR_file = os.path.join(base_dir,'gsfc_slr_5x5c61s61.txt')
+        SLR_file = base_dir.joinpath('gsfc_slr_5x5c61s61.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C40 file: {SLR_file}')
+        logging.debug(f'Reading SLR C40 file: {str(SLR_file)}')
         # read SLR file
         C40_input = gravity_toolkit.SLR.C40(SLR_file,
             DATE=grace_Ylms['time'])
         FLAGS.append('_wGSFC_C40')
-        attributes['SLR C40'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR C40'] = ('GSFC', SLR_file.name)
 
     # Replacing C5,0 with SLR values
     if (kwargs['SLR_C50'] == 'CSR'):
-        SLR_file = os.path.join(base_dir,'CSR_Monthly_5x5_Gravity_Harmonics.txt')
+        SLR_file = base_dir.joinpath('CSR_Monthly_5x5_Gravity_Harmonics.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C50 file: {SLR_file}')
+        logging.debug(f'Reading SLR C50 file: {str(SLR_file)}')
         # read SLR file
         C50_input = gravity_toolkit.SLR.C50(SLR_file)
         FLAGS.append('_wCSR_C50')
-        attributes['SLR C50'] = ('CSR',os.path.basename(SLR_file))
+        attributes['SLR C50'] = ('CSR', SLR_file.name)
     elif (kwargs['SLR_C50'] == 'LARES'):
-        SLR_file = os.path.join(base_dir,'C50_LARES_filtered.txt')
+        SLR_file = base_dir.joinpath('C50_LARES_filtered.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C50 file: {SLR_file}')
+        logging.debug(f'Reading SLR C50 file: {str(SLR_file)}')
         # read SLR file
         C50_input = gravity_toolkit.SLR.C50(SLR_file)
         FLAGS.append('_wLARES_C50')
-        attributes['SLR C50'] = ('CSR LARES',os.path.basename(SLR_file))
+        attributes['SLR C50'] = ('CSR LARES', SLR_file.name)
     elif (kwargs['SLR_C50'] == 'GSFC'):
-        # SLR_file = os.path.join(base_dir,'GSFC_SLR_C20_C30_C50_GSM_replacement.txt')
-        SLR_file = os.path.join(base_dir,'gsfc_slr_5x5c61s61.txt')
+        # SLR_file = base_dir.joinpath('GSFC_SLR_C20_C30_C50_GSM_replacement.txt')
+        SLR_file = base_dir.joinpath('gsfc_slr_5x5c61s61.txt')
         # log SLR file if debugging
-        logging.debug(f'Reading SLR C50 file: {SLR_file}')
+        logging.debug(f'Reading SLR C50 file: {str(SLR_file)}')
         # read SLR file
         C50_input = gravity_toolkit.SLR.C50(SLR_file,
             DATE=grace_Ylms['time'])
         FLAGS.append('_wGSFC_C50')
-        attributes['SLR C50'] = ('GSFC',os.path.basename(SLR_file))
+        attributes['SLR C50'] = ('GSFC', SLR_file.name)
 
     # Correcting for Degree 1 (geocenter variations)
     # reading degree 1 file for given release if specified
     if (DEG1 == 'Tellus'):
         # Tellus (PO.DAAC) degree 1
         if DREL in ('RL04','RL05'):
             # old degree one files
-            default_geocenter = os.path.join(base_dir,'geocenter',
+            default_geocenter = base_dir.joinpath('geocenter',
                 f'deg1_coef_{DREL}.txt')
             JPL = False
         else:
             # new TN-13 degree one files
-            default_geocenter = os.path.join(base_dir,'geocenter',
+            default_geocenter = base_dir.joinpath('geocenter',
                 f'TN-13_GEOC_{PROC}_{DREL}.txt')
             JPL = True
         # read degree one files from JPL GRACE Tellus
         DEG1_file = kwargs.get('DEG1_FILE') or default_geocenter
         # log geocenter file if debugging
         logging.debug(f'Reading Geocenter file: {DEG1_file}')
         DEG1_input = gravity_toolkit.geocenter().from_tellus(DEG1_file,JPL=JPL)
         FLAGS.append(f'_w{DEG1}_DEG1')
-        attributes['geocenter'] = ('JPL Tellus',os.path.basename(DEG1_file))
+        attributes['geocenter'] = ('JPL Tellus', DEG1_file.name)
     elif (DEG1 == 'SLR'):
         # CSR Satellite Laser Ranging (SLR) degree 1
         # # SLR-derived degree-1 mass variations
         # # ftp://ftp.csr.utexas.edu/pub/slr/geocenter/
-        # DEG1_file = os.path.join(base_dir,'geocenter',f'GCN_{DREL}.txt')
+        # DEG1_file = base_dir.joinpath('geocenter',f'GCN_{DREL}.txt')
         # COLUMNS = ['time','X','Y','Z','X_sigma','Y_sigma','Z_sigma']
         # DEG1_input = gravity_toolkit.geocenter().from_SLR(DEG1_file,
         #      AOD=True, release=DREL, header=16, COLUMNS=COLUMNS)
 
         # # new CF-CM file of degree-1 mass variations
         # # https://cddis.nasa.gov/lw20/docs/2016/papers/14-Ries_paper.pdf
         # # http://download.csr.utexas.edu/pub/slr/geocenter/GCN_L1_L2_30d_CF-CM.txt
-        # DEG1_file = os.path.join(base_dir,'geocenter','GCN_L1_L2_30d_CF-CM.txt')
+        # DEG1_file = base_dir.joinpath('geocenter','GCN_L1_L2_30d_CF-CM.txt')
         # COLUMNS = ['time','X','Y','Z','X_sigma','Y_sigma','Z_sigma']
         # DEG1_input = gravity_toolkit.geocenter().from_SLR(DEG1_file,
         #     AOD=True, release=DREL, header=111, columns=COLUMNS)
 
         # new file of degree-1 mass variations from Minkang Cheng
         # http://download.csr.utexas.edu/outgoing/cheng/gct2est.220_5s
-        DEG1_file = os.path.join(base_dir,'geocenter','gct2est.220_5s')
+        DEG1_file = base_dir.joinpath('geocenter','gct2est.220_5s')
         COLUMNS = ['MJD','time','X','Y','Z','XM','YM','ZM',
             'X_sigma','Y_sigma','Z_sigma','XM_sigma','YM_sigma','ZM_sigma']
         # log geocenter file if debugging
         logging.debug(f'Reading Geocenter file: {DEG1_file}')
         # read degree one files from CSR satellite laser ranging
         DEG1_input = gravity_toolkit.geocenter(radius=6.378136e9).from_SLR(
             DEG1_file, AOD=True, release=DREL, header=15, columns=COLUMNS)
         FLAGS.append(f'_w{DEG1}_DEG1')
-        attributes['geocenter'] = ('CSR SLR',os.path.basename(DEG1_file))
+        attributes['geocenter'] = ('CSR SLR', DEG1_file.name)
     elif DEG1 in ('SLF','UCI'):
         # degree one files from Sutterley and Velicogna (2019)
         # default: iterated and with self-attraction and loading effects
         MODEL = dict(RL04='OMCT', RL05='OMCT', RL06='MPIOM')
         args = (PROC,DREL,MODEL[DREL],'SLF_iter')
-        default_geocenter = os.path.join(base_dir,'geocenter',
+        default_geocenter = base_dir.joinpath('geocenter',
             '{0}_{1}_{2}_{3}.txt'.format(*args))
         # read degree one files from Sutterley and Velicogna (2019)
         DEG1_file = kwargs.get('DEG1_FILE') or default_geocenter
         # log geocenter file if debugging
         logging.debug(f'Reading Geocenter file: {DEG1_file}')
         DEG1_input = gravity_toolkit.geocenter().from_UCI(DEG1_file)
         FLAGS.append(f'_w{DEG1}_DEG1')
-        attributes['geocenter'] = ('UCI',os.path.basename(DEG1_file))
+        attributes['geocenter'] = ('UCI', DEG1_file.name)
     elif (DEG1 == 'Swenson'):
         # degree 1 coefficients provided by Sean Swenson in mm w.e.
-        default_geocenter = os.path.join(base_dir,'geocenter',
+        default_geocenter = base_dir.joinpath('geocenter',
             f'gad_gsm.{DREL}.txt')
         # read degree one files from Swenson et al. (2008)
         DEG1_file = kwargs.get('DEG1_FILE') or default_geocenter
         # log geocenter file if debugging
         logging.debug(f'Reading Geocenter file: {DEG1_file}')
         DEG1_input = gravity_toolkit.geocenter().from_swenson(DEG1_file)
         FLAGS.append(f'_w{DEG1}_DEG1')
-        attributes['geocenter'] = ('Swenson',os.path.basename(DEG1_file))
+        attributes['geocenter'] = ('Swenson', DEG1_file.name)
     elif (DEG1 == 'GFZ'):
         # degree 1 coefficients provided by GFZ GravIS
         # http://gravis.gfz-potsdam.de/corrections
-        default_geocenter = os.path.join(base_dir,'geocenter',
-            'GRAVIS-2B_GFZOP_GEOCENTER_0002.dat')
+        default_geocenter = base_dir.joinpath('geocenter',
+            'GRAVIS-2B_GFZOP_GEOCENTER_0003.dat')
         # read degree one files from GFZ GravIS
         DEG1_file = kwargs.get('DEG1_FILE') or default_geocenter
         # log geocenter file if debugging
         logging.debug(f'Reading Geocenter file: {DEG1_file}')
         DEG1_input = gravity_toolkit.geocenter().from_gravis(DEG1_file)
         FLAGS.append(f'_w{DEG1}_DEG1')
-        attributes['geocenter'] = ('GFZ GravIS',os.path.basename(DEG1_file))
+        attributes['geocenter'] = ('GFZ GravIS', DEG1_file.name)
 
     # atmospheric flag if correcting ECMWF "jumps" (using GAE/GAF/GAG files)
     if kwargs['ATM']:
         FLAGS.append('_wATM')
         attributes['atmospheric_correction'] = 'Fagiolini et al. (2015)'
     # pole tide flag if correcting for pole tide drift (Wahr et al. 2015)
     if kwargs['POLE_TIDE']:
@@ -834,15 +838,15 @@
                 grace_Ylms['clm'][1,0,i] = np.copy(DEG1_input.C10[k])
                 grace_Ylms['clm'][1,1,i] = np.copy(DEG1_input.C11[k])
                 grace_Ylms['slm'][1,1,i] = np.copy(DEG1_input.S11[k])
 
     # read and add/remove the GAE and GAF atmospheric correction coefficients
     if kwargs['ATM']:
         # read ECMWF correction files from Fagiolini et al. (2015)
-        atm_corr = read_ecmwf_corrections(base_dir,LMAX,months,MMAX=MMAX)
+        atm_corr = read_ecmwf_corrections(base_dir, LMAX, months, MMAX=MMAX)
         # add files to lineage attribute
         attributes['lineage'].extend(atm_corr['files'])
         # Removing GAE/GAF/GAG from RL05 GSM Products
         if (DSET == 'GSM'):
             for m in range(0,MMAX+1):# MMAX+1 to include l
                 for l in range(m,LMAX+1):# LMAX+1 to include LMAX
                     grace_Ylms['clm'][l,m,:] -= atm_corr['clm'][l,m,:]
@@ -851,15 +855,15 @@
         elif DSET in ('GAC','GAA'):
             for m in range(0,MMAX+1):# MMAX+1 to include l
                 for l in range(m,LMAX+1):# LMAX+1 to include LMAX
                     grace_Ylms['clm'][l,m,:] += atm_corr['clm'][l,m,:]
                     grace_Ylms['slm'][l,m,:] += atm_corr['slm'][l,m,:]
 
     # input directory for product
-    grace_Ylms['directory'] = copy.copy(grace_dir)
+    grace_Ylms['directory'] = grace_dir
     # append attributes to output dictionary
     grace_Ylms['attributes'] = attributes
 
     # return the harmonic solutions and associated attributes
     return grace_Ylms
 
 # PURPOSE: read atmospheric jump corrections from Fagiolini et al. (2015)
@@ -907,19 +911,19 @@
     # number of months to consider in analysis
     n_cons = len(months)
     # set maximum order if not equal to maximum degree
     MMAX = LMAX if (MMAX is None) else MMAX
     # iterate through python dictionary keys (GAE, GAF, GAG)
     for key, val in corr_file.items():
         # log ECMWF correction file if debugging
-        infile = os.path.join(base_dir, val)
-        logging.debug(f'Reading ECMWF file: {infile}')
+        infile = base_dir.joinpath(val)
+        logging.debug(f'Reading ECMWF file: {str(infile)}')
         # allocate for clm and slm of atmospheric corrections
-        atm_corr_clm[key] = np.zeros((LMAX+1,MMAX+1))
-        atm_corr_slm[key] = np.zeros((LMAX+1,MMAX+1))
+        atm_corr_clm[key] = np.zeros((LMAX+1, MMAX+1))
+        atm_corr_slm[key] = np.zeros((LMAX+1, MMAX+1))
         # GRACE correction files are compressed gz files
         with gzip.open(infile,'rb') as f:
             file_contents = f.read().decode('ISO-8859-1').splitlines()
         # for each line in the GRACE correction file
         for line in file_contents:
             # find if line starts with GRCOF2
             if bool(re.match(r'GRCOF2',line)):
@@ -931,16 +935,16 @@
                 # if degree and order are below the truncation limits
                 if ((l1 <= LMAX) and (m1 <= MMAX)):
                     atm_corr_clm[key][l1,m1] = np.float64(line_contents[3])
                     atm_corr_slm[key][l1,m1] = np.float64(line_contents[4])
 
     # create output atmospheric corrections to be removed/added to data
     atm_corr = {}
-    atm_corr['clm'] = np.zeros((LMAX+1,LMAX+1,n_cons))
-    atm_corr['slm'] = np.zeros((LMAX+1,LMAX+1,n_cons))
+    atm_corr['clm'] = np.zeros((LMAX+1, LMAX+1, n_cons))
+    atm_corr['slm'] = np.zeros((LMAX+1, LMAX+1, n_cons))
     atm_corr['files'] = sorted(corr_file.values())
     # for each considered date
     for i,grace_month in enumerate(months):
         # remove correction based on dates
         if (grace_month >= 50) & (grace_month <= 97):
             atm_corr['clm'][:,:,i] = atm_corr_clm['GAE'][:,:]
             atm_corr['slm'][:,:,i] = atm_corr_slm['GAE'][:,:]
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/grace_months_index.py` & `gravity-toolkit-1.2.1/gravity_toolkit/grace_months_index.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 grace_months_index.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (05/2023)
 
 Creates a file with the start and end days for each dataset
 Shows the range of each month for (CSR/GFZ/JPL) (RL04/RL05/RL06)
 Shows which months are missing for each dataset as **missing**
 
 INPUTS:
     base_dir: Working data directory for GRACE/GRACE-FO data
@@ -36,14 +36,16 @@
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
+    Updated 05/2023: use formatting for reading from date file
+        use pathlib to define and operate on paths
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 05/2022: use argparse descriptions within documentation
         use new GSFC release 6 version 2 mascons as the default
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 09/2021: use functions for converting to and from GRACE months
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 10/2020: use argparse to set command line parameters
@@ -59,15 +61,15 @@
     Updated 10/2014: updated comments
     Updated 05/2014: added OPTION to not run RL04
     Updated 05/2013: added years to month label
     Written 07/2012
 """
 from __future__ import print_function
 
-import os
+import pathlib
 import argparse
 import calendar
 import numpy as np
 from gravity_toolkit.time import grace_to_calendar
 
 def grace_months_index(base_dir, DREL=['RL06','rl06v2.0'], MODE=None):
     """
@@ -83,16 +85,17 @@
         Working data directory for GRACE/GRACE-FO data
     DREL: list
         GRACE/GRACE-FO/Swarm data release
     MODE: oct or NoneType, default None
         Permissions mode of output index file
     """
     # Output GRACE months file
-    grace_months_file = os.path.join(base_dir,'GRACE_months.txt')
-    fid = open(grace_months_file, mode='w', encoding='utf8')
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_months_file = base_dir.joinpath('GRACE_months.txt')
+    fid = grace_months_file.open(mode='w', encoding='utf8')
 
     # Initial parameters
     # processing centers
     PROC = ['CSR', 'GFZ', 'GSFC', 'JPL']
     # read from GSM datasets
     DSET = 'GSM'
     # maximum month of the datasets
@@ -103,43 +106,28 @@
 
     # Looping through data releases first (all RL04 then all RL05)
     # for each considered data release (RL04,RL05)
     for rl in DREL:
         # for each processing centers (CSR, GFZ, JPL)
         for pr in PROC:
             # Setting the data directory for processing center and release
-            grace_dir = os.path.join(base_dir, pr, rl, DSET)
-            # read GRACE date ascii file
-            # file created in read_grace.py or grace_dates.py
-            grace_date_file = f'{pr}_{rl}_DATES.txt'
-            if os.access(os.path.join(grace_dir,grace_date_file), os.F_OK):
-                # skip the header line
-                date_input = np.loadtxt(os.path.join(grace_dir,grace_date_file),
-                    skiprows=1)
-                # number of months
-                nmon = np.shape(date_input)[0]
-
+            grace_dir = base_dir.joinpath(pr, rl, DSET)
+            # read GRACE/GRACE-FO date ascii file
+            grace_date_file = grace_dir.joinpath(f'{pr}_{rl}_DATES.txt')
+            # names and formats of GRACE/GRACE-FO date ascii file
+            names = ('t','mon','styr','stday','endyr','endday','total')
+            formats = ('f','i','i','i','i','i','i')
+            dtype = np.dtype({'names':names, 'formats':formats})
+            # check that the GRACE/GRACE-FO date file exists
+            if grace_date_file.exists():
                 # Setting the dictionary key e.g. 'CSR_RL04'
                 var_name = f'{pr}_{rl}'
-
-                # Creating a python dictionary for each dataset with parameters:
-                # month #, start year, start day, end year, end day
-                # Purpose is to get all of the dates loaded for each dataset
-                # Adding data to dictionary for data processing and release
-                var_info[var_name] = {}
-                # allocate for output variables
-                var_info[var_name]['mon'] = np.zeros((nmon),dtype=np.int64)
-                var_info[var_name]['styr'] = np.zeros((nmon),dtype=np.int64)
-                var_info[var_name]['stday'] = np.zeros((nmon),dtype=np.int64)
-                var_info[var_name]['endyr'] = np.zeros((nmon),dtype=np.int64)
-                var_info[var_name]['endday'] = np.zeros((nmon),dtype=np.int64)
-                # place output variables in dictionary
-                for i,key in enumerate(['mon','styr','stday','endyr','endday']):
-                    # first column is date in decimal form (start at 1 not 0)
-                    var_info[var_name][key] = date_input[:,i+1].astype(np.int64)
+                # skip the header line
+                var_info[var_name] = np.loadtxt(grace_date_file,
+                    skiprows=1, dtype=dtype)
                 # Finding the maximum month measured
                 if (var_info[var_name]['mon'].max() > max_mon):
                     # if the maximum month in this dataset is greater
                     # than the previously read datasets
                     max_mon = np.int64(var_info[var_name]['mon'].max())
 
     # sort datasets alphanumerically
@@ -186,28 +174,27 @@
         # printing data line to file
         args = (m, month_string, calendar_year, data_string)
         print('{0:03d} {1:>3}{2:4d} {3}'.format(*args), file=fid)
 
     # close months file
     fid.close()
     # set the permissions level of the output file
-    os.chmod(grace_months_file, MODE)
+    grace_months_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Creates a file with the start and end days for
             each month of GRACE/GRACE-FO data
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str, nargs='+',
         default=['RL06','rl06v2.0'],
         help='GRACE/GRACE-FO Data Release')
     # permissions mode of the local directories and files (number in octal)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/harmonic_summation.py` & `gravity-toolkit-1.2.1/gravity_toolkit/harmonic_summation.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 harmonic_summation.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (03/2023)
 
 Returns the spatial field for a series of spherical harmonics
 
 CALLING SEQUENCE:
     spatial = harmonic_summation(clm1, slm1, lon, lat, LMIN=0, LMAX=60)
 
 INPUTS:
@@ -26,14 +26,18 @@
 PROGRAM DEPENDENCIES:
     associated_legendre.py: Computes fully-normalized associated
         Legendre polynomials
     gauss_weights.py: Computes the Gaussian weights as a function of degree
     units.py: class for converting spherical harmonic data to specific units
 
 UPDATE HISTORY:
+    Updated 04/2023: allow love numbers to be None for custom units case
+    Updated 03/2023: allow units inputs to be strings for named types
+        improve typing for variables in docstrings
+        minor refactor in line ordering for readability
     Updated 02/2023: set custom units as top option in if/else statements
     Updated 01/2023: refactored associated legendre polynomials
         added wrapper function for smoothing and converting to output units
         added fft-based transform function
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 07/2020: added function docstrings
     Updated 05/2015: added parameter MMAX for MMAX != LMAX.
@@ -83,28 +87,28 @@
 
     # Longitude in radians
     phi = (np.squeeze(lon)*np.pi/180.0)[np.newaxis,:]
     # colatitude in radians
     th = (90.0 - np.squeeze(lat))*np.pi/180.0
     thmax = len(th)
 
-    # Calculate fourier coefficients from legendre coefficients
-    d_cos = np.zeros((MMAX+1,thmax))# [m,th]
-    d_sin = np.zeros((MMAX+1,thmax))# [m,th]
+    # if plms are not pre-computed: calculate Legendre polynomials
     if PLM is None:
-        # if plms are not pre-computed: calculate Legendre polynomials
         PLM, dPLM = plm_holmes(LMAX, np.cos(th))
 
     # Truncating harmonics to degree and order LMAX
     # removing coefficients below LMIN and above MMAX
     mm = np.arange(0, MMAX+1)
     clm = np.zeros((LMAX+1, MMAX+1))
     slm = np.zeros((LMAX+1, MMAX+1))
     clm[LMIN:LMAX+1,mm] = clm1[LMIN:LMAX+1,mm]
     slm[LMIN:LMAX+1,mm] = slm1[LMIN:LMAX+1,mm]
+    # Calculate fourier coefficients from legendre coefficients
+    d_cos = np.zeros((MMAX+1,thmax))# [m,th]
+    d_sin = np.zeros((MMAX+1,thmax))# [m,th]
     for k in range(0,thmax):
         # summation over all spherical harmonic degrees
         d_cos[:,k] = np.sum(PLM[:,mm,k]*clm[:,mm],axis=0)
         d_sin[:,k] = np.sum(PLM[:,mm,k]*slm[:,mm],axis=0)
 
     # Final signal recovery from fourier coefficients
     m = np.arange(0,MMAX+1)[:,np.newaxis]
@@ -159,21 +163,21 @@
     assert np.isclose(np.max(lon), 360.0)
     # number of longitudinal points
     phimax = len(np.squeeze(lon))
     # colatitude in radians
     th = (90.0 - np.squeeze(lat))*np.pi/180.0
     thmax = len(th)
 
-    # combined Ylms and Fourier coefficients (complex)
-    Ylms = np.zeros((LMAX+1,MMAX+1),dtype=np.complex128)
-    delta_M = np.zeros((MMAX+1,thmax),dtype=np.complex128)# [m,th]
+    # if plms are not pre-computed: calculate Legendre polynomials
     if PLM is None:
-        # if plms are not pre-computed: calculate Legendre polynomials
         PLM, dPLM = plm_holmes(LMAX, np.cos(th))
 
+    # combined Ylms and Fourier coefficients (complex)
+    Ylms = np.zeros((LMAX+1, MMAX+1),dtype=np.complex128)
+    delta_M = np.zeros((MMAX+1,thmax),dtype=np.complex128)# [m,th]
     # Real (cosine) and imaginary (sine) components
     # Truncating harmonics to degree and order LMAX
     # removing coefficients below LMIN and above MMAX
     Ylms[LMIN:LMAX+1,:MMAX+1] = clm1[LMIN:LMAX+1,0:MMAX+1] - \
         slm1[LMIN:LMAX+1,0:MMAX+1]*1j
     # calculate Ylms summation for each theta band
     for k in range(0,thmax):
@@ -193,21 +197,21 @@
 def stokes_summation(clm1, slm1, lon, lat,
     LMIN=0, LMAX=60, MMAX=None, RAD=0, UNITS=0, LOVE=None, PLM=None):
     """
     Converts data from spherical harmonic coefficients to a spatial field
 
     Parameters
     ----------
-    clm1: float
-        cosine spherical harmonic coefficients in output units
-    slm1: float
-        sine spherical harmonic coefficients in output units
-    lon: float
+    clm1: np.ndarray
+        cosine spherical harmonic coefficients
+    slm1: np.ndarray
+        sine spherical harmonic coefficients
+    lon: np.ndarray
         longitude array
-    lat: float
+    lat: np.ndarray
         latitude array
     LMIN: int, default 0
         Lower bound of Spherical Harmonic Degrees
     LMAX: int, default 60
         Upper bound of Spherical Harmonic Degrees
     MMAX: int or NoneType, default None
         Upper bound of Spherical Harmonic Orders
@@ -223,20 +227,20 @@
             - ``5``: mbar equivalent surface pressure
             - ``6``: cm viscoelastic crustal uplift (GIA)
             - list: custom degree-dependent unit conversion factor
     LMAX: int, default 0
         Upper bound of Spherical Harmonic Degrees
     LOVE: tuple or NoneType, default None
         Load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
-    PLM: float or NoneType, default None
+    PLM: np.ndarray or NoneType, default None
         Fully-normalized associated Legendre polynomials
 
     Returns
     -------
-    spatial: float
+    spatial: np.ndarray
         spatial field
 
     References
     ----------
     .. [Davis2004] J. L. Davis et al.,
         "Climate-driven deformation of the solid Earth from GRACE and GPS",
         *Geophysical Research Letters*, 31(L24605), (2004).
@@ -249,15 +253,15 @@
         `doi: 10.1007/s00190-002-0216-2 <https://doi.org/10.1007/s00190-002-0216-2>`_
 
     .. [Tscherning1982] C. C. Tscherning and K. Poder,
         "Some Geodetic Applications of Clenshaw Summation",
         *Bollettino di Geodesia e Scienze*, 4, 349--375, (1982).
 
     .. [Wahr1998] J. Wahr, M. Molenaar, and F. Bryan, "Time
-        variabilityof the Earth's gravity field: Hydrological
+        variability of the Earth's gravity field: Hydrological
         and oceanic effects and their possible detection using GRACE",
         *Journal of Geophysical Research*, 103(B12), 30205-30229, (1998).
         `doi: 10.1029/98JB02844 <https://doi.org/10.1029/98JB02844>`_
 
     .. [Wahr2000] J. Wahr, D. Wingham, and C. Bentley,
         "A method of combining ICESat and GRACE satellite data to constrain
         Antarctic mass balance", *Journal of Geophysical Research: Solid Earth*,
@@ -275,41 +279,25 @@
     if (RAD != 0):
         wl = 2.0*np.pi*gauss_weights(RAD, LMAX)
     else:
         # else = 1
         wl = np.ones((LMAX+1))
 
     # Setting units factor for output
-    # extract arrays of kl, hl, and ll Love Numbers
-    factors = units(lmax=LMAX).harmonic(*LOVE)
-    # dfactor computes the degree dependent coefficients
-    if isinstance(UNITS,(list,np.ndarray)):
+    # dfactor is the degree dependent coefficients
+    factors = units(lmax=LMAX)
+    if isinstance(UNITS, (list,np.ndarray)):
         # custom units
         dfactor = np.copy(UNITS)
-    elif (UNITS == 0):
-        # 0: keep original scale
-        dfactor = factors.norm
-    elif (UNITS == 1):
-        # 1: cmH2O, centimeters water equivalent
-        dfactor = factors.cmwe
-    elif (UNITS == 2):
-        # 2: mmGH, mm geoid height
-        dfactor = factors.mmGH
-    elif (UNITS == 3):
-        # 3: mmCU, mm elastic crustal deformation
-        dfactor = factors.mmCU
-    elif (UNITS == 4):
-        # 4: micGal, microGal gravity perturbations
-        dfactor = factors.microGal
-    elif (UNITS == 5):
-        # 5: mbar, equivalent surface pressure
-        dfactor = factors.mbar
-    elif (UNITS == 6):
-        # 6: cmVCU, cm viscoelastic  crustal uplift (GIA ONLY)
-        dfactor = factors.cmVCU
+    elif isinstance(UNITS, str):
+        # named units
+        dfactor = factors.harmonic(*LOVE).get(UNITS)
+    elif isinstance(UNITS, int):
+        # use named unit codes
+        dfactor = factors.harmonic(*LOVE).get(units.bycode(UNITS))
     else:
         raise ValueError(f'Unknown units {UNITS}')
 
     # truncate to degree and order
     mm = np.arange(0, MMAX+1)
     # smooth harmonics and convert to output units
     clm = np.zeros((LMAX+1, MMAX+1))
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/harmonics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 harmonics.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (06/2023)
 Contributions by Hugo Lecomte
 
 Spherical harmonic data class for processing GRACE/GRACE-FO Level-2 data
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
@@ -21,20 +21,26 @@
     time.py: utilities for calculating time operations
     read_GRACE_harmonics.py: read spherical harmonic data from SHM files
     read_gfc_harmonics.py: reads spherical harmonic data from gfc files
     read_ICGEM_harmonics.py: reads gravity model coefficients from GFZ ICGEM
     destripe_harmonics.py: filters spherical harmonics for correlated errors
 
 UPDATE HISTORY:
+    Updated 06/2023: fix GRACE/GRACE-FO months in drift function
+    Updated 05/2023: use reify decorators for complex form and amplitude
+        use pathlib to define and operate on paths
     Updated 03/2023: customizable file-level attributes to netCDF4 and HDF5
         add attributes fetching to the from_dict and to_dict functions
         retrieve all root attributes from HDF5 and netCDF4 datasets
         only attempt to squeeze from final dimension in harmonics objects
         add indexing of filenames to harmonics object iterator
         use copy.copy and not numpy.copy in copy harmonics object function
+        convert shape and ndim to harmonic class properties
+        improve typing for variables in docstrings
+        set case insensitive filename to None if filename is empty
     Updated 02/2023: fix expand case where data is a single degree
         fixed case where maximum spherical harmonic degree is 0
         use monospaced text for harmonics objects in docstrings
     Updated 01/2023: made amplitude a property of the harmonics class
         added property for the complex form of the spherical harmonics
     Updated 12/2022: add software information to output HDF5 and netCDF4
         moved GIA model reader to be an inherited class of harmonics
@@ -78,30 +84,31 @@
         make date optional for harmonic read functions.  add more math functions
         add option to sort if reading from an index or merging a list
         add options to flatten and expand harmonics matrices or arrays
     Written 03/2020
 """
 from __future__ import print_function, division
 
-import os
 import re
 import io
 import copy
 import gzip
 import time
 import uuid
 import logging
+import pathlib
 import zipfile
 import warnings
 import numpy as np
 import gravity_toolkit.version
 from gravity_toolkit.time import adjust_months,calendar_to_grace
 from gravity_toolkit.destripe_harmonics import destripe_harmonics
 from gravity_toolkit.read_gfc_harmonics import read_gfc_harmonics
 from gravity_toolkit.read_GRACE_harmonics import read_GRACE_harmonics
+from gravity_toolkit.utilities import reify
 
 # attempt imports
 try:
     from geoid_toolkit.read_ICGEM_harmonics import read_ICGEM_harmonics
 except (ImportError, ModuleNotFoundError) as exc:
     warnings.filterwarnings("module")
     warnings.warn("geoid_toolkit not available", ImportWarning)
@@ -124,28 +131,24 @@
 
     Attributes
     ----------
     lmax: int
         maximum degree of the spherical harmonic field
     mmax: int
         maximum order of the spherical harmonic field
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonics
-    slm: float
+    slm: np.ndarray
         sine spherical harmonics
-    time: float
+    time: np.ndarray
         time variable of the spherical harmonics
-    month: int
+    month: np.ndarray
         GRACE/GRACE-FO months variable of the spherical harmonics
     attributes: dict
         attributes of ``harmonics`` variables
-    shape: tuple
-        dimensions of ``harmonics`` object
-    ndim: int
-        number of dimensions of ``harmonics`` object
     filename: str
         input or output filename
     flattened: bool
         ``harmonics`` object is compressed into arrays
     """
     np.seterr(invalid='ignore')
     def __init__(self, **kwargs):
@@ -159,52 +162,70 @@
         self.month=None
         self.lmax=kwargs['lmax']
         self.mmax=kwargs['mmax']
         # calculate spherical harmonic degree and order (0 is falsy)
         self.l=np.arange(self.lmax+1) if (self.lmax is not None) else None
         self.m=np.arange(self.mmax+1) if (self.mmax is not None) else None
         self.attributes=dict()
-        self.shape=None
-        self.ndim=None
         self.filename=None
         self.flattened=False
         # iterator
         self.__index__ = 0
 
-    def case_insensitive_filename(self,filename):
+    def case_insensitive_filename(self, filename):
         """
         Searches a directory for a filename without case dependence
 
         Parameters
         ----------
-        filename: str
+        filename: str, io.IOBase, pathlib.Path or None
             input filename
         """
         # check if filename is open file object
         if isinstance(filename, io.IOBase):
             self.filename = copy.copy(filename)
-        elif isinstance(filename, type(None)):
+        elif isinstance(filename, type(None)) or not bool(filename):
             self.filename = None
         else:
             # tilde-expand input filename
-            self.filename = os.path.expanduser(filename)
+            self.filename = pathlib.Path(filename).expanduser().absolute()
             # check if file presently exists with input case
-            if not os.access(self.filename,os.F_OK):
+            if not self.filename.exists():
                 # search for filename without case dependence
-                basename = os.path.basename(filename)
-                directory = os.path.dirname(os.path.expanduser(filename))
-                f = [f for f in os.listdir(directory) if re.match(basename,f,re.I)]
+                f = [f.name for f in self.filename.parent.iterdir() if
+                    re.match(self.filename.name, f.name, re.I)]
                 if not f:
                     errmsg = f'{filename} not found in file system'
                     raise FileNotFoundError(errmsg)
-                self.filename = os.path.join(directory,f.pop())
+                self.filename = self.filename.with_name(f.pop())
         # print filename
         logging.debug(self.filename)
         return self
 
+    def compressuser(self, filename=None):
+        """
+        Tilde-compresses a file to be relative to the home directory
+
+        Parameters
+        ----------
+        filename: str or None, default None
+            output filename
+        """
+        if filename is None:
+            filename = self.filename
+        else:
+            filename = pathlib.Path(filename).expanduser().absolute()
+        # attempt to compress filename relative to home directory
+        try:
+            relative_to = filename.relative_to(pathlib.Path().home())
+        except (ValueError, AttributeError) as exc:
+            return filename
+        else:
+            return pathlib.Path('~').joinpath(relative_to)
+
     def from_ascii(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from an ascii file
 
         Parameters
         ----------
         filename: str
@@ -226,21 +247,21 @@
         kwargs.setdefault('date',True)
         kwargs.setdefault('verbose',False)
         kwargs.setdefault('compression',None)
         # open the ascii file and extract contents
         logging.info(self.filename)
         if (kwargs['compression'] == 'gzip'):
             # read input ascii data from gzip compressed file and split lines
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 file_contents = f.read().decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'zip'):
             # read input ascii data from zipped file and split lines
-            base,_ = os.path.splitext(self.filename)
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
-                file_contents = z.read(base).decode('ISO-8859-1').splitlines()
+                file_contents = z.read(stem).decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'bytes'):
             # read input file object and split lines
             file_contents = self.filename.read().splitlines()
         else:
             # read input ascii file (.txt, .asc) and split lines
             with open(self.filename, mode='r', encoding='utf8') as f:
                 file_contents = f.read().splitlines()
@@ -254,17 +275,14 @@
         # for each line in the file
         for line in file_contents:
             l1,m1,clm1,slm1,*aux = rx.findall(line)
             # convert line degree and order to integers
             l1,m1 = np.array([l1,m1],dtype=np.int64)
             self.lmax = np.copy(l1) if (l1 > self.lmax) else self.lmax
             self.mmax = np.copy(m1) if (m1 > self.mmax) else self.mmax
-        # output spherical harmonics dimensions array
-        self.l = np.arange(self.lmax+1)
-        self.m = np.arange(self.mmax+1)
         # output spherical harmonics data
         self.clm = np.zeros((self.lmax+1,self.mmax+1))
         self.slm = np.zeros((self.lmax+1,self.mmax+1))
         # if the ascii file contains date variables
         if kwargs['date']:
             self.time = np.float64(aux[0])
             self.month = np.int64(calendar_to_grace(self.time))
@@ -275,15 +293,15 @@
         for line in file_contents:
             l1,m1,clm1,slm1,*aux = rx.findall(line)
             # convert line degree and order to integers
             ll,mm = np.array([l1,m1],dtype=np.int64)
             # convert fortran exponentials if applicable
             self.clm[ll,mm] = np.float64(clm1.replace('D','E'))
             self.slm[ll,mm] = np.float64(slm1.replace('D','E'))
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def from_netCDF4(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from a netCDF4 file
 
@@ -307,34 +325,34 @@
         # set default parameters
         kwargs.setdefault('date',True)
         kwargs.setdefault('verbose',False)
         kwargs.setdefault('compression',None)
         # Open the NetCDF4 file for reading
         if (kwargs['compression'] == 'gzip'):
             # read as in-memory (diskless) netCDF4 dataset
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=f.read())
         elif (kwargs['compression'] == 'zip'):
             # read zipped file and extract file into in-memory file object
-            fileBasename,_ = os.path.splitext(os.path.basename(filename))
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
                 # first try finding a netCDF4 file with same base filename
                 # if none found simply try searching for a netCDF4 file
                 try:
-                    f,=[f for f in z.namelist() if re.match(fileBasename,f,re.I)]
+                    f,=[f for f in z.namelist() if re.match(stem,f,re.I)]
                 except:
                     f,=[f for f in z.namelist() if re.search(r'\.nc(4)?$',f)]
                 # read bytes from zipfile as in-memory (diskless) netCDF4 dataset
                 fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=z.read(f))
         elif (kwargs['compression'] == 'bytes'):
             # read as in-memory (diskless) netCDF4 dataset
             fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=filename.read())
         else:
             # read netCDF4 dataset
-            fileID = netCDF4.Dataset(self.filename, 'r')
+            fileID = netCDF4.Dataset(self.filename, mode='r')
         # Output NetCDF file information
         logging.info(fileID.filepath())
         logging.info(list(fileID.variables.keys()))
         # read flattened spherical harmonics
         temp = harmonics()
         temp.filename = copy.copy(self.filename)
         # create list of variables to retrieve
@@ -365,15 +383,15 @@
         # get global netCDF4 attributes
         self.attributes['ROOT'] = {}
         for att_name in fileID.ncattrs():
             self.attributes['ROOT'][att_name] = fileID.getncattr(att_name)
         # Closing the NetCDF file
         fileID.close()
         # remove singleton dimensions and
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.squeeze(update_dimensions=True)
         return self
 
     def from_HDF5(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from a HDF5 file
 
@@ -397,46 +415,46 @@
         # set default parameters
         kwargs.setdefault('date',True)
         kwargs.setdefault('verbose',False)
         kwargs.setdefault('compression',None)
         # Open the HDF5 file for reading
         if (kwargs['compression'] == 'gzip'):
             # read gzip compressed file and extract into in-memory file object
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 fid = io.BytesIO(f.read())
             # set filename of BytesIO object
-            fid.filename = os.path.basename(filename)
+            fid.filename = self.filename.name
             # rewind to start of file
             fid.seek(0)
             # read as in-memory (diskless) HDF5 dataset from BytesIO object
-            fileID = h5py.File(fid, 'r')
+            fileID = h5py.File(fid, mode='r')
         elif (kwargs['compression'] == 'zip'):
             # read zipped file and extract file into in-memory file object
-            fileBasename,_ = os.path.splitext(os.path.basename(filename))
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
                 # first try finding a HDF5 file with same base filename
                 # if none found simply try searching for a HDF5 file
                 try:
-                    f,=[f for f in z.namelist() if re.match(fileBasename,f,re.I)]
+                    f,=[f for f in z.namelist() if re.match(stem,f,re.I)]
                 except:
                     f,=[f for f in z.namelist() if re.search(r'\.H(DF)?5$',f,re.I)]
                 # read bytes from zipfile into in-memory BytesIO object
                 fid = io.BytesIO(z.read(f))
             # set filename of BytesIO object
-            fid.filename = os.path.basename(filename)
+            fid.filename = self.filename.name
             # rewind to start of file
             fid.seek(0)
             # read as in-memory (diskless) HDF5 dataset from BytesIO object
-            fileID = h5py.File(fid, 'r')
+            fileID = h5py.File(fid, mode='r')
         elif (kwargs['compression'] == 'bytes'):
             # read as in-memory (diskless) HDF5 dataset
-            fileID = h5py.File(self.filename, 'r')
+            fileID = h5py.File(self.filename, mode='r')
         else:
             # read HDF5 dataset
-            fileID = h5py.File(self.filename, 'r')
+            fileID = h5py.File(self.filename, mode='r')
         # Output HDF5 file information
         logging.info(fileID.filename)
         logging.info(list(fileID.keys()))
         # read flattened spherical harmonics
         temp = harmonics()
         temp.filename = copy.copy(self.filename)
         # create list of variables to retrieve
@@ -467,15 +485,15 @@
         # get global HDF5 attributes
         self.attributes['ROOT'] = {}
         for att_name,att_val in fileID.attrs.items():
             self.attributes['ROOT'][att_name] = att_val
         # Closing the HDF5 file
         fileID.close()
         # remove singleton dimensions and
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.squeeze(update_dimensions=True)
         return self
 
     def from_gfc(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from a gfc gravity model file
 
@@ -511,26 +529,24 @@
         logging.info(self.filename)
         logging.info(list(Ylms.keys()))
         # copy variables for gravity model
         self.clm = Ylms['clm'].copy()
         self.slm = Ylms['slm'].copy()
         self.lmax = np.int64(Ylms['max_degree'])
         self.mmax = np.int64(Ylms['max_degree'])
-        self.l = np.arange(self.lmax+1)
-        self.m = np.arange(self.mmax+1)
         # copy date variables
         if kwargs['date']:
             self.time = Ylms['time'].copy()
             # adjust months to fix special cases if necessary
             self.month = adjust_months(Ylms['month'])
         # geophysical parameters of gravity model
         self.GM = np.float64(Ylms['earth_gravity_constant'])
         self.R = np.float64(Ylms['radius'])
         self.tide = Ylms['tide_system']
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def from_SHM(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from a spherical harmonic model file
 
@@ -557,15 +573,15 @@
         # copy variables for gravity model
         self.clm = Ylms['clm'].copy()
         self.slm = Ylms['slm'].copy()
         self.time = Ylms['time'].copy()
         self.month = np.int64(calendar_to_grace(self.time))
         # copy header information for gravity model
         self.header = Ylms['header']
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def from_index(self, filename, **kwargs):
         """
         Read a ``harmonics`` object from an index of ascii, netCDF4 or HDF5 files
 
@@ -639,17 +655,14 @@
         if kwargs['date'] and kwargs['sort']:
             list_sort = np.argsort([d.time for d in object_list],axis=None)
         else:
             list_sort = np.arange(n)
         # truncate to maximum degree and order
         self.lmax = np.min([d.lmax for d in object_list])
         self.mmax = np.min([d.mmax for d in object_list])
-        # output degree and order
-        self.l = np.arange(self.lmax+1)
-        self.m = np.arange(self.mmax+1)
         # create output harmonics
         self.clm = np.zeros((self.lmax+1,self.mmax+1,n))
         self.slm = np.zeros((self.lmax+1,self.mmax+1,n))
         # create list of files
         self.filename = []
         # output dates
         if kwargs['date']:
@@ -664,15 +677,15 @@
                 self.month[t] = np.atleast_1d(object_list[i].month)
             # append filename to list
             if getattr(object_list[i], 'filename'):
                 self.filename.append(object_list[i].filename)
         # adjust months to fix special cases if necessary
         if kwargs['date']:
             self.month = adjust_months(self.month)
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         # clear the input list to free memory
         if kwargs['clear']:
             object_list = None
         # return the single harmonic object
         return self
 
@@ -736,15 +749,15 @@
             except (AttributeError, KeyError):
                 pass
         # maximum degree and order
         self.lmax = np.max(d['l'])
         self.mmax = np.max(d['m'])
         # add attributes to root if in dictionary
         self.attributes['ROOT'] = d.get('attributes')
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def to_ascii(self, filename, date=True, **kwargs):
         """
         Write a ``harmonics`` object to ascii file
 
@@ -753,15 +766,15 @@
         filename: str
             full path of output ascii file
         date: bool, default True
             ``harmonics`` objects contain date information
         verbose: bool, default False
             Output file and variable information
         """
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         # set default verbosity
         kwargs.setdefault('verbose',False)
         logging.info(self.filename)
         # open the output file
         fid = open(self.filename, mode='w', encoding='utf8')
         if date:
             file_format = '{0:5d} {1:5d} {2:+21.12e} {3:+21.12e} {4:10.4f}'
@@ -827,15 +840,15 @@
         kwargs.setdefault('reference',None)
         kwargs.setdefault('date',True)
         kwargs.setdefault('clobber',True)
         kwargs.setdefault('verbose',False)
         # setting NetCDF clobber attribute
         clobber = 'w' if kwargs['clobber'] else 'a'
         # opening netCDF file for writing
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         fileID = netCDF4.Dataset(self.filename, clobber, format="NETCDF4")
         # flatten harmonics
         temp = self.flatten(date=kwargs['date'])
         # mapping between output keys and netCDF4 variable names
         if not kwargs['field_mapping']:
             kwargs['field_mapping']['l'] = 'l'
             kwargs['field_mapping']['m'] = 'm'
@@ -972,15 +985,15 @@
         kwargs.setdefault('reference',None)
         kwargs.setdefault('date',True)
         kwargs.setdefault('clobber',True)
         kwargs.setdefault('verbose',False)
         # setting HDF5 clobber attribute
         clobber = 'w' if kwargs['clobber'] else 'w-'
         # opening HDF5 file for writing
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         fileID = h5py.File(self.filename, clobber)
         # flatten harmonics
         temp = self.flatten(date=kwargs['date'])
         if kwargs['date']:
             temp.expand_dims(update_dimensions=False)
         # mapping between output keys and HDF5 variable names
         if not kwargs['field_mapping']:
@@ -1071,22 +1084,22 @@
             ``harmonics`` object contains date information
         verbose: bool, default False
             print file and variable information
         kwargs: dict
             keyword arguments for output writers
         """
         # Write index file of output spherical harmonics
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         fid = open(self.filename, mode='w', encoding='utf8')
         # set default verbosity
         kwargs.setdefault('verbose',False)
         # for each file to be in the index
         for i,f in enumerate(file_list):
             # print filename to index
-            print(f.replace(os.path.expanduser('~'),'~'), file=fid)
+            print(self.compressuser(f), file=fid)
             # index harmonics object at i
             h = self.index(i, date=date)
             # write to file
             if (format == 'ascii'):
                 # ascii (.txt)
                 h.to_ascii(f, date=date, **kwargs)
             elif (format == 'netCDF4'):
@@ -1151,18 +1164,18 @@
         # return the dictionary object
         return d
 
     def to_masked_array(self):
         """
         Convert a ``harmonics`` object to a masked numpy array
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         # verify dimensions and get shape
-        ndim_prev = self.ndim
+        ndim_prev = np.copy(self.ndim)
         self.expand_dims()
         l1,m1,nt = self.shape
         # create single triangular matrices with harmonics
         Ylms = np.ma.zeros((self.lmax+1,2*self.lmax+1,nt))
         Ylms.mask = np.ones((self.lmax+1,2*self.lmax+1,nt),dtype=bool)
         for m in range(-self.mmax,self.mmax+1):
             mm = np.abs(m)
@@ -1177,33 +1190,31 @@
         if (self.ndim != ndim_prev):
             self.squeeze()
         # return the triangular matrix
         return Ylms
 
     def update_dimensions(self):
         """
-        Update the dimensions of the ``harmonics`` object
+        Update the dimension variables of the ``harmonics`` object
         """
-        self.ndim = self.clm.ndim
-        self.shape = self.clm.shape
         # calculate spherical harmonic degree and order (0 is falsy)
         self.l=np.arange(self.lmax+1) if (self.lmax is not None) else None
         self.m=np.arange(self.mmax+1) if (self.mmax is not None) else None
         return self
 
     def add(self, temp):
         """
         Add two ``harmonics`` objects
 
         Parameters
         ----------
         temp: obj
             harmonic object to be added
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp.update_dimensions()
         l1 = self.lmax+1 if (temp.lmax > self.lmax) else temp.lmax+1
         m1 = self.mmax+1 if (temp.mmax > self.mmax) else temp.mmax+1
         if (self.ndim == 2):
             self.clm[:l1,:m1] += temp.clm[:l1,:m1]
             self.slm[:l1,:m1] += temp.slm[:l1,:m1]
@@ -1221,15 +1232,15 @@
         Subtract one ``harmonics`` object from another
 
         Parameters
         ----------
         temp: obj
             harmonic object to be subtracted
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp.update_dimensions()
         l1 = self.lmax+1 if (temp.lmax > self.lmax) else temp.lmax+1
         m1 = self.mmax+1 if (temp.mmax > self.mmax) else temp.mmax+1
         if (self.ndim == 2):
             self.clm[:l1,:m1] -= temp.clm[:l1,:m1]
             self.slm[:l1,:m1] -= temp.slm[:l1,:m1]
@@ -1247,15 +1258,15 @@
         Multiply two ``harmonics`` objects
 
         Parameters
         ----------
         temp: obj
             harmonic object to be multiplied
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp.update_dimensions()
         l1 = self.lmax+1 if (temp.lmax > self.lmax) else temp.lmax+1
         m1 = self.mmax+1 if (temp.mmax > self.mmax) else temp.mmax+1
         if (self.ndim == 2):
             self.clm[:l1,:m1] *= temp.clm[:l1,:m1]
             self.slm[:l1,:m1] *= temp.slm[:l1,:m1]
@@ -1273,15 +1284,15 @@
         Divide one ``harmonics`` object from another
 
         Parameters
         ----------
         temp: obj
             harmonic object to be divided
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp.update_dimensions()
         l1 = self.lmax+1 if (temp.lmax > self.lmax) else temp.lmax+1
         m1 = self.mmax+1 if (temp.mmax > self.mmax) else temp.mmax+1
         # indices for cosine spherical harmonics (including zonals)
         lc,mc = np.tril_indices(l1, m=m1)
         # indices for sine spherical harmonics (excluding zonals)
@@ -1306,21 +1317,21 @@
         temp = harmonics(lmax=self.lmax, mmax=self.mmax)
         # copy attributes or update attributes dictionary
         if isinstance(self.attributes, list):
             setattr(temp,'attributes',self.attributes)
         elif isinstance(self.attributes, dict):
             temp.attributes.update(self.attributes)
         # try to assign variables to self
-        for key in ['clm','slm','time','month','shape','ndim','filename']:
+        for key in ['clm','slm','time','month','filename']:
             try:
                 val = getattr(self, key)
                 setattr(temp, key, copy.copy(val))
             except AttributeError:
                 pass
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         return temp
 
     def zeros(self, lmax=None, mmax=None, nt=None):
         """
         Create an empty ``harmonics`` object
         """
@@ -1334,15 +1345,15 @@
             self.clm = np.zeros((self.lmax+1, self.mmax+1, nt))
             self.slm = np.zeros((self.lmax+1, self.mmax+1, nt))
             self.time = np.zeros((nt))
             self.month = np.zeros((nt), dtype=int)
         else:
             self.clm = np.zeros((self.lmax+1, self.mmax+1))
             self.slm = np.zeros((self.lmax+1, self.mmax+1))
-        # assign ndim and shape attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def zeros_like(self):
         """
         Create a ``harmonics`` object using the dimensions of another
         """
@@ -1350,15 +1361,15 @@
         # assign variables to temp
         for key in ['clm','slm','time','month']:
             try:
                 val = getattr(self, key)
                 setattr(temp, key, np.zeros_like(val))
             except AttributeError:
                 pass
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         return temp
 
     def expand_dims(self, update_dimensions=True):
         """
         Add a singleton dimension to a ``harmonics`` object if non-existent
 
@@ -1373,20 +1384,17 @@
         # output harmonics with a third dimension
         if (self.ndim == 2) and not self.flattened:
             self.clm = self.clm[:,:,None]
             self.slm = self.slm[:,:,None]
         elif (self.ndim == 1) and self.flattened:
             self.clm = self.clm[:,None]
             self.slm = self.slm[:,None]
-        # reassign ndim and shape attributes
+        # assign degree and order fields
         if update_dimensions:
             self.update_dimensions()
-        else:
-            self.ndim = self.clm.ndim
-            self.shape = self.clm.shape
         # return the expanded harmonics object
         return self
 
     def squeeze(self, update_dimensions=True):
         """
         Remove singleton dimensions from a ``harmonics`` object
 
@@ -1399,15 +1407,15 @@
         try:
             self.clm = np.squeeze(self.clm, axis=-1)
             self.slm = np.squeeze(self.slm, axis=-1)
             self.time = np.squeeze(self.time)
             self.month = np.squeeze(self.month)
         except ValueError as exc:
             pass
-        # reassign ndim and shape attributes
+        # assign degree and order fields
         if update_dimensions:
             self.update_dimensions()
         else:
             self.ndim = self.clm.ndim
             self.shape = self.clm.shape
         return self
 
@@ -1451,17 +1459,14 @@
                     temp.clm[lm] = self.clm[l,m]
                     temp.slm[lm] = self.slm[l,m]
                 else:
                     temp.clm[lm,:] = self.clm[l,m,:]
                     temp.slm[lm,:] = self.slm[l,m,:]
                 # add 1 to lm counter variable
                 lm += 1
-        # assign ndim and shape attributes
-        temp.ndim = temp.clm.ndim
-        temp.shape = temp.clm.shape
         # update flattened attribute
         temp.flattened = True
         # return the flattened arrays
         return temp
 
     def expand(self, date=True):
         """
@@ -1499,15 +1504,15 @@
                 temp.clm[l,m] = self.clm[lm]
                 temp.slm[l,m] = self.slm[lm]
             else:
                 temp.clm[l,m,:] = self.clm[lm,:]
                 temp.slm[l,m,:] = self.slm[lm,:]
         # update flattened attribute
         temp.flattened = False
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # return the expanded harmonics object
         return temp
 
     def index(self, indice, date=True):
         """
         Subset a ``harmonics`` object to specific index
@@ -1516,29 +1521,29 @@
         ----------
         indice: int
             index in matrix for subsetting
         date: bool, default True
             ``harmonics`` objects contain date information
         """
         # output harmonics object
-        temp = harmonics(lmax=np.copy(self.lmax),mmax=np.copy(self.mmax))
+        temp = harmonics(lmax=np.copy(self.lmax), mmax=np.copy(self.mmax))
         # subset output harmonics
         temp.clm = self.clm[:,:,indice].copy()
         temp.slm = self.slm[:,:,indice].copy()
         # subset output dates
         if date:
             temp.time = self.time[indice].copy()
             temp.month = self.month[indice].copy()
         # subset filenames if applicable
         if getattr(self, 'filename'):
             if isinstance(self.filename, (list, tuple, np.ndarray)):
                 temp.filename = str(self.filename[indice])
             elif isinstance(self.filename, str):
                 temp.filename = copy.copy(self.filename)
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # return the subsetted object
         return temp
 
     def subset(self, months):
         """
         Subset a ``harmonics`` object to specific GRACE/GRACE-FO months
@@ -1556,15 +1561,15 @@
         months_check = list(set(months) - set(self.month))
         if months_check:
             m = ','.join([f'{m:03d}' for m in months_check])
             raise IOError(f'GRACE/GRACE-FO months {m} not Found')
         # indices to sort data objects
         months_list = [i for i,m in enumerate(self.month) if m in months]
         # output harmonics object
-        temp = harmonics(lmax=np.copy(self.lmax),mmax=np.copy(self.mmax))
+        temp = harmonics(lmax=np.copy(self.lmax), mmax=np.copy(self.mmax))
         # create output harmonics
         temp.clm = np.zeros((temp.lmax+1,temp.mmax+1,n))
         temp.slm = np.zeros((temp.lmax+1,temp.mmax+1,n))
         temp.time = np.zeros((n))
         temp.month = np.zeros((n),dtype=np.int64)
         temp.filename = []
         # for each indice
@@ -1575,15 +1580,15 @@
             temp.month[t] = self.month[i].copy()
             # subset filenames if applicable
             if getattr(self, 'filename'):
                 if isinstance(self.filename, list):
                     temp.filename.append(str(self.filename[i]))
                 elif isinstance(self.filename, str):
                     temp.filename.append(self.filename)
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # remove singleton dimensions if importing a single value
         return temp.squeeze()
 
     def truncate(self, lmax, lmin=0, mmax=None):
         """
         Truncate or expand a ``harmonics`` object to a new degree and order
@@ -1617,15 +1622,15 @@
             self.clm[lmin:l1,:m1,:] = temp.clm[lmin:l1,:m1,:].copy()
             self.slm[lmin:l1,:m1,:] = temp.slm[lmin:l1,:m1,:].copy()
         else:
             self.clm = np.zeros((self.lmax+1,self.mmax+1))
             self.slm = np.zeros((self.lmax+1,self.mmax+1))
             self.clm[lmin:l1,:m1] = temp.clm[lmin:l1,:m1].copy()
             self.slm[lmin:l1,:m1] = temp.slm[lmin:l1,:m1].copy()
-        # reassign ndim and shape attributes
+        # assign degree and order fields
         self.update_dimensions()
         # return the truncated or expanded harmonics object
         return self
 
     def mean(self, apply=False, indices=Ellipsis):
         """
         Compute mean gravitational field and remove from data if specified
@@ -1633,15 +1638,15 @@
         Parameters
         ----------
         apply: bool, default False
             remove the mean field from the input ``harmonics`` object
         indices: int, default Ellipsis
             indices of input ``harmonics`` object to compute mean
         """
-        temp = harmonics(lmax=np.copy(self.lmax),mmax=np.copy(self.mmax))
+        temp = harmonics(lmax=np.copy(self.lmax), mmax=np.copy(self.mmax))
         # allocate for mean field
         temp.clm = np.zeros((temp.lmax+1,temp.mmax+1))
         temp.slm = np.zeros((temp.lmax+1,temp.mmax+1))
         # Computes the mean for each spherical harmonic degree and order
         for m in range(0,temp.mmax+1):# MMAX+1 to include l
             for l in range(m,temp.lmax+1):# LMAX+1 to include LMAX
                 # calculate mean static field
@@ -1655,29 +1660,29 @@
         # calculate mean of temporal variables
         for key in ['time','month']:
             try:
                 val = getattr(self, key)
                 setattr(temp, key, np.mean(val[indices]))
             except:
                 continue
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # return the mean field
         return temp
 
     def scale(self, var):
         """
         Multiply a ``harmonics`` object by a constant
 
         Parameters
         ----------
-        var: float
+        var: float or np.ndarray
             scalar value to which the ``harmonics`` object will be multiplied
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp = harmonics(lmax=self.lmax, mmax=self.mmax)
         temp.time = np.copy(self.time)
         temp.month = np.copy(self.month)
         # get filenames if applicable
         if getattr(self, 'filename'):
             temp.filename = copy.copy(self.filename)
@@ -1691,85 +1696,85 @@
             for i,v in enumerate(var):
                 temp.clm[:,:,i] = v*self.clm
                 temp.slm[:,:,i] = v*self.slm
         elif (np.ndim(var) == 1) and (self.ndim == 3):
             for i,v in enumerate(var):
                 temp.clm[:,:,i] = v*self.clm[:,:,i]
                 temp.slm[:,:,i] = v*self.slm[:,:,i]
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         return temp
 
     def power(self, power):
         """
         Raise a ``harmonics`` object to a power
 
         Parameters
         ----------
         var: float
             power to which the ``harmonics`` object will be raised
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp = harmonics(lmax=self.lmax, mmax=self.mmax)
         temp.time = np.copy(self.time)
         temp.month = np.copy(self.month)
         # get filenames if applicable
         if getattr(self, 'filename'):
             temp.filename = copy.copy(self.filename)
         for key in ['clm','slm']:
             val = getattr(self, key)
             setattr(temp, key, np.power(val,power))
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         return temp
 
     def drift(self, t, epoch=2003.3):
         """
         Integrate a ``harmonics`` rate field over time to calculate drift
 
         Parameters
         ----------
-        t: float
+        t: np.ndarray
             times for calculating drift
         epoch: float
             reference epoch for times
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         temp = harmonics(lmax=self.lmax, mmax=self.mmax)
         # allocate for drift field
         temp.clm = np.zeros((temp.lmax+1,temp.mmax+1,len(t)))
         temp.slm = np.zeros((temp.lmax+1,temp.mmax+1,len(t)))
         # copy time variables and calculate GRACE/GRACE-FO months
         temp.time = np.copy(t)
-        temp.month = np.int64(calendar_to_grace(temp.time))
+        temp.month = calendar_to_grace(temp.time)
         # adjust months to fix special cases if necessary
         temp.month = adjust_months(temp.month)
         # get filenames if applicable
         if getattr(self, 'filename'):
             temp.filename = copy.copy(self.filename)
         # calculate drift
         for i,ti in enumerate(t):
             temp.clm[:,:,i] = self.clm*(ti - epoch)
             temp.slm[:,:,i] = self.slm*(ti - epoch)
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         return temp
 
     def convolve(self, var):
         """
         Convolve ``harmonics`` with a degree-dependent array
 
         Parameters
         ----------
-        var: float
+        var: np.ndarray
             degree dependent array for convolution
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         # check if a single field or a temporal field
         if (self.ndim == 2):
             for l in range(0,self.lmax+1):# LMAX+1 to include LMAX
                 self.clm[l,:] *= var[l]
                 self.slm[l,:] *= var[l]
         else:
@@ -1793,17 +1798,17 @@
         References
         ----------
         .. [Swenson2006] S. Swenson and J. Wahr,
             "Post-processing removal of correlated errors in GRACE data",
             *Geophysical Research Letters*, 33(L08402), (2006).
             `doi: 10.1029/2005GL025285 <https://doi.org/10.1029/2005GL025285>`_
         """
-        # reassign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
-        temp = harmonics(lmax=np.copy(self.lmax),mmax=np.copy(self.mmax))
+        temp = harmonics(lmax=np.copy(self.lmax), mmax=np.copy(self.mmax))
         temp.time = np.copy(self.time)
         temp.month = np.copy(self.month)
         # get filenames if applicable
         if getattr(self, 'filename'):
             temp.filename = copy.copy(self.filename)
         # check if a single field or a temporal field
         if (self.ndim == 2):
@@ -1816,20 +1821,20 @@
             temp.clm = np.zeros((self.lmax+1,self.mmax+1,n))
             temp.slm = np.zeros((self.lmax+1,self.mmax+1,n))
             for i in range(n):
                 Ylms = destripe_harmonics(self.clm[:,:,i], self.slm[:,:,i],
                     LMIN=1, LMAX=self.lmax, MMAX=self.mmax, **kwargs)
                 temp.clm[:,:,i] = Ylms['clm'].copy()
                 temp.slm[:,:,i] = Ylms['slm'].copy()
-        # assign ndim and shape attributes
+        # assign degree and order fields
         temp.update_dimensions()
         # return the destriped field
         return temp
 
-    @property
+    @reify
     def amplitude(self):
         """
         Degree amplitude of the spherical harmonics
         """
         # temporary matrix for squared harmonics
         temp = self.power(2)
         # check if a single field or a temporal field
@@ -1851,14 +1856,31 @@
                 # degree amplitude of spherical harmonic degree
                 var = temp.clm[l,m,:] + temp.slm[l,m,:]
                 amp[l,:] = np.sqrt(np.sum(var, axis=0))
         # return the degree amplitudes
         return amp
 
     @property
+    def dtype(self):
+        """Main data type of ``harmonics`` object"""
+        return self.clm.dtype
+
+    @property
+    def shape(self):
+        """Dimensions of ``harmonics`` object
+        """
+        return np.shape(self.clm)
+
+    @property
+    def ndim(self):
+        """Number of dimensions in ``harmonics`` object
+        """
+        return np.ndim(self.clm)
+
+    @reify
     def ilm(self):
         """
         Complex form of the spherical harmonics
         """
         return self.clm - self.slm*1j
 
     def __len__(self):
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/legendre.py` & `gravity-toolkit-1.2.1/gravity_toolkit/legendre.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 legendre.py
-Written by Tyler Sutterley (04/2022)
+Written by Tyler Sutterley (03/2023)
 Computes associated Legendre functions of degree l evaluated for elements x
 l must be a scalar integer and x must contain real values ranging -1 <= x <= 1
 Parallels the MATLAB legendre function
 
 Based on Fortran program by Robert L. Parker, Scripps Institution of
 Oceanography, Institute for Geophysics and Planetary Physics, UCSD. 1993
 
@@ -18,22 +18,22 @@
     Pl: legendre polynomials of degree l for orders 0 to l
 
 OPTIONS:
     NORMALIZE: output Fully Normalized Associated Legendre Functions
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
-    scipy: Scientific Tools for Python (https://docs.scipy.org/doc/)
 
 REFERENCES:
     M. Abramowitz and I.A. Stegun, "Handbook of Mathematical Functions",
         Dover Publications, 1965, Ch. 8.
     J. A. Jacobs, "Geomagnetism", Academic Press, 1987, Ch.4.
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 11/2021: modify normalization to prevent high degree overflows
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 02/2021: modify case with underflow
     Updated 09/2020: verify dimensions of x variable
     Updated 07/2020: added function docstrings
     Updated 05/2020: added normalization option for output polynomials
@@ -47,24 +47,25 @@
     Computes associated Legendre functions for a particular degree
     following [Abramowitz1965]_ and [Jacobs1987]_
 
     Parameters
     ----------
     l: int
         degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
     NORMALIZE: bool, default False
         Fully-normalize the Legendre Functions
 
     Returns
     -------
-    Pl: legendre polynomials of degree ``l``
+    Pl: np.ndarray
+        legendre polynomials of degree ``l``
 
     References
     ----------
     .. [Abramowitz1965] M. Abramowitz and I. A. Stegun,
         *Handbook of Mathematical Functions*, 1046 pp., (1965).
 
     .. [Jacobs1987] J. A. Jacobs, *Geomagnetism*,
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/legendre_polynomials.py` & `gravity-toolkit-1.2.1/gravity_toolkit/legendre_polynomials.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 legendre_polynomials.py
-Written by Tyler Sutterley (04/2022)
+Written by Tyler Sutterley (03/2023)
 
 Computes fully normalized Legendre polynomials for an array of x values
     and their first derivative
 Calculates Legendre polynomials for zonal harmonics (order 0)
 
 CALLING SEQUENCE:
     Pl,dPl = legendre_polynomials(lmax, np.cos(theta))
@@ -29,53 +29,55 @@
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 REFERENCES:
     Hofmann-Wellenhof and Moritz, "Physical Geodesy" (2005)
         http://www.springerlink.com/content/978-3-211-33544-4
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 09/2020: verify dimensions of x variable
     Updated 08/2020: prevent zero divisions by changing u==0 to eps of data type
     Updated 07/2020: added function docstrings
     Updated 07/2017: added first derivative of Legendre polynomials (dpl)
         added option ASTYPE to output as different variable types e.g. np.float64
     Written 03/2013
 """
 import numpy as np
 
-def legendre_polynomials(lmax,x,ASTYPE=np.float64):
+def legendre_polynomials(lmax, x, ASTYPE=np.float64):
     """
     Computes fully-normalized Legendre polynomials and their first derivative
     following [HofmannWellenhof2006]_
 
     Parameters
     ----------
     lmax: int
         maximum degree of Legrendre polynomials
-    x: float
+    x: np.ndarray
         elements ranging from -1 to 1
 
         Typically ``cos(theta)``, where ``theta`` is the colatitude in radians
-    ASTYPE: obj, default np.float64
+    ASTYPE: np.dtype, default np.float64
         output variable data type
 
     Returns
     -------
-    pl: float
+    pl: np.ndarray
         fully-normalized Legendre polynomials
-    dpl: float
+    dpl: np.ndarray
         first derivative of Legendre polynomials
 
     References
     ----------
     .. [HofmannWellenhof2006] B. Hofmann-Wellenhof and H. Moritz,
         *Physical Geodesy*, 2nd Edition, 403 pp., (2006).
-        `doi: 10.1007/978-3-211-33545-1 <https://doi.org/10.1007/978-3-211-33545-1>`_
+        `doi: 10.1007/978-3-211-33545-1
+        <https://doi.org/10.1007/978-3-211-33545-1>`_
     """
     # verify dimensions
     x = np.atleast_1d(x).flatten().astype(ASTYPE)
     # size of the x array
     nx = len(x)
     # verify data type of spherical harmonic truncation
     lmax = np.int64(lmax)
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/mascons.py` & `gravity-toolkit-1.2.1/gravity_toolkit/mascons.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 #!/usr/bin/env python
 u"""
 mascons.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (03/2023)
 Conversion routines for publicly available GRACE/GRACE-FO mascon solutions
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 REFERENCES:
     grid2mascon.m written by Felix Landerer and David Wiese (JPL)
     mascon2grid.m written by Felix Landerer and David Wiese (JPL)
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 11/2022: use lowercase keyword arguments
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 10/2021: publicly released version
     Updated 09/2019: check number of latitude points for using regional grids
     Updated 08/2018: add GSFC grid and mascon conversion routines
         extract number of mascons and number of variables
         use lat_bound and lon_bound variables as inputs to JPL conversion
@@ -30,36 +31,36 @@
 
 def to_gsfc(gdata, lon, lat, lon_center, lat_center, lon_span, lat_span):
     """
     Converts an input gridded field to an output GSFC mascon array
 
     Parameters
     ----------
-    gdata: float
+    gdata: np.ndarray
         gridded data
-    lon: float
+    lon: np.ndarray
         column vector of defined longitude points
-    lat: float
+    lat: np.ndarray
         column vector of defined latitude points
-    lon_center: float
+    lon_center: np.ndarray
         mascon longitudinal center points
-    lat_center: float
+    lat_center: np.ndarray
         mascon latitudinal center points
-    lon_span: float
+    lon_span: np.ndarray
         mascon longitudinal central angles
-    lat_span: float
+    lat_span: np.ndarray
         mascon latitudinal central angles
 
     Returns
     -------
-    data: float
+    data: np.ndarray
         row vector of mascons
-    lat_center: float
+    lat_center: np.ndarray
         row vector of latitude values for mascon centers
-    lon_center: float
+    lon_center: np.ndarray
         row vector of longitude values for mascon centers
 
     References
     ----------
     .. [Luthcke2013] S. B. Luthcke, T. J. Sabaka, B. D. Loomis,
         A. A. Arendt, J. J. McCarthy, and J. Camp,
         "Antarctica, Greenland and Gulf of Alaska land-ice evolution
@@ -119,34 +120,34 @@
 
 def to_jpl(gdata, lon, lat, lon_bound, lat_bound):
     """
     Converts an input gridded field to an output JPL mascon array
 
     Parameters
     ----------
-    gdata: float
+    gdata: np.ndarray
         gridded data
-    lon: float
+    lon: np.ndarray
         column vector of defined longitude points
-    lat: float
+    lat: np.ndarray
         column vector of defined latitude points
-    lon_bound: float
+    lon_bound: np.ndarray
         mascon longitudinal bounds from coordinate file
-    lat_bound: float
+    lat_bound: np.ndarray
         mascon latitudinal bounds from coordinate file
 
     Returns
     -------
-    data: float
+    data: np.ndarray
         row vector of mascons
-    mask: float
+    mask: np.ndarray
         row vector of mask values showing if mascon has no data
-    lat: float
+    lat: np.ndarray
         row vector of latitude values for mascons
-    lon: float
+    lon: np.ndarray
         row vector of longitude values for mascons
 
     References
     ----------
     .. [Watkins2015] M. M. Watkins, D. N. Wiese, D.-N. Yuan, C. Boening,
         and F. W. Landerer, "Improved methods for observing Earth's time
         variable mass distribution with GRACE using spherical cap mascons".
@@ -194,32 +195,32 @@
 def from_gsfc(mscdata, grid_spacing, lon_center, lat_center, lon_span, lat_span,
     **kwargs):
     """
     Converts an input GSFC mascon array to an output gridded field
 
     Parameters
     ----------
-    mscdata: float
+    mscdata: np.ndarray
         row vector of mascons
-    grid_spacing: float
+    grid_spacing: np.ndarray
         spacing of the lat/lon grid
     lon_center: float
-        mascon longitudinal center points
-    lat_center: float
+        mascon np.ndarray center points
+    lat_center: np.ndarray
         mascon latitudinal center points
-    lon_span: float
+    lon_span: np.ndarray
         mascon longitudinal central angles
-    lat_span: float
+    lat_span: np.ndarray
         mascon latitudinal central angles
     transpose: bool, default False
         transpose output matrix (lon/lat)
 
     Returns
     -------
-    mdata: float
+    mdata: np.ndarray
         distributed mass grid
 
     References
     ----------
     .. [Luthcke2013] S. B. Luthcke, T. J. Sabaka, B. D. Loomis,
         A. A. Arendt, J. J. McCarthy, and J. Camp,
         "Antarctica, Greenland and Gulf of Alaska land-ice evolution
@@ -243,22 +244,22 @@
     # convert mascon centers to -180:180
     gt180, = np.nonzero(lon_center > 180)
     lon_center[gt180] -= 360.0
 
     # Define output latitude and longitude grids
     lon = np.arange(-180.0+grid_spacing/2.0,180.0+grid_spacing/2.0,grid_spacing)
     lat = np.arange(90.0-grid_spacing/2.0,-90.0-grid_spacing/2.0,-grid_spacing)
-    nlon,nlat = (len(lon),len(lat))
+    nlon, nlat = (len(lon),len(lat))
     # for mascons centered on 180: use 0:360
     alon = np.copy(lon)
     lt0, = np.nonzero(lon < 0)
     alon[lt0] += 360.0
 
     # loop over each mascon bin and assign value to grid points inside bin:
-    mdata = np.zeros((nlat,nlon))
+    mdata = np.zeros((nlat, nlon))
     for k in range(0, nmas):
         # create latitudinal and longitudinal bounds for mascon k
         if (lat_center[k] == 90.0) | (lat_center[k] == -90.0):
             # NH and SH polar mascons
             lon_bound = [0.0,360.0]
             lat_bound = lat_center[k] + np.array([-1.0,1.0])*lat_span[k]
         else:
@@ -287,28 +288,28 @@
 
 def from_jpl(mscdata, grid_spacing, lon_bound, lat_bound, **kwargs):
     """
     Converts an input JPL mascon array to an output gridded field
 
     Parameters
     ----------
-    mscdata: float
+    mscdata: np.ndarray
         row vector of mascons
-    grid_spacing: float
+    grid_spacing: np.ndarray
         spacing of lat/lon grid
-    lon_bound: float
+    lon_bound: np.ndarray
         mascon longitudinal bounds from coordinate file
-    lat_bound: float
+    lat_bound: np.ndarray
         mascon latitudinal bounds from coordinate file
     transpose: bool, default False
         transpose output matrix (lon/lat)
 
     Returns
     -------
-    mdata: float
+    mdata: np.ndarray
         distributed mass grid
 
     References
     ----------
     .. [Watkins2015] M. M. Watkins, D. N. Wiese, D.-N. Yuan, C. Boening,
         and F. W. Landerer, "Improved methods for observing Earth's time
         variable mass distribution with GRACE using spherical cap mascons".
@@ -330,18 +331,18 @@
     nmas,nvar = lat_bound.shape
 
     # Define latitude and longitude grids
     # output lon will not include 360
     # output lat will not include 90
     lon = np.arange(grid_spacing/2.0, 360.0+grid_spacing/2.0, grid_spacing)
     lat = np.arange(-90.0+grid_spacing/2.0, 90.0+grid_spacing/2.0, grid_spacing)
-    nlon,nlat = (len(lon),len(lat))
+    nlon, nlat = (len(lon),len(lat))
 
     # loop over each mascon bin and assign value to grid points inside bin:
-    mdata = np.zeros((nlat,nlon))
+    mdata = np.zeros((nlat, nlon))
     for k in range(0, nmas):
         I, = np.nonzero((lat >= lat_bound[k,1]) & (lat < lat_bound[k,0]))
         J, = np.nonzero((lon >= lon_bound[k,0]) & (lon < lon_bound[k,2]))
         I,J = (I[np.newaxis,:], J[:,np.newaxis])
         mdata[I,J] = mscdata[k]
 
     # return array
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/ocean_stokes.py` & `gravity-toolkit-1.2.1/gravity_toolkit/ocean_stokes.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 ocean_stokes.py
-Written by Tyler Sutterley (04/2022)
+Written by Tyler Sutterley (05/2023)
 
 Reads a land-sea mask and converts to a series of spherical harmonics
 
 INPUTS:
     LANDMASK: Mask file to use as input following Sutterley et al. (2020)
         1x1 degree mask distributed from UCAR as part of NCL
             https://www.ncl.ucar.edu/Applications/Data/cdf/landsea.nc
@@ -39,34 +39,38 @@
 
 REFERENCES:
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Self-Consistent Ice Mass
         Balance and Regional Sea Level From Time-Variable Gravity",
     Earth and Space Science, 7, 2020. https://doi.org/10.1029/2019EA000860
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: fixed docstring summary of ocean stokes function
+        improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 01/2021: added option VARNAME to generalize input masks
     Updated 12/2020: added simplify function to remove isolated points
     Updated 07/2020: added function docstrings
     Updated 06/2020: using spatial data class for input and output operations
     Updated 04/2020 for public release
     Updated 04/2020: added option LOVE for passing load love numbers
     Updated 09/2017: added option MASK for different input masks
     Updated 05/2015: added parameter MMAX for MMAX != LMAX
     Written 03/2015
 """
+import pathlib
 import numpy as np
 from gravity_toolkit.spatial import spatial
 from gravity_toolkit.gen_stokes import gen_stokes
 
 def ocean_stokes(LANDMASK, LMAX, MMAX=None, LOVE=None, VARNAME='LSMASK',
     SIMPLIFY=False):
     """
-    Converts data from spherical harmonic coefficients to a spatial field
+    Reads a land-sea mask and converts to a series of spherical harmonics
 
     Parameters
     ----------
     LANDMASK: str
         netCDF4 land-sea mask file
     LMAX: int
         maximum spherical harmonic degree
@@ -77,67 +81,71 @@
     VARNAME: str, default 'LSMASK'
         variable name for mask in netCDF4 file
     SIMPLIFY: bool, default False
         simplify land mask by removing isolated points
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
     """
+    # verify that land-sea mask file exists
+    LANDMASK = pathlib.Path(LANDMASK).expanduser().absolute()
+    if not LANDMASK.exists():
+        raise FileNotFoundError(f'{str(LANDMASK)} not found in file system')
     # maximum spherical harmonic order
     MMAX = np.copy(LMAX) if MMAX is None else MMAX
     # Read Land-Sea Mask of specified input file
     # 0=Ocean, 1=Land, 2=Lake, 3=Small Island, 4=Ice Shelf
     # Open the land-sea NetCDF file for reading
     landsea = spatial().from_netCDF4(LANDMASK,
         date=False, varname=VARNAME)
     # create land function
     nth,nphi = landsea.shape
-    land_function = np.zeros((nth,nphi),dtype=np.float64)
+    land_function = np.zeros((nth,nphi), dtype=np.float64)
     # combine land and island levels for land function
     indx,indy = np.nonzero((landsea.data >= 1) & (landsea.data <= 3))
     land_function[indx,indy] = 1.0
     # remove isolated points if specified
     if SIMPLIFY:
         land_function -= find_isolated_points(land_function)
     # ocean function reciprocal of land function
     ocean_function = 1.0 - land_function
     # convert to spherical harmonics (1 cm w.e.)
-    ocean_Ylms = gen_stokes(ocean_function.T,landsea.lon,landsea.lat,
-        UNITS=1,LMIN=0,LMAX=LMAX,MMAX=MMAX,LOVE=LOVE)
+    Ylms = gen_stokes(ocean_function.T, landsea.lon, landsea.lat,
+        UNITS=1, LMIN=0, LMAX=LMAX, MMAX=MMAX, LOVE=LOVE)
     # return the spherical harmonic coefficients
-    return ocean_Ylms
+    return Ylms
 
 def find_isolated_points(mask):
     """
-    Simplify mask by removing isolated points
+    Simplify a mask by removing isolated points
 
     Parameters
     ----------
-    mask: int
+    mask: np.ndarray
         land-sea mask
 
     Returns
     -------
-    isolated: int
+    isolated: np.ndarray
         simplified land-sea mask
     """
     nth,_ = mask.shape
     laplacian = -4.0*np.copy(mask)
     laplacian += mask*np.roll(mask,1,axis=1)
     laplacian += mask*np.roll(mask,-1,axis=1)
     temp = np.roll(mask,1,axis=0)
     temp[0,:] = mask[1,:]
     laplacian += mask*temp
     temp = np.roll(mask,-1,axis=0)
     temp[nth-1,:] = mask[nth-2,:]
     laplacian += mask*temp
     # create mask of isolated points
     isolated = np.where(np.abs(laplacian) >= 3, 1, 0)
-    return isolated
+    return isolated
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/read_GIA_model.py` & `gravity-toolkit-1.2.1/gravity_toolkit/read_GIA_model.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 read_GIA_model.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Reads GIA data files that can come in various formats depending on the group
 Outputs spherical harmonics for the GIA rates and the GIA model parameters
 Can also output fully normalized harmonics to netCDF4 or HDF5 formatted files
 
 INPUTS:
     input_file: GIA file to read
@@ -92,15 +92,18 @@
 
     W. R. Peltier, D. F. Argus, and R. Drummond, "Comment on 'An Assessment of
     the ICE-6G_C (VM5a) Glacial Isostatic Adjustment Model' by Purcell et al."
     Journal of Geophysical Research: Solid Earth, 123(2), 2019-2028 (2018).
     https://doi.org/10.1002/2016JB013844
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: case insensitive searching for HDF5 and netCDF4 attributes
+        convert shape and ndim to harmonic class properties
+        improve typing for variables in docstrings
     Updated 02/2023: use monospaced text for harmonics objects in gia docstring
     Updated 12/2022: made interited GIA model harmonics class
         set default parameters, title, reference and url as None
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: use logging for debugging level verbose output
     Updated 05/2022: output full citation for each GIA model group
     Updated 04/2022: updated docstrings to numpy documentation format
@@ -134,23 +137,24 @@
         changed Wu code to convert to numerical array first
             then unwrapping the numerical array (vs. string)
     Updated 05/2013: converted to python and updated SM09 with latest best
         updated SM09 parameters to be automated from file input
     Updated 12/2012: changed the naming scheme for Simpson and Whitehouse
     Updated 09/2012: combined several GIA read programs into this standard
 """
-from __future__ import print_function
-
-import os
 import re
 import copy
 import logging
+import pathlib
 import numpy as np
 from gravity_toolkit.harmonics import harmonics
 
+_known_gia_types = ['IJ05-R2', 'W12a', 'SM09', 'ICE6G', 'ICE6G-D', 'Wu10',
+    'AW13-ICE6G', 'AW13-IJ05', 'Caron', 'ascii', 'netCDF4', 'HDF5']
+
 def read_GIA_model(input_file, GIA=None, MMAX=None, DATAFORM=None, **kwargs):
     """
     Reads Glacial Isostatic Adjustment (GIA) data files
 
     Parameters
     ----------
     input_file: str
@@ -184,21 +188,21 @@
             - ``'HDF5'``: output to HDF5 format (.H5)
 
     MODE: oct, default 0o775
         Permissions mode of output spherical harmonic files
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonic coefficients
-    l: int
+    l: np.ndarray
         spherical harmonic degree
-    m: int
+    m: np.ndarray
         spherical harmonic order
     title: str
         parameters of GIA model
     citation: str
         abbreviated citation for GIA model
     reference: str
         full citation for GIA model
@@ -413,29 +417,32 @@
     # initially read for spherical harmonic degree up to LMAX
     # will truncate to MMAX before exiting program
     gia_Ylms['clm'] = np.zeros((LMAX+1,LMAX+1))
     gia_Ylms['slm'] = np.zeros((LMAX+1,LMAX+1))
     # output spherical harmonic degree and order
     gia_Ylms['l'],gia_Ylms['m'] = (np.arange(LMAX+1),np.arange(LMAX+1))
 
+    # if reading a GIA model
+    if GIA in _known_gia_types:
+        # check that GIA data file is present in file system
+        input_file = pathlib.Path(input_file).expanduser().absolute()
+        if not input_file.exists():
+            raise FileNotFoundError(f'{str(input_file)} not found')
+        # log GIA file if debugging
+        logging.debug(f'Reading GIA file: {str(input_file)}')
+
     # Reading GIA files (ICE-6G and Wu have more complex formats)
     if GIA in ('IJ05-R2', 'W12a', 'SM09', 'AW13-ICE6G', 'AW13-IJ05'):
         # AW13, IJ05, W12a, SM09
         # AW13 notes: file headers
         # IJ05 notes: need to scale by 1e-11 for geodesy-normalization
         # exponents are denoted with D for double
 
-        # check that GIA data file is present in file system
-        input_file = os.path.expanduser(input_file)
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
-        # opening gia data file and read contents
-        with open(input_file, mode='r', encoding='utf8') as f:
+        # opening GIA data file and read contents
+        with input_file.open(mode='r', encoding='utf8') as f:
             gia_data = f.read().splitlines()
         # number of lines in file
         gia_lines = len(gia_data)
 
         # Skipping file header for geruo files with header
         for ii in range(start,gia_lines):
             # check if contents in line
@@ -455,22 +462,16 @@
 
     elif (GIA == 'ICE6G'):
         # ICE-6G VM5 notes
         # need to scale by 1e-11 for geodesy-normalization
         # spherical harmonic degrees listed only on order 0
         # spherical harmonic order is not listed in file
 
-        # check that GIA data file is present in file system
-        input_file = os.path.expanduser(input_file)
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
-        # opening gia data file and read contents
-        with open(input_file, mode='r', encoding='utf8') as f:
+        # opening GIA data file and read contents
+        with input_file.open(mode='r', encoding='utf8') as f:
             gia_data = f.read().splitlines()
 
         # counter variable
         ii = 0
         for l in range(0, LMAX+1):
             for m in range(0, l+1):
                 if ((m % 2) == 0):
@@ -495,21 +496,15 @@
                     gia_Ylms['slm'][l,m] = np.float64(line[1+c])*scale
 
     elif (GIA == 'Wu10'):
         # Wu (2010) notes:
         # Need to convert from mm geoid to fully normalized
         rad_e = 6.371e9# Average Radius of the Earth [mm]
 
-        # check that GIA data file is present in file system
-        input_file = os.path.expanduser(input_file)
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
-        # The file starts with a header.
+        # note: the GIA file starts with a header
         # converting to numerical array (note 64 bit floating point)
         gia_data = np.loadtxt(input_file, skiprows=1, dtype='f8')
 
         # counter variable to upwrap gia file
         ii = 0
 
         # Order of harmonics in the file:
@@ -530,21 +525,15 @@
                     if (m != 0) and (cs == 1):
                         gia_Ylms['slm'][l,m] = gia_data[ii]/rad_e
                         ii += 1
 
     elif (GIA == 'Caron'):
         # Caron et al. (2018)
 
-        # check that GIA data file is present in file system
-        input_file = os.path.expanduser(input_file)
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
-        # The file starts with a header.
+        # note: the GIA file starts with a header.
         # converting to numerical array (note 64 bit floating point)
         dtype = {'names':('l','m','Ylms'),'formats':('i','i','f8')}
         gia_data=np.loadtxt(input_file, skiprows=4, dtype=dtype)
         # Order of harmonics in the file
         #    0    0   c
         #    1    1   s
         #    1    0   c
@@ -560,22 +549,16 @@
                 gia_Ylms['clm'][l,m] = Ylm.copy()
             elif (m < 0) and (l <= LMAX) and (m <= LMAX):# Slm
                 gia_Ylms['slm'][l,np.abs(m)] = Ylm.copy()
 
     # Reading ICE-6G Version-D  GIA files
     elif (GIA == 'ICE6G-D'):
 
-        # check that GIA data file is present in file system
-        input_file = os.path.expanduser(input_file)
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
-        # opening gia data file and read contents
-        with open(input_file, mode='r', encoding='utf8') as f:
+        # opening GIA data file and read contents
+        with input_file.open(mode='r', encoding='utf8') as f:
             gia_data = f.read().splitlines()
         # number of lines in file
         gia_lines = len(gia_data)
 
         # find header lines to skip
         h1 = r'^GRACE Approximation for degrees 0 to 2'
         h2 = r'^GRACE Approximation\/Absolute Sea-level Values for degrees \> 2'
@@ -618,31 +601,27 @@
                     # scaling to geodesy normalization
                     gia_Ylms['clm'][l1,m1] = np.float64(line[2])*scale
                     gia_Ylms['slm'][l1,m1] = np.float64(line[3])*scale
 
 
     # ascii: reformatted GIA in ascii format
     elif (GIA == 'ascii'):
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
         # reading GIA data from reformatted (simplified) ascii files
         Ylms = harmonics().from_ascii(input_file, date=False)
         Ylms.truncate(LMAX)
         gia_Ylms.update(Ylms.to_dict())
         # copy filename (without extension) for parameters
-        gia_Ylms['title'] = os.path.basename(os.path.splitext(input_file)[0])
+        gia_Ylms['title'] = input_file.stem
         gia_Ylms['citation'] = None
         gia_Ylms['reference'] = None
         gia_Ylms['url'] = None
 
     # netCDF4: reformatted GIA in netCDF4 format
     # HDF5: reformatted GIA in HDF5 format
     elif GIA in ('netCDF4','HDF5'):
-        # log GIA file if debugging
-        logging.debug(f'Reading GIA file: {input_file}')
         # reading GIA data from reformatted netCDF4 and HDF5 files
         Ylms = harmonics().from_file(input_file, format=GIA, date=False)
         Ylms.truncate(LMAX)
         gia_Ylms.update(Ylms.to_dict())
         # copy title and reference for model
         for att_name in ('title','citation','reference','url'):
             try:
@@ -653,73 +632,69 @@
                 gia_Ylms[att_name] = None
 
     # GIA model parameter strings
     # extract rheology from the file name
     if (GIA == 'IJ05-R2'):
         # IJ05-R2: Ivins R2 GIA Models
         # adding file specific earth parameters
-        parameters, = re.findall(file_pattern,os.path.basename(input_file))
+        parameters, = re.findall(file_pattern, input_file.name)
         gia_Ylms['title'] = f'{prefix}_{parameters}'
     elif (GIA == 'ICE6G'):
         # ICE6G: ICE-6G GIA Models
         # adding file specific earth parameters
-        parameters, = re.findall(file_pattern,os.path.basename(input_file))
+        parameters, = re.findall(file_pattern, input_file.name)
         gia_Ylms['title'] = f'{prefix}_{parameters}'
     elif (GIA == 'W12a'):
         # W12a: Whitehouse GIA Models
         # for Whitehouse W12a (BEST, LOWER, UPPER):
-        model = re.findall(file_pattern,os.path.basename(input_file)).pop()
+        model = re.findall(file_pattern, input_file.name).pop()
         gia_Ylms['title'] = f'{prefix}_{parameters[model]}'
     elif (GIA == 'SM09'):
         # SM09: Simpson/Milne GIA Models
         # making parameters in the file similar to IJ05
         # split rheological parameters between lithospheric thickness,
         # upper mantle viscosity and lower mantle viscosity
-        LTh,UMV,LMV=re.findall(file_pattern,os.path.basename(input_file)).pop()
+        LTh,UMV,LMV = re.findall(file_pattern, input_file.name).pop()
         # formatting rheology parameters similar to IJ05 models
         gia_Ylms['title'] = f'{prefix}_{LTh}_.{UMV}_{LMV}'
     elif (GIA == 'Wu10'):
         # Wu10: Wu (2010) GIA Correction
         gia_Ylms['title'] = 'Wu_2010'
     elif (GIA == 'Caron'):
         # Caron: Caron JPL GIA Assimilation
         gia_Ylms['title'] = 'Caron_expt'
     elif (GIA == 'ICE6G-D'):
         # ICE6G-D: ICE-6G Version-D GIA Models
         # adding file specific earth parameters
-        m1,p1,p2 = re.findall(file_pattern,os.path.basename(input_file)).pop()
+        m1,p1,p2 = re.findall(file_pattern, input_file.name).pop()
         gia_Ylms['title'] = f'{prefix}_{p1}{p2}'
     elif (GIA == 'AW13-ICE6G'):
         # AW13-ICE6G: Geruo A ICE-6G GIA Models
         # extract the ice history and case flags
-        hist,case,sf=re.findall(file_pattern,os.path.basename(input_file)).pop()
+        hist,case,sf=re.findall(file_pattern, input_file.name).pop()
         gia_Ylms['title'] = f'{prefix}_{hist}_{case}'
     elif (GIA == 'AW13-IJ05'):
         # AW13-IJ05: Geruo A IJ05-R2 GIA Models
         # adding file specific earth parameters
-        vrs,param,aux=re.findall(file_pattern,os.path.basename(input_file)).pop()
+        vrs,param,aux=re.findall(file_pattern, input_file.name).pop()
         gia_Ylms['title'] = f'{prefix}_{vrs}_{param}'
 
     # output harmonics to ascii, netCDF4 or HDF5 file
     if DATAFORM in ('ascii', 'netCDF4', 'HDF5'):
         # convert dictionary to harmonics object
         Ylms = harmonics().from_dict(gia_Ylms)
-        # attributes for output files
-        attributes = {}
-        attributes['title'] = copy.copy(gia_Ylms['title'])
-        attributes['reference'] = copy.copy(gia_Ylms['reference'])
+        title = copy.copy(gia_Ylms['title'])
         # output harmonics to file
         suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
-        args = (gia_Ylms['title'], LMAX, suffix[DATAFORM])
-        output_file = 'stokes_{0}_L{1:d}.{2}'.format(*args)
-        Ylms.to_file(os.path.join(os.path.dirname(input_file),output_file),
-            format=DATAFORM, date=False, **attributes)
+        filename = f'stokes_{title}_L{LMAX:d}.{suffix[DATAFORM]}'
+        output_file = input_file.with_name(filename)
+        Ylms.to_file(output_file, format=DATAFORM, date=False,
+            title=title, reference=gia_Ylms['reference'])
         # set permissions level of output file
-        os.chmod(os.path.join(os.path.dirname(input_file),output_file),
-            mode=kwargs['MODE'])
+        output_file.chmod(mode=kwargs['MODE'])
 
     # truncate to MMAX if specified
     if MMAX is not None:
         # spherical harmonic variables
         gia_Ylms['clm'] = gia_Ylms['clm'][:,:MMAX+1]
         gia_Ylms['slm'] = gia_Ylms['slm'][:,:MMAX+1]
         # spherical harmonic order
@@ -735,24 +710,20 @@
 
     Attributes
     ----------
     lmax: int
         maximum degree of the spherical harmonic field
     mmax: int
         maximum order of the spherical harmonic field
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonics
-    slm: float
+    slm: np.ndarray
         sine spherical harmonics
     attributes: dict
         attributes of harmonics variables
-    shape: tuple
-        dimensions of harmonics object
-    ndim: int
-        number of dimensions of harmonics object
     filename: str
         input or output filename
     """
     np.seterr(invalid='ignore')
     # inherit harmonics class to read GIA models
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
@@ -808,10 +779,10 @@
         self.lmax = np.max(Ylms['l'])
         self.mmax = np.max(Ylms['m'])
         # copy information for GIA model
         self.title = Ylms['title']
         self.citation = Ylms['citation']
         self.reference = Ylms['reference']
         self.url = Ylms['url']
-        # assign shape and ndim attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/read_GRACE_harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/read_GRACE_harmonics.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 read_GRACE_harmonics.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte
 
 Reads GRACE files and extracts spherical harmonic data and drift rates (RL04)
 Adds drift rates to clm and slm for release 4 harmonics
 Correct GSM data for drift in pole tide following Wahr et al. (2015)
 Parses date of GRACE/GRACE-FO data from filename
 
@@ -38,14 +38,17 @@
     PyYAML: YAML parser and emitter for Python
         https://github.com/yaml/pyyaml
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: added regex formatting for CNES GRGS harmonics
+        improve typing for variables in docstrings
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 10/2022: make keyword arguments part of kwargs dictionary
     Updated 05/2022: updated comments
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
         check if GRACE/GRACE-FO data file is present in file-system
     Updated 09/2021: added COST-G combined solutions from the GFZ ICGEM
@@ -59,19 +62,19 @@
     Updated 08/2019: specify yaml loader (PyYAML yaml.load(input) Deprecation)
     Updated 07/2019: replace colons in yaml header if within quotations
     Updated 11/2018: decode gzip read with ISO-8859-1 for python3 compatibility
     Updated 05/2018: updates to file name structure with release 6 and GRACE-FO
         output file headers and parse new YAML headers for RL06 and GRACE-FO
     Written 10/2017 for public release
 """
-import os
 import re
 import io
 import gzip
 import yaml
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 
 # PURPOSE: read Level-2 GRACE and GRACE-FO spherical harmonic files
 def read_GRACE_harmonics(input_file, LMAX, **kwargs):
     """
     Extracts spherical harmonic coefficients from GRACE/GRACE-FO files
@@ -91,25 +94,25 @@
     -------
     time: float
         mid-month date in year-decimal
     start: float
         start date of range as Julian day
     end: float
         end date of range as Julian day
-    l: int
+    l: np.ndarray
         spherical harmonic degree to LMAX
-    m: int
+    m: np.ndarray
         spherical harmonic order to MMAX
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonics coefficients
-    slm: float
+    slm: np.ndarray
         sine spherical harmonics coefficients
-    eclm: float
+    eclm: np.ndarray
         cosine spherical harmonic uncalibrated standard deviations
-    eslm: float
+    eslm: np.ndarray
         sine spherical harmonic uncalibrated standard deviations
     header: str
         Header text from the GRACE/GRACE-FO file
 
     References
     ----------
     .. [Wahr2015] J. Wahr, R. S. Nerem, and S. V. Bettadpur, "The pole tide
@@ -179,23 +182,25 @@
 
     # set maximum spherical harmonic order
     MMAX = np.copy(LMAX) if (kwargs['MMAX'] is None) else np.copy(kwargs['MMAX'])
     # output dimensions
     grace_L2_input['l'] = np.arange(LMAX+1)
     grace_L2_input['m'] = np.arange(MMAX+1)
     # Spherical harmonic coefficient matrices to be filled from data file
-    grace_L2_input['clm'] = np.zeros((LMAX+1,MMAX+1))
-    grace_L2_input['slm'] = np.zeros((LMAX+1,MMAX+1))
+    grace_L2_input['clm'] = np.zeros((LMAX+1, MMAX+1))
+    grace_L2_input['slm'] = np.zeros((LMAX+1, MMAX+1))
     # spherical harmonic uncalibrated standard deviations
-    grace_L2_input['eclm'] = np.zeros((LMAX+1,MMAX+1))
-    grace_L2_input['eslm'] = np.zeros((LMAX+1,MMAX+1))
+    grace_L2_input['eclm'] = np.zeros((LMAX+1, MMAX+1))
+    grace_L2_input['eslm'] = np.zeros((LMAX+1, MMAX+1))
     if ((DREL == 4) and (DSET == 'GSM')):
         # clm and slm drift rates for RL04
-        drift_c = np.zeros((LMAX+1,MMAX+1))
-        drift_s = np.zeros((LMAX+1,MMAX+1))
+        drift_c = np.zeros((LMAX+1, MMAX+1))
+        drift_s = np.zeros((LMAX+1, MMAX+1))
+    # set default degree 0 harmonics for intercomparability between centers
+    grace_L2_input['clm'][0, 0] = 1.0
 
     # extract GRACE and GRACE-FO file headers
     # replace colons in header if within quotations
     head = [re.sub(r'\"(.*?)\:\s(.*?)\"',r'"\1, \2"',l) for l in file_contents
         if not re.match(rf'{FLAG}|GRDOTA',l)]
     if SFX in ('.gfc',):
         # extract parameters from header
@@ -301,23 +306,25 @@
     # UTCSR: The University of Texas at Austin Center for Space Research
     # EIGEN: GFZ German Research Center for Geosciences (RL01-RL05)
     # GFZOP: GFZ German Research Center for Geosciences (RL06+GRACE-FO)
     # JPLEM: NASA Jet Propulsion Laboratory (harmonic solutions)
     # JPLMSC: NASA Jet Propulsion Laboratory (mascon solutions)
     # GRGS: French Centre National D'Etudes Spatiales (CNES)
     # COSTG: International Combined Time-variable Gravity Fields
-    args = r'UTCSR|EIGEN|GFZOP|JPLEM|JPLMSC|GRGS|COSTG'
-    regex_pattern = (r'(.*?)-2_(\d{{4}})(\d{{3}})-(\d{{4}})(\d{{3}})_'
-        r'(.*?)_({0})_(.*?)_(\d+)(.*?)(\.gz|\.gfc)?$').format(args)
+    # GRGS: CNES Groupe de Recherche de Geodesie Spatiale
+    centers = r'UTCSR|EIGEN|GFZOP|JPLEM|JPLMSC|GRGS|COSTG|GRGS'
+    suffixes = r'\.gz|\.gfc|\.txt'
+    regex_pattern = (r'(.*?)-2_(\d{4})(\d{3})-(\d{4})(\d{3})_'
+        rf'(.*?)_({centers})_(.*?)_(\d+)(.*?)({suffixes})?$')
     rx = re.compile(regex_pattern, re.VERBOSE)
     # extract parameters from input filename
     if isinstance(input_file, io.IOBase):
         return rx.findall(input_file.filename).pop()
     else:
-        return rx.findall(os.path.basename(input_file)).pop()
+        return rx.findall(pathlib.Path(input_file).name).pop()
 
 # PURPOSE: read input file and extract contents
 def extract_file(input_file, compressed):
     """
     Read input file and extract contents
 
     Parameters
@@ -325,18 +332,18 @@
     input_file: str
         GRACE/GRACE-FO Level-2 spherical harmonic data file
     compressed: bool
         denotes if the file is compressed
     """
     # tilde expansion of input file if not byteIO object
     if not isinstance(input_file, io.IOBase):
-        input_file = os.path.expanduser(input_file)
+        input_file = pathlib.Path(input_file).expanduser().absolute()
         # check that data file is present in file system
-        if not os.access(input_file, os.F_OK):
-            raise FileNotFoundError(f'{input_file} not found')
+        if not input_file.exists():
+            raise FileNotFoundError(f'{str(input_file)} not found')
     # check if file is uncompressed byteIO object
     if isinstance(input_file, io.IOBase) and not compressed:
         # extract spherical harmonic coefficients
         return input_file.read().decode('ISO-8859-1').splitlines()
     else:
         # check if file is compressed (read with gzip if gz)
         file_opener = gzip.open if compressed else open
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/read_SLR_harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/read_SLR_harmonics.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 read_SLR_harmonics.py
-Written by Tyler Sutterley (11/2022)
+Written by Tyler Sutterley (05/2023)
 
 Reads in low-degree spherical harmonic coefficients calculated from
     Satellite Laser Ranging (SLR) measurements
 
 Dataset distributed by UTCSR
     ftp://ftp.csr.utexas.edu/outgoing/cheng/slrgeo.5d561_187_naod
     http://download.csr.utexas.edu/pub/slr/degree_5/
@@ -46,14 +46,16 @@
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
         include utf-8 encoding in reads to be windows compliant
     Updated 12/2021: added function for converting from 7-day arcs
     Updated 11/2021: renamed module. added reader for GSFC weekly 5x5 fields
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 04/2021: renamed module. new SLR 5x5 format from CSR (see notes)
@@ -62,18 +64,18 @@
     Updated 12/2020: using utilities from time module
     Updated 07/2020: added function docstrings
     Updated 07/2019: following new format with mean field in header and no C6,0
     Updated 10/2018: using future division for python3 Compatibility
     Updated 10/2017: include the 6,0 and 6,1 coefficients in output Ylms
     Written 10/2017
 """
-from __future__ import print_function, division
+from __future__ import division
 
-import os
 import re
+import pathlib
 import numpy as np
 import gravity_toolkit.time
 
 # PURPOSE: wrapper function for calling individual readers
 def read_SLR_harmonics(SLR_file, **kwargs):
     """
     Wrapper function for reading spherical harmonic coefficients
@@ -82,17 +84,17 @@
     Parameters
     ----------
     SLR_file: str
         Satellite Laser Ranging file
     **kwargs: dict
         keyword arguments for input readers
     """
-    if bool(re.search(r'gsfc_slr_5x5c61s61',SLR_file,re.I)):
+    if bool(re.search(r'gsfc_slr_5x5c61s61', SLR_file.name, re.I)):
         return read_GSFC_weekly_6x1(SLR_file, **kwargs)
-    elif bool(re.search(r'CSR_Monthly_5x5_Gravity_Harmonics',SLR_file,re.I)):
+    elif bool(re.search(r'CSR_Monthly_5x5_Gravity_Harmonics', SLR_file.name, re.I)):
         return read_CSR_monthly_6x1(SLR_file, **kwargs)
     else:
         raise Exception(f'Unknown SLR file format {SLR_file}')
 
 # PURPOSE: read monthly degree harmonic data from Satellite Laser Ranging (SLR)
 def read_CSR_monthly_6x1(SLR_file, SCALE=1e-10, HEADER=True):
     """
@@ -106,40 +108,41 @@
     SCALE: float, default 1e-10
         Scale factor for converting to fully-normalized spherical harmonics
     HEADER: bool, default True
         File contains header text to be skipped
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         Cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         Sine spherical harmonic coefficients
-    error/clm: float
+    error/clm: np.ndarray
         Cosine spherical harmonic coefficient uncertainty
-    error/slm: float
+    error/slm: np.ndarray
         Sine spherical harmonic coefficients uncertainty
-    MJD: float
+    MJD: np.ndarray
         output date as Modified Julian Day
-    time: float
+    time: np.ndarray
         output date in year-decimal
 
     References
     ----------
     .. [Cheng2010] M. Cheng, J. C. Ries, and B. D. Tapley,
         "Variations of the Earth's figure axis from satellite laser ranging
         and GRACE", *Journal of Geophysical Research*, 116(B01409), (2010).
         `doi: 10.1029/2010JB000850 <https://doi.org/10.1029/2010JB000850>`_
     """
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
         raise FileNotFoundError('SLR file not found in file system')
 
     # read the file and get contents
-    with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+    with SLR_file.open(mode='r', encoding='utf8') as f:
         file_contents = f.read().splitlines()
     file_lines = len(file_contents)
 
     # spherical harmonic degree range (5x5 with 6,1)
     # new 5x5 fields no longer include geocenter components
     LMIN = 2
     LMAX = 6
@@ -255,37 +258,38 @@
     SCALE: float, default 1.0
         Scale factor for converting to fully-normalized spherical harmonics
     HEADER: bool, default True
         File contains header text to be skipped
 
     Returns
     -------
-    clm: float
+    clm: np.ndarray
         Cosine spherical harmonic coefficients
-    slm: float
+    slm: np.ndarray
         Sine spherical harmonic coefficients
-    MJD: float
+    MJD: np.ndarray
         output date as Modified Julian Day
-    time: float
+    time: np.ndarray
         output date in year-decimal
 
     References
     ----------
     .. [Loomis2020] B. D. Loomis, K. E. Rachlin, D. N. Wiese, F. W. Landerer,
         and S. B. Luthcke, "Replacing GRACE/GRACE-FO *C*\ :sub:`30` with
         satellite laser ranging: Impacts on Antarctic Ice Sheet mass change".
         *Geophysical Research Letters*, 47, (2020).
         `doi: 10.1029/2019GL085488 <https://doi.org/10.1029/2019GL085488>`_
     """
     # check that SLR file exists
-    if not os.access(os.path.expanduser(SLR_file), os.F_OK):
+    SLR_file = pathlib.Path(SLR_file).expanduser().absolute()
+    if not SLR_file.exists():
         raise FileNotFoundError('SLR file not found in file system')
 
     # read the file and get contents
-    with open(os.path.expanduser(SLR_file), mode='r', encoding='utf8') as f:
+    with SLR_file.open(mode='r', encoding='utf8') as f:
         file_contents = f.read().splitlines()
     file_lines = len(file_contents)
 
     # spherical harmonic degree range (5x5 with 6,1)
     LMIN = 2
     LMAX = 6
     n_harm = (LMAX**2 + 3*LMAX - LMIN**2 - LMIN)//2 - 5
@@ -351,19 +355,19 @@
     DATE: list, default []
         Output monthly time for central averages
     NEIGHBORS: int, default 28
         Number of days to use in average
 
     Returns
     -------
-    time: float
+    time: np.ndarray
         output date in year-decimal
-    month: int
+    month: np.ndarray
         GRACE/GRACE-FO month
-    data: float
+    data: np.ndarray
         monthly spherical harmonic coefficients
     """
     # duplicate time and harmonics
     tdec = np.repeat(t_in, 7)
     data = np.repeat(d_in, 7)
     # calculate daily dates to use in centered moving average
     tdec += (np.mod(np.arange(len(tdec)),7) - 3.5)/365.25
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/read_gfc_harmonics.py` & `gravity-toolkit-1.2.1/gravity_toolkit/read_gfc_harmonics.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 read_gfc_harmonics.py
-Written by Tyler Sutterley (04/2022)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte
 
 Reads gfc files and extracts spherical harmonics for Swarm and
     GRAZ GRACE/GRACE-FO data
 Parses date of GRACE/GRACE-FO/Swarm data from filename
 
 GRAZ: https://www.tugraz.at/institute/ifg/downloads/gravity-field-models
@@ -51,26 +51,28 @@
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
     read_ICGEM_harmonics.py: reads gravity model coefficients from GFZ ICGEM
     calculate_tidal_offset.py: calculates the C20 offset for a tidal system
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 09/2021: forked from read_ICGEM_harmonics in geoid toolkit
         use gravity toolkit time modules and reorganize structure
     Updated 05/2021: Add GRAZ/Swarm/COST-G ICGEM file
     Updated 03/2021: made degree of truncation LMAX a keyword argument
     Updated 07/2020: added function docstrings
     Updated 07/2019: split read and wrapper funciton into separate files
     Updated 07/2017: include parameters to change the tide system
     Written 12/2015
 """
-import os
 import re
+import pathlib
 import warnings
 import numpy as np
 import gravity_toolkit.time
 
 # attempt imports
 try:
     from geoid_toolkit.read_ICGEM_harmonics import read_ICGEM_harmonics
@@ -103,49 +105,56 @@
     -------
     time: float
         mid-month date in decimal form
     start: float
         Julian dates of the start date
     end: float
         Julian dates of the start date
-    l: int
+    l: np.ndarray
         spherical harmonic degree to maximum degree of data
-    m: int
+    m: np.ndarray
         spherical harmonic order to maximum degree of data
-    clm: float
+    clm: np.ndarray
         cosine spherical harmonics of input data
-    slm: float
+    slm: np.ndarray
         sine spherical harmonics of input data
-    eclm: float
+    eclm: np.ndarray
         cosine spherical harmonic standard deviations of type errors
-    eslm: float
+    eslm: np.ndarray
         sine spherical harmonic standard deviations of type errors
     modelname: str
         name of the gravity model
     earth_gravity_constant: str
         GM constant of the Earth for gravity model
     radius: str
         semi-major axis of the Earth for gravity model
     max_degree: str
         maximum degree and order for gravity model
     errors: str
         error type of the gravity model
     norm: str
         normalization of the spherical harmonics
     tide_system: str
-        Permanent tide system of gravity model (``'mean_tide'``, ``'zero_tide'``, ``'tide_free'``)
+        Permanent tide system of gravity model
+
+            - ``'mean_tide'``
+            - ``'zero_tide'``
+            - ``'tide_free'``
 
     Reference
     ---------
     .. [Losch2003] M. Losch and V. Seufer,
         "How to Compute Geoid Undulations (Geoid Height Relative
         to a Given Reference Ellipsoid) from Spherical Harmonic
         Coefficients for Satellite Altimetry Applications", (2003).
         `eprint ID: 11802 <http://mitgcm.org/~mlosch/geoidcookbook.pdf>`_
     """
+    # full path to input filename
+    input_file = pathlib.Path(input_file).expanduser().absolute()
+
     # regular expression operators for ITSG data and models
     itsg_products = []
     itsg_products.append(r'atmosphere')
     itsg_products.append(r'dealiasing')
     itsg_products.append(r'oceanBottomPressure')
     itsg_products.append(r'ocean')
     itsg_products.append(r'Grace2014')
@@ -154,49 +163,49 @@
     itsg_products.append(r'Grace_operational')
     itsg_pattern = (r'(AOD1B_RL\d+|model|ITSG)[-_]({0})(_n\d+)?_'
         r'(\d+)-(\d+)(\.gfc)').format(r'|'.join(itsg_products))
     # regular expression operators for Swarm data and models
     swarm_data = r'(SW)_(.*?)_(EGF_SHA_2)__(.*?)_(.*?)_(.*?)(\.gfc|\.ZIP)'
     swarm_model = r'(GAA|GAB|GAC|GAD)_Swarm_(\d+)_(\d{2})_(\d{4})(\.gfc|\.ZIP)'
     # extract parameters for each data center and product
-    if re.match(itsg_pattern, os.path.basename(input_file)):
+    if re.match(itsg_pattern, input_file.name):
         # compile numerical expression operator for parameters from files
         # GRAZ: Institute of Geodesy from GRAZ University of Technology
         rx = re.compile(itsg_pattern, re.VERBOSE | re.IGNORECASE)
         # extract parameters from input filename
-        PFX,PRD,trunc,year,month,SFX = rx.findall(input_file).pop()
+        PFX,PRD,trunc,year,month,SFX = rx.findall(input_file.name).pop()
         # number of days in each month for the calendar year
         dpm = gravity_toolkit.time.calendar_days(int(year))
         # create start and end date lists
         start_date = [int(year),int(month),1,0,0,0]
         end_date = [int(year),int(month),dpm[int(month)-1],23,59,59]
-    elif re.match(swarm_data, os.path.basename(input_file)):
+    elif re.match(swarm_data, input_file.name):
         # compile numerical expression operator for parameters from files
         # Swarm: data from Swarm satellite
         rx = re.compile(swarm_data, re.VERBOSE | re.IGNORECASE)
         # extract parameters from input filename
-        SAT,tmp,PROD,starttime,endtime,RL,SFX = rx.findall(input_file).pop()
+        SAT,tmp,PROD,starttime,endtime,RL,SFX = rx.findall(input_file.name).pop()
         start_date,_ = gravity_toolkit.time.parse_date_string(starttime)
         end_date,_ = gravity_toolkit.time.parse_date_string(endtime)
         # number of days in each month for the calendar year
         dpm = gravity_toolkit.time.calendar_days(start_date[0])
-    elif re.match(swarm_model, os.path.basename(input_file)):
+    elif re.match(swarm_model, input_file.name):
         # compile numerical expression operator for parameters from files
         # Swarm: dealiasing products for Swarm data
         rx = re.compile(swarm_data, re.VERBOSE | re.IGNORECASE)
         # extract parameters from input filename
-        PROD,trunc,month,year,SFX = rx.findall(input_file).pop()
+        PROD,trunc,month,year,SFX = rx.findall(input_file.name).pop()
         # number of days in each month for the calendar year
         dpm = gravity_toolkit.time.calendar_days(int(year))
         # create start and end date lists
         start_date = [int(year),int(month),1,0,0,0]
         end_date = [int(year),int(month),dpm[int(month)-1],23,59,59]
 
     # python dictionary with model input and headers
-    ZIP = bool(re.search('ZIP',SFX,re.IGNORECASE))
+    ZIP = bool(re.search('ZIP', SFX, re.IGNORECASE))
     model_input = read_ICGEM_harmonics(input_file, TIDE=TIDE,
         FLAG=FLAG, ZIP=ZIP)
 
     # start and end day of the year
     start_day = np.sum(dpm[:start_date[1]-1]) + start_date[2] + \
         start_date[3]/24.0 + start_date[4]/1440.0 + start_date[5]/86400.0
     end_day = np.sum(dpm[:end_date[1]-1]) + end_date[2] + \
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/read_love_numbers.py` & `gravity-toolkit-1.2.1/gravity_toolkit/read_love_numbers.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 #!/usr/bin/env python
 u"""
 read_love_numbers.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 
 Reads sets of load Love numbers from PREM and applies isomorphic parameters
-Linearly interpolates load love numbers for missing degrees
-Linearly extrapolates load love numbers beyond maximum degree of dataset
+Linearly interpolates load Love/Shida numbers for missing degrees
+Linearly extrapolates load Love/Shida numbers beyond maximum degree of dataset
 
 INPUTS:
-    love_numbers_file: Elastic load Love numbers file
+    love_numbers_file: Elastic load Love/Shida numbers file
         computed using Preliminary Reference Earth Model (PREM) outputs
 
 OUTPUTS:
-    kl: Love number of Gravitational Potential
     hl: Love number of Vertical Displacement
-    ll: Love number of Horizontal Displacement
+    kl: Love number of Gravitational Potential
+    ll: Love (Shida) number of Horizontal Displacement
 
 OPTIONS:
     LMAX: truncate or interpolate to maximum spherical harmonic degree
     HEADER: number of header lines to be skipped
     COLUMNS: column names of ascii file
         l: spherical harmonic degree
         hl: vertical displacement
         kl: gravitational potential
         ll: horizontal displacement
-    REFERENCE: Reference frame for calculating degree 1 love numbers
+    REFERENCE: Reference frame for calculating degree 1 Love/Shida numbers
         CF: Center of Surface Figure
         CL: Center of Surface Lateral Figure
         CH: Center of Surface Height Figure
         CM: Center of Mass of Earth System
         CE: Center of Mass of Solid Earth (default)
     FORMAT: format of output variables
         'dict': dictionary with variable keys as listed above
@@ -52,14 +52,16 @@
         Geophysical Journal International, 108(1), (1992)
     J. Wahr, M. Molenaar, and F. Bryan, "Time variability of the Earth's
         gravity field: Hydrological and oceanic effects and their possible
         detection using GRACE", Journal of Geophysical Research,
         103(B12), 30205-30229, (1998)
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 02/2023: fix degree zero case and add load love number formatter
         added options for hard and soft PREM sediment cases
         added data class for load love numbers with attributes for model
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: use logging for debugging level verbose output
     Updated 04/2022: updated docstrings to numpy documentation format
         added wrapper function for reading load Love numbers from file
@@ -79,45 +81,45 @@
         added while loop for skipping header text and option HEADER
     Updated 03/2015: using regular expressions and generic read
         Updated comments
     Updated 10/2013: minor changes to use the numpy genfromtxt function
     Updated 05/2013: python updates and comment updates
     Written 01/2012
 """
-import os
 import io
 import re
 import logging
+import pathlib
 import numpy as np
 from gravity_toolkit.utilities import get_data_path
 
-# PURPOSE: read load love numbers from PREM
+# PURPOSE: read load Love/Shida numbers from PREM
 def read_love_numbers(love_numbers_file, LMAX=None, HEADER=2,
     COLUMNS=['l','hl','kl','ll'], REFERENCE='CE', FORMAT='tuple'):
     """
-    Reads PREM load Love numbers file and applies isomorphic
+    Reads PREM load Love/Shida numbers file and applies isomorphic
     parameters [Dziewonski1981]_ [Blewett2003]_
 
     Parameters
     ----------
     love_numbers_file: str
-        Elastic load Love numbers file
+        Elastic load Love/Shida numbers file
     LMAX: int or NoneType, default None
         Truncate or interpolate to maximum spherical harmonic degree
     HEADER: int, default 2
         Number of header lines to be skipped
     COLUMNS: list
         Column names of ascii file
 
             - ``'l'``: spherical harmonic degree
             - ``'hl'``: vertical displacement
             - ``'kl'``: gravitational potential
             - ``'ll'``: horizontal displacement
     REFERENCE: str, default 'CE'
-        Reference frame of degree 1 love numbers
+        Reference frame of degree 1 Love/Shida numbers
 
             - ``'CF'``: Center of Surface Figure
             - ``'CL'``: Center of Surface Lateral Figure
             - ``'CH'``: Center of Surface Height Figure
             - ``'CM'``: Center of Mass of Earth System
             - ``'CE'``: Center of Mass of Solid Earth
     FORMAT: str, default 'tuple'
@@ -126,20 +128,20 @@
             - ``'dict'``: dictionary with variable keys as listed above
             - ``'tuple'``: tuple with variable order (``hl``, ``kl``, ``ll``)
             - ``'zip'``: aggregated variable sets
             - ``'class'``: ``love_numbers`` class
 
     Returns
     -------
-    kl: float
-        Love number of Gravitational Potential
-    hl: float
+    hl: np.ndarray
         Love number of Vertical Displacement
-    ll: float
-        Love number of Horizontal Displacement
+    kl: np.ndarray
+        Love number of Gravitational Potential
+    ll: np.ndarray
+        Love (Shida) number of Horizontal Displacement
 
     References
     ----------
     .. [Blewett2003] G. Blewitt, "Self-consistency in reference frames, geocenter
         definition, and surface loading of the solid Earth",
         *Journal of Geophysical Research: Solid Earth*, 108(B2), 2103, (2003).
         `doi: 10.1029/2002JB002082 <https://doi.org/10.1029/2002JB002082>`_
@@ -167,70 +169,70 @@
 
     .. [Wang2012] H. Wang et al., "Load Love numbers and Green's
         functions for elastic Earth models PREM, iasp91, ak135, and
         modified models with refined crustal structure from Crust 2.0",
         *Computers & Geosciences*, 49, 190--199, (2012).
         `doi: 10.1016/j.cageo.2012.06.022 <https://doi.org/10.1016/j.cageo.2012.06.022>`_
     """
-    # Input load love number data file and read contents
+    # Input load Love/Shida number data file and read contents
     file_contents = extract_love_numbers(love_numbers_file)
 
     # compile regular expression operator to find numerical instances
     regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?'
     rx = re.compile(regex_pattern, re.VERBOSE)
 
     # extract maximum spherical harmonic degree from final line in file
     if LMAX is None:
         LMAX = np.int64(rx.findall(file_contents[-1])[COLUMNS.index('l')])
 
-    # dictionary of output love numbers
+    # dictionary of output Love/Shida numbers
     love = {}
     # spherical harmonic degree
     love['l'] = np.arange(LMAX+1)
     # vertical displacement hl
     # gravitational potential kl
-    # horizontal displacement ll
+    # horizontal displacement ll (Shida number)
     for n in ('hl','kl','ll'):
         love[n] = np.zeros((LMAX+1))
     # check if needing to interpolate between degrees
     flag = np.ones((LMAX+1),dtype=bool)
     # for each line in the file (skipping header lines)
     for file_line in file_contents[HEADER:]:
         # find numerical instances in line
         # replacing fortran double precision exponential
         love_numbers = rx.findall(file_line.replace('D','E'))
         # spherical harmonic degree
         l = np.int64(love_numbers[COLUMNS.index('l')])
         # truncate to spherical harmonic degree LMAX
         if (l <= LMAX):
-            # convert love numbers to float
+            # convert Love/Shida numbers to float
             # vertical displacement hl
             # gravitational potential kl
-            # horizontal displacement ll
+            # horizontal displacement ll (Shida number)
             for n in ('hl','kl','ll'):
                 love[n][l] = np.float64(love_numbers[COLUMNS.index(n)])
             # set interpolation flag for degree
             flag[l] = False
 
-    # return love numbers in output format
+    # return Love/Shida numbers in output format
     if (LMAX == 0):
         return love_number_formatter(love, FORMAT=FORMAT)
 
-    # if needing to linearly interpolate love numbers
+    # if needing to linearly interpolate Love/Shida numbers
     if np.any(flag):
-        # linearly interpolate each load love number following Wahr (1998)
+        # linearly interpolate following Wahr (1998)
         for n in ('hl','kl','ll'):
             love[n][flag] = np.interp(love['l'][flag],
                 love['l'][~flag], love[n][~flag])
 
-    # if needing to linearly extrapolate love numbers
+    # if needing to linearly extrapolate Love/Shida numbers
     # NOTE: use caution if extrapolating far beyond the
-    # maximum degree of the love numbers dataset
+    # maximum degree of the Love/Shida numbers dataset
     for lint in range(l,LMAX+1):
-        # linearly extrapolate each load love number
+        # linearly extrapolate to maximum degree
         for n in ('hl','kl','ll'):
             love[n][lint] = 2.0*love[n][lint-1] - love[n][lint-2]
 
     # calculate isomorphic parameters for different reference frames
     # From Blewitt (2003), Wahr (1998), Trupin (1992) and Farrell (1972)
     if (REFERENCE.upper() == 'CF'):
         # Center of Surface Figure
@@ -249,118 +251,120 @@
         alpha = 0.0
     else:
         raise Exception(f'Invalid Reference Frame {REFERENCE}')
     # apply isomorphic parameters
     for n in ('hl','kl','ll'):
         love[n][1] -= alpha
 
-    # return love numbers in output format
+    # return Love/Shida numbers in output format
     return love_number_formatter(love, FORMAT=FORMAT)
 
-# PURPOSE: return load Love numbers in a particular format
+# PURPOSE: return load Love/Shida numbers in a particular format
 def love_number_formatter(love, FORMAT='tuple'):
     """
-    Converts a dictionary of Load Love Numbers
+    Converts a dictionary of Load Love/Shida Numbers
     to a particular output fomrat
 
     Parameters
     ----------
     love: dict
-        Load Love numbers
+        Load Love/Shida numbers
     FORMAT: str, default 'tuple'
         Format of output variables
 
             - ``'dict'``: dictionary with variable keys
             - ``'tuple'``: tuple with variable order (``hl``, ``kl``, ``ll``)
             - ``'zip'``: aggregated variable sets
             - ``'class'``: ``love_numbers`` class
 
     Returns
     -------
-    kl: float
-        Love number of Gravitational Potential
-    hl: float
+    hl: np.ndarray
         Love number of Vertical Displacement
-    ll: float
-        Love number of Horizontal Displacement
+    kl: np.ndarray
+        Love number of Gravitational Potential
+    ll: np.ndarray
+        Love (Shida) number of Horizontal Displacement
     """
     if (FORMAT == 'dict'):
         return love
     elif (FORMAT == 'tuple'):
         return (love['hl'], love['kl'], love['ll'])
     elif (FORMAT == 'zip'):
         return zip(love['hl'], love['kl'], love['ll'])
     elif (FORMAT == 'class'):
         return love_numbers().from_dict(love)
 
 # PURPOSE: read input file and extract contents
 def extract_love_numbers(love_numbers_file):
     """
-    Read load love number file and extract contents
+    Read load Love/Shida number file and extract contents
 
     Parameters
     ----------
-    love_numbers_file: str
-        Elastic load Love numbers file
+    love_numbers_file: str, bytesIO or pathlib.Path
+        Elastic load Love/Shida numbers file
     """
-    # check if input love numbers are a string or bytesIO object
-    if isinstance(love_numbers_file, str):
+    # check if input Love/Shida numbers are a string or bytesIO object
+    if isinstance(love_numbers_file, (str, pathlib.Path)):
         # tilde expansion of load love number data file
-        love_numbers_file = os.path.expanduser(love_numbers_file)
-        # check that load love number data file is present in file system
-        if not os.access(love_numbers_file, os.F_OK):
-            raise FileNotFoundError(f'{love_numbers_file} not found')
-        # Input load love number data file and read contents
-        with open(love_numbers_file, mode='r', encoding='utf8') as f:
+        love_numbers_file = pathlib.Path(love_numbers_file).expanduser().absolute()
+        # check that load Love/Shida number data file is present in file system
+        if not love_numbers_file.exists():
+            raise FileNotFoundError(f'{str(love_numbers_file)} not found')
+        # Input load Love/Shida number data file and read contents
+        with love_numbers_file.open(mode='r', encoding='utf8') as f:
             return f.read().splitlines()
     elif isinstance(love_numbers_file, io.IOBase):
-        # read contents from load love number data
+        # read contents from load Love/Shida number data
         return love_numbers_file.read().decode('utf8').splitlines()
+    else:
+        raise ValueError('Invalid Love/Shida numbers file input')
 
-# PURPOSE: read load love numbers for a range of spherical harmonic degrees
+# PURPOSE: read load Love/Shida numbers for a range of spherical harmonic degrees
 def load_love_numbers(LMAX, LOVE_NUMBERS=0, REFERENCE='CF', FORMAT='tuple'):
     """
-    Wrapper function for reading PREM load Love numbers for a
+    Wrapper function for reading PREM load Love/Shida numbers for a
     range of spherical harmonic degrees and applying
     isomorphic parameters [Blewett2003]_
 
     Parameters
     ----------
     LMAX: int
         maximum spherical harmonic degree
     LOVE_NUMBERS: int, default 0
-        Treatment of the Load Love numbers
+        Treatment of the Load Love/Shida numbers
 
             - ``0``: [Han1995]_ values from PREM
             - ``1``: [Gegout2010]_ values from PREM
             - ``2``: [Wang2012]_ values from PREM
             - ``3``: [Wang2012]_ values from PREM with hard sediment
             - ``4``: [Wang2012]_ values from PREM with soft sediment
     REFERENCE: str
-        Reference frame for calculating degree 1 love numbers [Blewett2003]_
+        Reference frame for calculating degree 1 Love/Shida numbers [Blewett2003]_
 
             - ``'CF'``: Center of Surface Figure (default)
             - ``'CM'``: Center of Mass of Earth System
             - ``'CE'``: Center of Mass of Solid Earth
     FORMAT: str, default 'tuple'
         Format of output variables
 
             - ``'dict'``: dictionary with variable keys as listed above
             - ``'tuple'``: tuple with variable order (``hl``, ``kl``, ``ll``)
             - ``'zip'``: aggregated variable sets
             - ``'class'``: ``love_numbers`` class
 
     Returns
     -------
-    kl: float
-        Love number of Gravitational Potential
-    hl: float
+    hl: np.ndarray
         Love number of Vertical Displacement
-    ll: float
-        Love number of Horizontal Displacement
+    kl: np.ndarray
+        Love number of Gravitational Potential
+    ll: np.ndarray
+        Love (Shida) number of Horizontal Displacement
 
     References
     ----------
     .. [Blewett2003] G. Blewitt, "Self-consistency in reference frames, geocenter
         definition, and surface loading of the solid Earth",
         *Journal of Geophysical Research: Solid Earth*, 108(B2), 2103, (2003).
         `doi: 10.1029/2002JB002082 <https://doi.org/10.1029/2002JB002082>`_
@@ -377,15 +381,15 @@
 
     .. [Wang2012] H. Wang et al., "Load Love numbers and Green's
         functions for elastic Earth models PREM, iasp91, ak135, and
         modified models with refined crustal structure from Crust 2.0",
         *Computers & Geosciences*, 49, 190--199, (2012).
         `doi: 10.1016/j.cageo.2012.06.022 <https://doi.org/10.1016/j.cageo.2012.06.022>`_
     """
-    # load love numbers file
+    # load Love/Shida numbers file
     if (LOVE_NUMBERS == 0):
         # PREM outputs from Han and Wahr (1995)
         # https://doi.org/10.1111/j.1365-246X.1995.tb01819.x
         love_numbers_file = get_data_path(['data','love_numbers'])
         model = 'PREM'
         citation = 'Han and Wahr (1995)'
         header = 2
@@ -422,61 +426,63 @@
         love_numbers_file = get_data_path(['data','PREMsoft-LLNs-truncated.dat'])
         model = 'PREMsoft'
         citation = 'Wang et al. (2012)'
         header = 1
         columns = ['l','hl','ll','kl','nl','nk']
     else:
         raise ValueError(f'Unknown Love Numbers Type {LOVE_NUMBERS:d}')
-    # log load love numbers file if debugging
-    logging.debug(f'Reading Love numbers file: {love_numbers_file}')
-    # LMAX of load love numbers from Han and Wahr (1995) is 696.
+    # validate as pathlib object
+    love_numbers_file = pathlib.Path(love_numbers_file).expanduser().absolute()
+    # log load Love/Shida numbers file if debugging
+    logging.debug(f'Reading Love/Shida numbers file: {str(love_numbers_file)}')
+    # LMAX of load Love/Shida numbers from Han and Wahr (1995) is 696.
     # from Wahr (2007) linearly interpolating kl works
     # however, as we are linearly extrapolating out, do not make
     # LMAX too much larger than 696
-    # read arrays of kl, hl, and ll Love Numbers
+    # read arrays of kl, hl, and ll Love/Shida Numbers
     love = read_love_numbers(love_numbers_file, LMAX=LMAX, HEADER=header,
         COLUMNS=columns, REFERENCE=REFERENCE, FORMAT=FORMAT)
     # append model and filename attributes to class
     if (FORMAT == 'class'):
-        love.filename = os.path.basename(love_numbers_file)
+        love.filename = love_numbers_file.name
         love.reference=REFERENCE
         love.model = model
         love.citation = citation
     # return the load love numbers
     return love
 
 class love_numbers(object):
     """
-    Data class for Load Love numbers
+    Data class for Load Love/Shida numbers
 
     Attributes
     ----------
     lmax: int
-        maximum degree of the Load Love Numbers
-    l: int
+        maximum degree of the Load Love/Shida Numbers
+    l: np.ndarray
         Spherical harmonic degrees
-    kl: float
-        Love number of Gravitational Potential
-    hl: float
+    hl: np.ndarray or List
         Love number of Vertical Displacement
-    ll: float
-        Love number of Horizontal Displacement
+    kl: np.ndarray or List
+        Love number of Gravitational Potential
+    ll: np.ndarray or List
+        Love (Shida) number of Horizontal Displacement
     reference: str
-        Reference frame for degree 1 love numbers
+        Reference frame for degree 1 Love/Shida numbers
 
             - ``'CF'``: Center of Surface Figure
             - ``'CM'``: Center of Mass of Earth System
             - ``'CE'``: Center of Mass of Solid Earth
 
     model: str
         Reference Earth Model
     citation: str
         Citation for Reference Earth Model
     filename: str
-        input filename of Load Love Numbers
+        input filename of Load Love/Shida Numbers
     """
     np.seterr(invalid='ignore')
     def __init__(self, **kwargs):
         # set default keyword arguments
         kwargs.setdefault('lmax',None)
         # set default class attributes
         self.hl=[]
@@ -495,15 +501,15 @@
         Convert a dict object to a ``love_numbers`` object
 
         Parameters
         ----------
         d: dict
             dictionary object to be converted
         """
-        # retrieve each Load Love Number
+        # retrieve each Load Love/Shida Number
         for key in ('hl','kl','ll'):
             setattr(self, key, d.get(key))
         self.lmax = len(self.hl) - 1
         # calculate spherical harmonic degree
         self.update_dimensions()
         return self
 
@@ -512,42 +518,42 @@
         Convert a ``love_numbers`` object to a dict object
 
         Returns
         -------
         d: dict
             output dictionary object
         """
-        # retrieve each Load Love Number
+        # retrieve each Load Love/Shida Number
         d = {}
         for key in ('hl','kl','ll'):
             d[key] = getattr(self, key)
         return d
 
     def to_tuple(self):
         """
         Convert a ``love_numbers`` object to a tuple object
 
         Returns
         -------
         t: tuple
             output tuple object
         """
-        # return Load Love Numbers
+        # return Load Love/Shida Numbers
         return (self.hl, self.kl, self.ll)
 
     def transform(self, reference):
         """
         Calculate and apply calculate isomorphic parameters to
         transform from the Center of Mass of the Solid Earth
         Reference Frame [Blewett2003]_
 
         Parameters
         ----------
         reference: str
-            Output reference frame for degree 1 love numbers
+            Output reference frame for degree 1 Love/Shida numbers
 
                 - ``'CF'``: Center of Surface Figure
                 - ``'CL'``: Center of Surface Lateral Figure
                 - ``'CH'``: Center of Surface Height Figure
                 - ``'CM'``: Center of Mass of Earth System
                 - ``'CE'``: Center of Mass of Solid Earth
 
@@ -595,12 +601,12 @@
 
     def __len__(self):
         """Number of degrees
         """
         return len(self.l)
 
     def __iter__(self):
-        """Iterate over load love numbers variables
+        """Iterate over load Love/Shida numbers variables
         """
         yield self.hl
         yield self.kl
         yield self.ll
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/sea_level_equation.py` & `gravity-toolkit-1.2.1/gravity_toolkit/sea_level_equation.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 #!/usr/bin/env python
 u"""
-sea_level_equation.py (01/2023)
+sea_level_equation.py (03/2023)
 Solves the sea level equation with the option of including polar motion feedback
 Uses a Clenshaw summation to calculate the spherical harmonic summation
 
 CALLING SEQUENCE:
     sea_level = sea_level_equation(loadClm, loadSlm, glon, glat, landmask,
         LMAX=LMAX, LOVE=(hl,kl,ll), BODY_TIDE_LOVE=0, FLUID_LOVE=0, POLAR=True,
         INTERATIONS=6, FILL_VALUE=0)
@@ -86,14 +86,15 @@
         Society, 64(3), 677--703, (1981).
         https://doi.org/10.1111/j.1365-246X.1981.tb02690.x
     J. M. Wahr, "Deformation induced by polar motion", Journal of
         Geophysical Research: Solid Earth, 90(B11), 9363--9368, (1985).
         https://doi.org/10.1029/JB090iB11p09363
 
 UPDATE HISTORY:
+    Updated 03/2023: improve typing for variables in docstrings
     Updated 01/2023: refactored associated legendre polynomials
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 10/2021: using python logging for handling verbose output
         can set custom values for BODY_TIDE_LOVE and FLUID_LOVE
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 01/2021: use harmonics class for spherical harmonic operations
@@ -134,23 +135,23 @@
     polar motion feedback [Farrell1976]_ [Kendall2005]_ [Mitrovica2003]_
 
     Uses a Clenshaw summation to calculate the spherical harmonic
     summation [Holmes2002]_ [Tscherning1982]_
 
     Parameters
     ----------
-    loadClm: float
+    loadClm: np.ndarray
         Cosine spherical harmonic coefficients
-    loadSlm: float
+    loadSlm: np.ndarray
         Sine spherical harmonic coefficients
-    glon: float
+    glon: np.ndarray
         longitude of the land-sea mask
-    glat: float
+    glat: np.ndarray
         latitude of the land-sea mask
-    land_function: int
+    land_function: np.ndarray
         land-sea mask with land=1
     LMAX: int, default 0
         Maximum spherical harmonic degree
     LOVE: tuple or NoneType, default None
         Load Love numbers up to degree LMAX (``hl``, ``kl``, ``ll``)
     BODY_TIDE_LOVE: int, default 0
         Treatment of the body tide Love number
@@ -166,26 +167,26 @@
             - ``2``: [Munk1960]_ fluid love number
             - ``3``: [Lambeck1980]  fluid love number
             - list or tuple: custom value ``(klf)``
     POLAR: bool, default True
         Include polar feedback
     ITERATIONS: int, default 6
         Maximum number of iterations for the solver
-    PLM: float or NoneType, default None
+    PLM: np.ndarray or NoneType, default None
         Legendre polynomials
     FILL_VALUE: float, default 0
         Invalid value used over land points
-    ASTYPE: obj, default np.longdouble
+    ASTYPE: np.dtype, default np.longdouble
         Floating point precision for calculating Clenshaw summation
     SCALE: float, default 1e-280
         Scaling factor to prevent underflow in Clenshaw summation
 
     Returns
     -------
-    sea_level: float
+    sea_level: np.ndarray
         spatial field calculated using sea level solver
 
     References
     ----------
     .. [Farrell1972] W. E. Farrell, "Deformation of the Earth by surface loads",
         *Reviews of Geophysics*, 10(3), 761--797, (1972).
         `doi: 10.1029/RG010i003p00761 <https://doi.org/10.1029/RG010i003p00761>`_
@@ -440,15 +441,43 @@
         sea_level[ii,jj] = FILL_VALUE
 
     # return the sea level spatial field
     return sea_level
 
 # PURPOSE: compute Clenshaw summation of the fully normalized associated
 # Legendre's function for constant order m
-def clenshaw_s_m(t, m, clm1, slm1, lmax, ASTYPE=np.longdouble, SCALE=1e-280):
+def clenshaw_s_m(t, m, clm1, slm1, lmax,
+    ASTYPE=np.longdouble, SCALE=1e-280
+    ):
+    """
+    Compute conditioned arrays for Clenshaw summation from the fully-normalized
+    associated Legendre's function for an order m
+
+    Parameters
+    ----------
+    t: np.ndarray
+        elements ranging from -1 to 1, typically cos(th)
+    m: int
+        spherical harmonic order
+    clm1: np.ndarray
+        cosine spherical harmonics
+    slm1: np.ndarray
+        sine spherical harmonics
+    lmax: int
+        maximum spherical harmonic degree
+    ASTYPE: np.dtype, default np.longdouble
+        floating point precision for calculating Clenshaw summation
+    SCALE: float, default 1e-280
+        scaling factor to prevent underflow in Clenshaw summation
+
+    Returns
+    -------
+    s_m_c: np.ndarray
+        conditioned array for clenshaw summation
+    """
     # allocate for output matrix
     N = len(t)
     s_m = np.zeros((N,2),dtype=ASTYPE)
     # scaling to prevent overflow
     clm = SCALE*clm1.astype(ASTYPE)
     slm = SCALE*slm1.astype(ASTYPE)
     # convert lmax and m to float
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/spatial.py` & `gravity-toolkit-1.2.1/gravity_toolkit/spatial.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 spatial.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Data class for reading, writing and processing spatial data
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
@@ -16,20 +16,28 @@
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
 
 PROGRAM DEPENDENCIES:
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+        more operatations on spatial error if in possible data keys
+        rename reverse function to flip to match numpy nomenclature
     Updated 03/2023: customizable file-level attributes to netCDF4 and HDF5
         add attributes fetching to from_dict function
         retrieve all root attributes from HDF5 and netCDF4 datasets
         fix indexing of filenames in single string case
         add indexing of filenames to spatial object iterator
         use copy.copy and not numpy.copy in copy spatial object function
+        fix mask and shape of subsetted spatial grid objects
+        add extend_matrix function and add error output to from_list
+        convert spacing, extent, shape and ndim to spatial class properties
+        improve typing for variables in docstrings
+        set case insensitive filename to None if filename is empty
     Updated 02/2023: use monospaced text to note spatial objects in docstrings
     Updated 12/2022: add software information to output HDF5 and netCDF4
         make spatial objects iterable and with length
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 08/2022: fix output latitude HDF5 and netCDF4 attributes
         place index filename within try/except statement
     Updated 04/2022: updated docstrings to numpy documentation format
@@ -56,134 +64,136 @@
     Updated 09/2020: added header option to skip rows in ascii files
     Updated 08/2020: added compression options for ascii, netCDF4 and HDF5 files
     Updated 07/2020: added class docstring and using kwargs for output to file
         added case_insensitive_filename function to search directories
     Updated 06/2020: added zeros_like() for creating an empty spatial object
     Written 06/2020
 """
-import os
 import re
 import io
 import copy
 import gzip
 import time
 import uuid
 import logging
+import pathlib
 import zipfile
 import warnings
 import numpy as np
 import gravity_toolkit.version
 from gravity_toolkit.time import adjust_months, calendar_to_grace
 
 # attempt imports
 try:
     import h5py
-except (ImportError, ModuleNotFoundError) as e:
+except (ImportError, ModuleNotFoundError) as exc:
     warnings.filterwarnings("module")
     warnings.warn("h5py not available", ImportWarning)
 try:
     import netCDF4
-except (ImportError, ModuleNotFoundError) as e:
+except (ImportError, ModuleNotFoundError) as exc:
     warnings.filterwarnings("module")
     warnings.warn("netCDF4 not available", ImportWarning)
 # ignore warnings
 warnings.filterwarnings("ignore")
 
 class spatial(object):
     """
     Data class for reading, writing and processing spatial data
 
     Attributes
     ----------
-    data: float
+    data: np.ndarray
         spatial grid data
-    mask: bool
+    mask: np.ndarray
         spatial grid mask
-    lon: float
+    lon: np.ndarray
         grid longitudes
-    lat: float
+    lat: np.ndarray
         grid latitudes
-    time: float
+    time: np.ndarray
         time variable of the spatial data
-    month: int
+    month: np.ndarray
         GRACE/GRACE-FO months variable of the spatial data
     fill_value: float or NoneType, default None
         invalid value for spatial grid data
     attributes: dict
         attributes of ``spatial`` variables
-    extent: list, default [None,None,None,None]
-        spatial grid bounds
-        ``[minimum longitude, maximum longitude,
-        minimum latitude, maximum latitude]``
-    spacing: list, default [None,None]
-        grid step size ``[longitude,latitude]``
-    shape: tuple
-        dimensions of ``spatial`` object
-    ndim: int
-        number of dimensions of ``spatial`` object
     filename: str
         input or output filename
 
     """
     np.seterr(invalid='ignore')
     def __init__(self, **kwargs):
         # set default keyword arguments
-        kwargs.setdefault('spacing',[None,None])
-        kwargs.setdefault('nlat',None)
-        kwargs.setdefault('nlon',None)
-        kwargs.setdefault('extent',[None]*4)
         kwargs.setdefault('fill_value',None)
         # set default class attributes
         self.data=None
         self.mask=None
         self.lon=None
         self.lat=None
         self.time=None
         self.month=None
         self.fill_value=kwargs['fill_value']
         self.attributes=dict()
-        self.extent=kwargs['extent']
-        self.spacing=kwargs['spacing']
-        self.shape=[kwargs['nlat'],kwargs['nlon'],None]
-        self.ndim=None
         self.filename=None
         # iterator
         self.__index__ = 0
 
-    def case_insensitive_filename(self,filename):
+    def case_insensitive_filename(self, filename):
         """
         Searches a directory for a filename without case dependence
 
         Parameters
         ----------
-        filename: str
+        filename: str, io.IOBase, pathlib.Path or None
             input filename
         """
         # check if filename is open file object
         if isinstance(filename, io.IOBase):
             self.filename = copy.copy(filename)
-        elif isinstance(filename, type(None)):
+        elif isinstance(filename, type(None)) or not bool(filename):
             self.filename = None
         else:
             # tilde-expand input filename
-            self.filename = os.path.expanduser(filename)
+            self.filename = pathlib.Path(filename).expanduser().absolute()
             # check if file presently exists with input case
-            if not os.access(self.filename,os.F_OK):
+            if not self.filename.exists():
                 # search for filename without case dependence
-                basename = os.path.basename(filename)
-                directory = os.path.dirname(os.path.expanduser(filename))
-                f = [f for f in os.listdir(directory) if re.match(basename,f,re.I)]
+                f = [f.name for f in self.filename.parent.iterdir() if
+                    re.match(self.filename.name, f.name, re.I)]
                 if not f:
                     errmsg = f'{filename} not found in file system'
                     raise FileNotFoundError(errmsg)
-                self.filename = os.path.join(directory,f.pop())
+                self.filename = self.filename.with_name(f.pop())
         # print filename
         logging.debug(self.filename)
         return self
 
+    def compressuser(self, filename=None):
+        """
+        Tilde-compresses a file to be relative to the home directory
+
+        Parameters
+        ----------
+        filename: str or None, default None
+            output filename
+        """
+        if filename is None:
+            filename = self.filename
+        else:
+            filename = pathlib.Path(filename).expanduser().absolute()
+        # attempt to compress filename relative to home directory
+        try:
+            relative_to = filename.relative_to(pathlib.Path().home())
+        except (ValueError, AttributeError) as exc:
+            return filename
+        else:
+            return pathlib.Path('~').joinpath(relative_to)
+
     def from_ascii(self, filename, date=True, **kwargs):
         """
         Read a ``spatial`` object from an ascii file
 
         Parameters
         ----------
         filename: str
@@ -192,88 +202,113 @@
             ascii file has date information
         compression: str or NoneType, default None
             file compression type
 
                 - ``'gzip'``
                 - ``'zip'``
                 - ``'bytes'``
+        spacing: list, default [None,None]
+            grid step size ``[longitude,latitude]``
+        extent: list, default [None,None,None,None]
+            spatial grid bounds
+            ``[minimum longitude, maximum longitude,
+            minimum latitude, maximum latitude]``
+        nlat: int or NoneType, default None
+            length of latitude dimension
+        nlon: int or NoneType, default None
+            length of longitude dimension
         columns: list, default ['lon','lat','data','time']
             variable names for each column
         header: int, default 0
             Number of rows of header lines to skip
         verbose: bool, default False
             print file and variable information
         """
         # set filename
         self.case_insensitive_filename(filename)
         # set default parameters
         kwargs.setdefault('verbose',False)
         kwargs.setdefault('compression',None)
+        kwargs.setdefault('spacing',[None,None])
+        kwargs.setdefault('nlat',None)
+        kwargs.setdefault('nlon',None)
+        kwargs.setdefault('extent',[None]*4)
         kwargs.setdefault('columns',['lon','lat','data','time'])
         kwargs.setdefault('header',0)
         # open the ascii file and extract contents
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         if (kwargs['compression'] == 'gzip'):
             # read input ascii data from gzip compressed file and split lines
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 file_contents = f.read().decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'zip'):
             # read input ascii data from zipped file and split lines
-            base,_ = os.path.splitext(self.filename)
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
-                file_contents = z.read(base).decode('ISO-8859-1').splitlines()
+                file_contents = z.read(stem).decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'bytes'):
             # read input file object and split lines
             file_contents = self.filename.read().splitlines()
         else:
             # read input ascii file (.txt, .asc) and split lines
-            with open(self.filename, mode='r', encoding='utf8') as f:
+            with self.filename.open(mode='r', encoding='utf8') as f:
                 file_contents = f.read().splitlines()
         # compile regular expression operator for extracting numerical values
         # from input ascii files of spatial data
         regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[EeD][+-]?\d+)?'
         rx = re.compile(regex_pattern, re.VERBOSE)
         # output spatial dimensions
-        if (None not in self.extent):
-            self.lat = np.linspace(self.extent[3],self.extent[2],self.shape[0])
-            self.lon = np.linspace(self.extent[0],self.extent[1],self.shape[1])
+        if (None not in kwargs['extent']) and kwargs['nlat'] and kwargs['nlon']:
+            extent = kwargs.get('extent')
+            self.lat = np.linspace(extent[3], extent[2], kwargs['nlat'])
+            self.lon = np.linspace(extent[0], extent[1], kwargs['nlon'])
+            dlon = np.abs(self.lon[1] - self.lon[0])
+            dlat = np.abs(self.lat[1] - self.lat[0])
+        elif (None not in kwargs['extent']) and (None not in kwargs['spacing']):
+            extent = kwargs.get('extent')
+            dlon, dlat = kwargs.get('spacing')
+            self.lat = np.arange(extent[3], extent[2] - dlat, dlat)
+            self.lon = np.arange(extent[0], extent[1] + dlon, dlon)
+        elif kwargs['nlat'] and kwargs['nlon'] and (None not in kwargs['spacing']):
+            dlon, dlat = kwargs.get('spacing')
+            self.lat = np.zeros((kwargs['nlat']))
+            self.lon = np.zeros((kwargs['nlon']))
         else:
-            self.lat = np.zeros((self.shape[0]))
-            self.lon = np.zeros((self.shape[1]))
+            raise ValueError('Unknown dimensions for input ``spatial`` object')
+        # get spatial dimensions
+        nlat = len(self.lat)
+        nlon = len(self.lon)
         # output spatial data
-        self.data = np.zeros((self.shape[0],self.shape[1]))
-        self.mask = np.zeros((self.shape[0],self.shape[1]),dtype=bool)
+        self.data = np.zeros((nlat, nlon))
+        self.mask = np.zeros((nlat, nlon), dtype=bool)
         # remove time from list of column names if not date
         columns = [c for c in kwargs['columns'] if (c != 'time')]
         # extract spatial data array and convert to matrix
         # for each line in the file
         header = kwargs['header']
         for line in file_contents[header:]:
             # extract columns of interest and assign to dict
             # convert fortran exponentials if applicable
             d = {c:r.replace('D','E') for c,r in zip(columns,rx.findall(line))}
             # convert line coordinates to integers
-            ilon = np.int64(np.float64(d['lon'])/self.spacing[0])
-            ilat = np.int64((90.0-np.float64(d['lat']))//self.spacing[1])
-            self.data[ilat,ilon] = np.float64(d['data'])
-            self.mask[ilat,ilon] = False
+            ilon = np.int64(np.float64(d['lon'])/dlon)
+            ilat = np.int64((90.0 - np.float64(d['lat']))//dlat)
+            self.data[ilat, ilon] = np.float64(d['data'])
+            self.mask[ilat, ilon] = False
             self.lon[ilon] = np.float64(d['lon'])
             self.lat[ilat] = np.float64(d['lat'])
             # if the ascii file contains date variables
             if date:
                 self.time = np.array(d['time'],dtype='f')
                 self.month = calendar_to_grace(self.time)
         # if the ascii file contains date variables
         if date:
             # adjust months to fix special cases if necessary
             self.month = adjust_months(self.month)
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         return self
 
     def from_netCDF4(self, filename, **kwargs):
         """
         Read a ``spatial`` object from a netCDF4 file
 
@@ -316,20 +351,20 @@
         # Open the NetCDF4 file for reading
         if (kwargs['compression'] == 'gzip'):
             # read as in-memory (diskless) netCDF4 dataset
             with gzip.open(self.filename, mode='r') as f:
                 fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=f.read())
         elif (kwargs['compression'] == 'zip'):
             # read zipped file and extract file into in-memory file object
-            fileBasename,_ = os.path.splitext(os.path.basename(filename))
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
                 # first try finding a netCDF4 file with same base filename
                 # if none found simply try searching for a netCDF4 file
                 try:
-                    f,=[f for f in z.namelist() if re.match(fileBasename,f,re.I)]
+                    f,=[f for f in z.namelist() if re.match(stem,f,re.I)]
                 except:
                     f,=[f for f in z.namelist() if re.search(r'\.nc(4)?$',f)]
                 # read bytes from zipfile as in-memory (diskless) netCDF4 dataset
                 fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=z.read(f))
         elif (kwargs['compression'] == 'bytes'):
             # read as in-memory (diskless) netCDF4 dataset
             fileID = netCDF4.Dataset(uuid.uuid4().hex, memory=filename.read())
@@ -342,19 +377,18 @@
         # set automasking
         fileID.set_auto_mask(False)
         # list of variable attributes
         attributes_list = ['description','units','long_name','calendar',
             'standard_name','_FillValue','missing_value']
         # mapping between output keys and netCDF4 variable names
         if not kwargs['field_mapping']:
-            kwargs['field_mapping']['lon'] = kwargs['lonname']
-            kwargs['field_mapping']['lat'] = kwargs['latname']
-            kwargs['field_mapping']['data'] = kwargs['varname']
+            fields = [kwargs['lonname'],kwargs['latname'],kwargs['varname']]
             if kwargs['date']:
-                kwargs['field_mapping']['time'] = kwargs['timename']
+                fields.append(kwargs['timename'])
+            kwargs['field_mapping'] = self.default_field_mapping(fields)
         # for each variable
         for field,key in kwargs['field_mapping'].items():
             # Getting the data from each NetCDF variable
             # remove singleton dimensions
             setattr(self, field, np.squeeze(fileID.variables[key][:]))
             # Getting attributes of included variables
             self.attributes[field] = {}
@@ -382,18 +416,15 @@
         else:
             self.mask = np.zeros(self.data.shape, dtype=bool)
         # set GRACE/GRACE-FO month if file has date variables
         if kwargs['date']:
             self.month = calendar_to_grace(self.time)
             # adjust months to fix special cases if necessary
             self.month = adjust_months(self.month)
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         return self
 
     def from_HDF5(self, filename, **kwargs):
         """
         Read a ``spatial`` object from a HDF5 file
 
@@ -435,33 +466,33 @@
         kwargs.setdefault('verbose',False)
         # Open the HDF5 file for reading
         if (kwargs['compression'] == 'gzip'):
             # read gzip compressed file and extract into in-memory file object
             with gzip.open(self.filename, mode='r') as f:
                 fid = io.BytesIO(f.read())
             # set filename of BytesIO object
-            fid.filename = os.path.basename(filename)
+            fid.filename = self.filename.name
             # rewind to start of file
             fid.seek(0)
             # read as in-memory (diskless) HDF5 dataset from BytesIO object
             fileID = h5py.File(fid, 'r')
         elif (kwargs['compression'] == 'zip'):
             # read zipped file and extract file into in-memory file object
-            fileBasename,_ = os.path.splitext(os.path.basename(filename))
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
                 # first try finding a HDF5 file with same base filename
                 # if none found simply try searching for a HDF5 file
                 try:
-                    f,=[f for f in z.namelist() if re.match(fileBasename,f,re.I)]
+                    f,=[f for f in z.namelist() if re.match(stem,f,re.I)]
                 except:
                     f,=[f for f in z.namelist() if re.search(r'\.H(DF)?5$',f,re.I)]
                 # read bytes from zipfile into in-memory BytesIO object
                 fid = io.BytesIO(z.read(f))
             # set filename of BytesIO object
-            fid.filename = os.path.basename(filename)
+            fid.filename = self.filename.name
             # rewind to start of file
             fid.seek(0)
             # read as in-memory (diskless) HDF5 dataset from BytesIO object
             fileID = h5py.File(fid, mode='r')
         elif (kwargs['compression'] == 'bytes'):
             # read as in-memory (diskless) HDF5 dataset
             fileID = h5py.File(filename, mode='r')
@@ -472,19 +503,18 @@
         logging.info(fileID.filename)
         logging.info(list(fileID.keys()))
         # list of variable attributes
         attributes_list = ['description','units','long_name','calendar',
             'standard_name','_FillValue','missing_value']
         # mapping between output keys and HDF5 variable names
         if not kwargs['field_mapping']:
-            kwargs['field_mapping']['lon'] = kwargs['lonname']
-            kwargs['field_mapping']['lat'] = kwargs['latname']
-            kwargs['field_mapping']['data'] = kwargs['varname']
+            fields = [kwargs['lonname'],kwargs['latname'],kwargs['varname']]
             if kwargs['date']:
-                kwargs['field_mapping']['time'] = kwargs['timename']
+                fields.append(kwargs['timename'])
+            kwargs['field_mapping'] = self.default_field_mapping(fields)
         # for each variable
         for field,key in kwargs['field_mapping'].items():
             # Getting the data from each HDF5 variable
             # remove singleton dimensions
             setattr(self, field, np.squeeze(fileID[key][:]))
             # Getting attributes of included variables
             self.attributes[field] = {}
@@ -510,18 +540,15 @@
         else:
             self.mask = np.zeros(self.data.shape, dtype=bool)
         # set GRACE/GRACE-FO month if file has date variables
         if kwargs['date']:
             self.month = calendar_to_grace(self.time)
             # adjust months to fix special cases if necessary
             self.month = adjust_months(self.month)
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         return self
 
     def from_index(self, filename, **kwargs):
         """
         Read a ``spatial`` object from an index of
         ascii, netCDF4 or HDF5 files
@@ -550,15 +577,15 @@
         # set filename
         self.case_insensitive_filename(filename)
         # file parser for reading index files
         # removes commented lines (can comment out files in the index)
         # removes empty lines (if there are extra empty lines)
         parser = re.compile(r'^(?!\#|\%|$)', re.VERBOSE)
         # Read index file of input spatial data
-        with open(self.filename, mode='r', encoding='utf8') as f:
+        with self.filename.open(mode='r', encoding='utf8') as f:
             file_list = [l for l in f.read().splitlines() if parser.match(l)]
         # create a list of spatial objects
         s = []
         # for each file in the index
         for i,f in enumerate(file_list):
             if (kwargs['format'] == 'ascii'):
                 # netcdf (.nc)
@@ -595,49 +622,51 @@
         # number of spatial objects in list
         n = len(object_list)
         # indices to sort data objects if spatial list contain dates
         if kwargs['date'] and kwargs['sort']:
             list_sort = np.argsort([d.time for d in object_list],axis=None)
         else:
             list_sort = np.arange(n)
-        # extract dimensions and grid spacing
-        self.spacing = object_list[0].spacing
-        self.extent = object_list[0].extent
-        self.shape = object_list[0].shape
+        # extract grid spacing
+        shape = object_list[0].shape
         # create output spatial grid and mask
-        self.data = np.zeros((self.shape[0],self.shape[1],n))
-        self.mask = np.zeros((self.shape[0],self.shape[1],n),dtype=bool)
+        self.data = np.zeros((shape[0], shape[1], n))
+        self.mask = np.zeros((shape[0], shape[1], n),dtype=bool)
+        # add error if in original list attributes
+        if hasattr(object_list[0], 'error'):
+            self.error = np.zeros((shape[0], shape[1], n))
         self.fill_value = object_list[0].fill_value
         self.lon = object_list[0].lon.copy()
         self.lat = object_list[0].lat.copy()
         # create list of files and attributes
         self.filename = []
         self.attributes = []
         # output dates
         if kwargs['date']:
             self.time = np.zeros((n))
             self.month = np.zeros((n),dtype=np.int64)
         # for each indice
         for t,i in enumerate(list_sort):
             self.data[:,:,t] = object_list[i].data[:,:].copy()
             self.mask[:,:,t] |= object_list[i].mask[:,:]
+            if hasattr(object_list[i], 'error'):
+                self.error[:,:,t] = object_list[i].error[:,:].copy()
             if kwargs['date']:
                 self.time[t] = np.atleast_1d(object_list[i].time)
                 self.month[t] = np.atleast_1d(object_list[i].month)
             # append filename to list
             if getattr(object_list[i], 'filename'):
                 self.filename.append(object_list[i].filename)
             # append attributes to list
             if getattr(object_list[i], 'attributes'):
                 self.attributes.append(object_list[i].attributes)
         # adjust months to fix special cases if necessary
         if kwargs['date']:
             self.month = adjust_months(self.month)
-        # update the dimensions
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         # clear the input list to free memory
         if kwargs['clear']:
             object_list = None
         # return the single spatial object
         return self
 
@@ -692,18 +721,15 @@
                 setattr(self, key, d[key].copy())
             except (AttributeError, KeyError):
                 pass
         # create output mask for data
         self.mask = np.zeros_like(self.data, dtype=bool)
         # add attributes to root if in dictionary
         self.attributes['ROOT'] = d.get('attributes')
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         return self
 
     def to_ascii(self, filename, **kwargs):
         """
         Write a ``spatial`` object to ascii file
 
@@ -712,29 +738,37 @@
         filename: str
             full path of output ascii file
         date: bool, default True
             ``spatial`` objects contain date information
         verbose: bool, default False
             Output file and variable information
         """
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         # set default verbosity and parameters
         kwargs.setdefault('date',True)
         kwargs.setdefault('verbose',False)
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         # open the output file
-        fid = open(self.filename, mode='w', encoding='utf8')
-        if kwargs['date']:
-            file_format = '{0:10.4f} {1:10.4f} {2:12.4f} {3:10.4f}'
+        fid = self.filename.open(mode='w', encoding='utf8')
+        if hasattr(self, 'error') and kwargs['date']:
+            file_format = '{0:10.4f} {1:10.4f} {2:12.4f} {3:12.4f} {4:10.4f}'
+        elif hasattr(self, 'error'):
+            file_format = '{0:10.4f} {1:10.4f} {2:12.4f} {3:12.4f}'
+        elif kwargs['date']:
+            file_format = '{0:10.4f} {1:10.4f} {2:12.4f} {4:10.4f}'
         else:
             file_format = '{0:10.4f} {1:10.4f} {2:12.4f}'
         # write to file for each valid latitude and longitude
         ii,jj = np.nonzero((self.data != self.fill_value) & (~self.mask))
-        for ln,lt,dt in zip(self.lon[jj],self.lat[ii],self.data[ii,jj]):
-            print(file_format.format(ln,lt,dt,self.time), file=fid)
+        for i,j in zip(ii,jj):
+            ln = self.lon[j]
+            lt = self.lat[i]
+            data = self.data[i,j]
+            error = self.error[i,j] if hasattr(self, 'error') else 0.0
+            print(file_format.format(ln,lt,data,error,self.time), file=fid)
         # close the output file
         fid.close()
 
     def to_netCDF4(self, filename, **kwargs):
         """
         Write a ``spatial`` object to netCDF4 file
 
@@ -791,23 +825,22 @@
         kwargs.setdefault('reference',None)
         kwargs.setdefault('date',True)
         kwargs.setdefault('clobber',True)
         kwargs.setdefault('verbose',False)
         # setting NetCDF clobber attribute
         clobber = 'w' if kwargs['clobber'] else 'a'
         # opening NetCDF file for writing
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         fileID = netCDF4.Dataset(self.filename, clobber, format="NETCDF4")
         # mapping between output keys and netCDF4 variable names
         if not kwargs['field_mapping']:
-            kwargs['field_mapping']['lon'] = kwargs['lonname']
-            kwargs['field_mapping']['lat'] = kwargs['latname']
-            kwargs['field_mapping']['data'] = kwargs['varname']
+            fields = [kwargs['lonname'],kwargs['latname'],kwargs['varname']]
             if kwargs['date']:
-                kwargs['field_mapping']['time'] = kwargs['timename']
+                fields.append(kwargs['timename'])
+            kwargs['field_mapping'] = self.default_field_mapping(fields)
         # create attributes dictionary for output variables
         if not all(key in kwargs['attributes'] for key in kwargs['field_mapping'].values()):
             # Defining attributes for longitude and latitude
             kwargs['attributes'][kwargs['field_mapping']['lon']] = {}
             kwargs['attributes'][kwargs['field_mapping']['lon']]['long_name'] = 'longitude'
             kwargs['attributes'][kwargs['field_mapping']['lon']]['units'] = 'degrees_east'
             kwargs['attributes'][kwargs['field_mapping']['lat']] = {}
@@ -869,15 +902,15 @@
             fileID.setncattr(att_name, att_val)
         # add software information
         fileID.software_reference = gravity_toolkit.version.project_name
         fileID.software_version = gravity_toolkit.version.full_version
         # date created
         fileID.date_created = time.strftime('%Y-%m-%d',time.localtime())
         # Output NetCDF structure information
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         logging.info(list(fileID.variables.keys()))
         # Closing the NetCDF file
         fileID.close()
 
     def to_HDF5(self, filename, **kwargs):
         """
         Write a ``spatial`` object to HDF5 file
@@ -935,23 +968,22 @@
         kwargs.setdefault('reference',None)
         kwargs.setdefault('date',True)
         kwargs.setdefault('clobber',True)
         kwargs.setdefault('verbose',False)
         # setting NetCDF clobber attribute
         clobber = 'w' if kwargs['clobber'] else 'w-'
         # opening NetCDF file for writing
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         fileID = h5py.File(self.filename, clobber)
-        # mapping between output keys and netCDF4 variable names
+        # mapping between output keys and HDF5 variable names
         if not kwargs['field_mapping']:
-            kwargs['field_mapping']['lon'] = kwargs['lonname']
-            kwargs['field_mapping']['lat'] = kwargs['latname']
-            kwargs['field_mapping']['data'] = kwargs['varname']
+            fields = [kwargs['lonname'],kwargs['latname'],kwargs['varname']]
             if kwargs['date']:
-                kwargs['field_mapping']['time'] = kwargs['timename']
+                fields.append(kwargs['timename'])
+            kwargs['field_mapping'] = self.default_field_mapping(fields)
         # create attributes dictionary for output variables
         if not all(key in kwargs['attributes'] for key in kwargs['field_mapping'].values()):
             # Defining attributes for longitude and latitude
             kwargs['attributes'][kwargs['field_mapping']['lon']] = {}
             kwargs['attributes'][kwargs['field_mapping']['lon']]['long_name'] = 'longitude'
             kwargs['attributes'][kwargs['field_mapping']['lon']]['units'] = 'degrees_east'
             kwargs['attributes'][kwargs['field_mapping']['lat']] = {}
@@ -1012,15 +1044,15 @@
             fileID.attrs[att_name] = att_val
         # add software information
         fileID.attrs['software_reference'] = gravity_toolkit.version.project_name
         fileID.attrs['software_version'] = gravity_toolkit.version.full_version
         # date created
         fileID.attrs['date_created'] = time.strftime('%Y-%m-%d',time.localtime())
         # Output HDF5 structure information
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         logging.info(list(fileID.keys()))
         # Closing the NetCDF file
         fileID.close()
 
     def to_index(self, filename, file_list, format=None, date=True, **kwargs):
         """
         Write a ``spatial`` object to index of ascii, netCDF4 or HDF5 files
@@ -1041,22 +1073,22 @@
             ``spatial`` object contains date information
         verbose: bool, default False
             print file and variable information
         kwargs: dict
             keyword arguments for output writers
         """
         # Write index file of output spatial files
-        self.filename = os.path.expanduser(filename)
-        fid = open(self.filename, mode='w', encoding='utf8')
+        self.filename = pathlib.Path(filename).expanduser().absolute()
+        fid = self.filename.open(mode='w', encoding='utf8')
         # set default verbosity
         kwargs.setdefault('verbose',False)
         # for each file to be in the index
         for i,f in enumerate(file_list):
             # print filename to index
-            print(f.replace(os.path.expanduser('~'),'~'), file=fid)
+            print(self.compressuser(f), file=fid)
             # index spatial object at i
             s = self.index(i, date=date)
             # write to file
             if (format == 'ascii'):
                 # ascii (.txt)
                 s.to_ascii(f, date=date, **kwargs)
             elif (format == 'netCDF4'):
@@ -1098,56 +1130,61 @@
         elif (format == 'netCDF4'):
             # netcdf (.nc)
             self.to_netCDF4(filename, date=date, **kwargs)
         elif (format == 'HDF5'):
             # HDF5 (.H5)
             self.to_HDF5(filename, date=date, **kwargs)
 
-    def to_masked_array(self):
-        """
-        Convert a ``spatial`` object to a masked numpy array
+    def default_field_mapping(self, variables):
         """
-        return np.ma.array(self.data, mask=self.mask,
-            fill_value=self.fill_value)
+        Builds field mappings from a variable list
 
-    def update_spacing(self):
-        """
-        Calculate the step size of ``spatial`` object
-        """
-        # calculate degree spacing
-        dlat = np.abs(self.lat[1] - self.lat[0])
-        dlon = np.abs(self.lon[1] - self.lon[0])
-        self.spacing = (dlon,dlat)
-        return self
 
-    def update_extents(self):
-        """
-        Calculate the bounds of ``spatial`` object
+        Parameters
+        ----------
+        variables: list
+            netCDF4/HDF5 variables names to be mapped
+
+                - ``lonname``
+                - ``latname``
+                - ``varname``
+                - ``timename``
+
+        Returns
+        -------
+        field_mapping: dict
+            Field mappings for netCDF4/HDF5 read and write functions
         """
-        self.extent[0] = np.min(self.lon)
-        self.extent[1] = np.max(self.lon)
-        self.extent[2] = np.min(self.lat)
-        self.extent[3] = np.max(self.lat)
+        # get each variable name and add to field mapping dictionary
+        field_mapping = {}
+        for i, var in enumerate(['lon', 'lat', 'data', 'time']):
+            try:
+                field_mapping[var] = copy.copy(variables[i])
+            except IndexError as exc:
+                pass
+        # return the field mapping
+        return field_mapping
 
-    def update_dimensions(self):
+    def to_masked_array(self):
         """
-        Update the dimensions of the ``spatial`` object
+        Convert a ``spatial`` object to a masked numpy array
         """
-        self.shape = np.shape(self.data)
-        self.ndim = np.ndim(self.data)
-        return self
+        return np.ma.array(self.data, mask=self.mask,
+            fill_value=self.fill_value)
 
     def update_mask(self):
         """
         Update the mask of the ``spatial`` object
         """
         if self.fill_value is not None:
             self.mask |= (self.data == self.fill_value)
             self.mask |= np.isnan(self.data)
             self.data[self.mask] = self.fill_value
+            if hasattr(self, 'error'):
+                self.error[self.mask] = self.fill_value
         return self
 
     def copy(self):
         """
         Copy a ``spatial`` object to a new ``spatial`` object
         """
         temp = spatial(fill_value=self.fill_value)
@@ -1160,18 +1197,15 @@
         var = ['lon','lat','data','mask','error','time','month','filename']
         for key in var:
             try:
                 val = getattr(self, key)
                 setattr(temp, key, copy.copy(val))
             except AttributeError:
                 pass
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
+        # update mask
         temp.replace_masked()
         return temp
 
     def zeros_like(self):
         """
         Create a ``spatial`` object using the dimensions of another
         """
@@ -1182,18 +1216,15 @@
         var = ['data','mask','error','time','month']
         for key in var:
             try:
                 val = getattr(self, key)
                 setattr(temp, key, np.zeros_like(val))
             except AttributeError:
                 pass
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
+        # update mask
         temp.replace_masked()
         return temp
 
     def expand_dims(self):
         """
         Add a singleton dimension to a ``spatial`` object if non-existent
         """
@@ -1202,40 +1233,83 @@
         self.month = np.atleast_1d(self.month)
         # output spatial with a third dimension
         if (np.ndim(self.data) == 2):
             self.data = self.data[:,:,None]
             # try expanding mask variable
             try:
                 self.mask = self.mask[:,:,None]
-            except Exception as e:
+            except Exception as exc:
                 pass
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+            # try expanding spatial error
+            try:
+                self.error = self.error[:,:,None]
+            except AttributeError as exc:
+                pass
+        # update mask
         self.update_mask()
         return self
 
+    # PURPOSE: Extend a global matrix
+    def extend_matrix(self):
+        """
+        Extends a global matrix to wrap along longitudes
+
+        Returns
+        -------
+        temp: float
+            extended matrix
+        """
+        temp = self.copy()
+        # shape of the original data object
+        ny, nx, *nt = self.shape
+        # extended longitude array [x-1,x0,...,xN,xN+1]
+        temp.lon = np.zeros((nx+2), dtype=self.lon.dtype)
+        temp.lon[0] = self.lon[0] - self.spacing[0]
+        temp.lon[1:-1] = self.lon[:]
+        temp.lon[-1] = self.lon[-1] + self.spacing[1]
+        # attempt to extend possible data variables
+        for key in ['data','mask','error']:
+            try:
+                # get the original data variable
+                var = getattr(self, key)
+                # extended data matrices along longitude axis
+                if (self.ndim == 2):
+                    tmp = np.zeros((ny, nx+2), dtype=var.dtype)
+                    tmp[:,0] = var[:,-1]
+                    tmp[:,1:-1] = var[:,:]
+                    tmp[:,-1] = var[:,0]
+                elif (self.ndim == 3):
+                    var = getattr(self, key)
+                    tmp = np.zeros((ny, nx+2, nt[0]), dtype=var.dtype)
+                    tmp[:,0,:] = var[:,-1,:]
+                    tmp[:,1:-1,:] = var[:,:,:]
+                    tmp[:,-1,:] = var[:,0,:]
+                # set the output extended data variable
+                setattr(temp, key, tmp)
+            except Exception as exc:
+                pass
+        # update mask
+        temp.update_mask()
+        # return the extended spatial object
+        return temp
+
     def squeeze(self):
         """
         Remove singleton dimensions from a ``spatial`` object
         """
         # squeeze singleton dimensions
         self.time = np.squeeze(self.time)
         self.month = np.squeeze(self.month)
-        self.data = np.squeeze(self.data)
-        # try squeezing mask variable
-        try:
-            self.mask = np.squeeze(self.mask)
-        except Exception as e:
-            pass
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # attempt to squeeze possible data variables
+        for key in ['data','mask','error']:
+            try:
+                setattr(self, key, np.squeeze(getattr(self, key)))
+            except Exception as exc:
+                pass
+        # update mask
         self.update_mask()
         return self
 
     def index(self, indice, date=True):
         """
         Subset a ``spatial`` object to specific index
 
@@ -1244,40 +1318,36 @@
         indice: int
             index in matrix for subsetting
         date: bool, default True
             ``spatial`` objects contain date information
         """
         # output spatial object
         temp = spatial(fill_value=self.fill_value)
-        # subset output spatial field
-        temp.data = self.data[:,:,indice].copy()
-        temp.mask = self.mask[:,:,indice].copy()
-        # subset output spatial error
-        try:
-            temp.error = self.error[:,:,indice].copy()
-        except AttributeError:
-            pass
+        # attempt to subset possible data variables
+        for key in ['data','mask','error']:
+            try:
+                tmp = getattr(self, key)
+                setattr(temp, key, tmp[:,:,indice].copy())
+            except Exception as exc:
+                pass
         # copy dimensions
         temp.lon = self.lon.copy()
         temp.lat = self.lat.copy()
         # subset output dates
         if date:
             temp.time = self.time[indice].copy()
             temp.month = self.month[indice].copy()
         # subset filenames if applicable
         if getattr(self, 'filename'):
             if isinstance(self.filename, (list, tuple, np.ndarray)):
                 temp.filename = str(self.filename[indice])
             elif isinstance(self.filename, str):
                 temp.filename = copy.copy(self.filename)
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
-        return temp
+        # remove singleton dimensions if importing a single value
+        return temp.squeeze()
 
     def subset(self, months):
         """
         Subset a ``spatial`` object to specific GRACE/GRACE-FO months
 
         Parameters
         ----------
@@ -1292,19 +1362,19 @@
         months_check = list(set(months) - set(self.month))
         if months_check:
             m = ','.join([f'{m:03d}' for m in months_check])
             raise IOError(f'GRACE/GRACE-FO months {m} not Found')
         # indices to sort data objects
         months_list = [i for i,m in enumerate(self.month) if m in months]
         # output spatial object
-        temp = spatial(nlon=self.shape[0],nlat=self.shape[1],
+        temp = spatial(nlat=self.shape[0], nlon=self.shape[1],
             fill_value=self.fill_value)
         # create output spatial object
         temp.data = np.zeros((temp.shape[0],temp.shape[1],n))
-        temp.mask = np.zeros((temp.shape[0],temp.shape[1],n))
+        temp.mask = np.zeros((temp.shape[0],temp.shape[1],n), dtype=bool)
         # create output spatial error
         try:
             getattr(self, 'error')
             temp.error = np.zeros((temp.shape[0],temp.shape[1],n))
         except AttributeError:
             pass
         # copy dimensions
@@ -1360,18 +1430,14 @@
             temp.data = self.data + var
         elif (np.ndim(var) == 2) and (self.ndim == 3):
             for i,t in enumerate(self.time):
                 temp.data[:,:,i] = self.data[:,:,i] + var
         elif (np.ndim(var) == 3) and (self.ndim == 3):
             for i,t in enumerate(self.time):
                 temp.data[:,:,i] = self.data[:,:,i] + var[:,:,i]
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
     def scale(self, var):
         """
         Multiply a ``spatial`` object by a constant
@@ -1399,18 +1465,14 @@
             temp.data = var*self.data
         elif (np.ndim(var) == 2) and (self.ndim == 3):
             for i,t in enumerate(self.time):
                 temp.data[:,:,i] = var*self.data[:,:,i]
         elif (np.ndim(var) == 3) and (self.ndim == 3):
             for i,t in enumerate(self.time):
                 temp.data[:,:,i] = var[:,:,i]*self.data[:,:,i]
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
     def mean(self, apply=False, indices=Ellipsis):
         """
         Compute mean spatial field and remove from data if specified
@@ -1437,67 +1499,63 @@
             temp.time = np.mean(val[indices])
         except (AttributeError,TypeError):
             pass
         # calculate the spatial anomalies by removing the mean field
         if apply:
             for i,t in enumerate(self.time):
                 self.data[:,:,i] -= temp.data[:,:]
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
-    def reverse(self, axis=0):
+    def flip(self, axis=0):
         """
         Reverse the order of data and dimensions along an axis
 
         Parameters
         ----------
         axis: int, default 0
             axis to reorder
         """
         # output spatial object
         temp = self.copy()
-        temp.expand_dims()
         # copy dimensions and reverse order
         if (axis == 0):
             temp.lat = temp.lat[::-1].copy()
-            temp.data = temp.data[::-1,:,:].copy()
-            temp.mask = temp.mask[::-1,:,:].copy()
         elif (axis == 1):
             temp.lon = temp.lon[::-1].copy()
-            temp.data = temp.data[:,::-1,:].copy()
-            temp.mask = temp.mask[:,::-1,:].copy()
-        # squeeze output spatial object
-        # get spacing and dimensions
+        elif (axis == 2):
+            temp.time = temp.time[::-1].copy()
+        # attempt to reverse possible data variables
+        for key in ['data','mask','error']:
+            try:
+                setattr(temp, key, np.flip(getattr(self, key), axis=axis))
+            except Exception as exc:
+                pass
         # update mask
-        temp.squeeze()
+        temp.update_mask()
         return temp
 
     def transpose(self, axes=None):
         """
-        Reverse or permute the axes of a ``spatial`` object
+        Transpose or permute the axes of a ``spatial`` object
 
         Parameters
         ----------
         axis: int or NoneType, default None
             order of the output axes
         """
         # output spatial object
         temp = self.copy()
-        # copy dimensions and reverse order
-        temp.data = np.transpose(temp.data, axes=axes)
-        temp.mask = np.transpose(temp.mask, axes=axes)
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
+        # attempt to transpose possible data variables
+        for key in ['data','mask','error']:
+            try:
+                setattr(temp, key, np.transpose(getattr(self, key), axes=axes))
+            except Exception as exc:
+                pass
         # update mask
         temp.update_mask()
         return temp
 
     def sum(self, power=1):
         """
         Compute summation of a ``spatial`` object
@@ -1512,18 +1570,14 @@
             fill_value=self.fill_value)
         # copy dimensions
         temp.lon = self.lon.copy()
         temp.lat = self.lat.copy()
         # create output summation spatial object
         temp.data = np.sum(np.power(self.data,power),axis=2)
         temp.mask = np.any(self.mask,axis=2)
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
     def power(self, power):
         """
         Raise a ``spatial`` object to a power
@@ -1531,16 +1585,14 @@
         Parameters
         ----------
         power: int
             power to which the ``spatial`` object will be raised
         """
         temp = self.copy()
         temp.data = np.power(self.data,power)
-        # assign ndim and shape attributes
-        temp.update_dimensions()
         return temp
 
     def max(self):
         """
         Compute maximum value of a ``spatial`` object
         """
         # output spatial object
@@ -1548,18 +1600,14 @@
             fill_value=self.fill_value)
         # copy dimensions
         temp.lon = self.lon.copy()
         temp.lat = self.lat.copy()
         # create output maximum spatial object
         temp.data = np.max(self.data,axis=2)
         temp.mask = np.any(self.mask,axis=2)
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
     def min(self):
         """
         Compute minimum value of a ``spatial`` object
@@ -1569,18 +1617,14 @@
             fill_value=self.fill_value)
         # copy dimensions
         temp.lon = self.lon.copy()
         temp.lat = self.lat.copy()
         # create output minimum spatial object
         temp.data = np.min(self.data,axis=2)
         temp.mask = np.any(self.mask,axis=2)
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         return temp
 
     def replace_invalid(self, fill_value, mask=None):
         """
         Replace the masked values with a new ``fill_value``
@@ -1602,24 +1646,65 @@
                 # broadcast mask over third dimension
                 temp = np.repeat(mask[:,:,np.newaxis],self.shape[2],axis=2)
                 self.mask |= temp
         # update the fill value
         self.fill_value = fill_value
         # replace invalid values with new fill value
         self.data[self.mask] = self.fill_value
+        if hasattr(self, 'error'):
+            self.error[self.mask] = self.fill_value
         return self
 
     def replace_masked(self):
         """
         Replace the masked values with ``fill_value``
         """
-        if self.fill_value is not None:
+        if (self.fill_value is not None):
             self.data[self.mask] = self.fill_value
+        if (self.fill_value is not None) and hasattr(self, 'error'):
+            self.error[self.mask] = self.fill_value
         return self
 
+    @property
+    def dtype(self):
+        """Main data type of ``spatial`` object"""
+        return self.data.dtype
+
+    @property
+    def spacing(self):
+        """Step size of ``spatial`` object ``[longitude,latitude]``
+        """
+        dlat = np.abs(self.lat[1] - self.lat[0])
+        dlon = np.abs(self.lon[1] - self.lon[0])
+        return (dlon,dlat)
+
+    @property
+    def extent(self):
+        """Bounds of ``spatial`` object
+        ``[minimum longitude, maximum longitude,
+        minimum latitude, maximum latitude]``
+        """
+        lonmin = np.min(self.lon)
+        lonmax = np.max(self.lon)
+        latmin = np.min(self.lat)
+        latmax = np.max(self.lat)
+        return [lonmin, lonmax, latmin, latmax]
+
+    @property
+    def shape(self):
+        """Dimensions of ``spatial`` object
+        """
+        return np.shape(self.data)
+
+    @property
+    def ndim(self):
+        """Number of dimensions in ``spatial`` object
+        """
+        return np.ndim(self.data)
+
     def __len__(self):
         """Number of months
         """
         return len(self.month)
 
     def __iter__(self):
         """Iterate over GRACE/GRACE-FO months
@@ -1639,29 +1724,25 @@
             temp.time = self.time[self.__index__].copy()
             temp.month = self.month[self.__index__].copy()
         except IndexError as exc:
             raise StopIteration from exc
         # subset output spatial error
         try:
             temp.error = self.error[:,:,self.__index__].copy()
-        except AttributeError:
+        except AttributeError as exc:
             pass
         # subset filename
         if getattr(self, 'filename'):
             if isinstance(self.filename, (list, tuple, np.ndarray)):
                 temp.filename = str(self.filename[self.__index__])
             elif isinstance(self.filename, str):
                 temp.filename = copy.copy(self.filename)
         # copy dimensions
         temp.lon = self.lon.copy()
         temp.lat = self.lat.copy()
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # add to index
         self.__index__ += 1
         return temp
 
 # PURPOSE: additional routines for the spatial module
 # for outputting scaling factor data
 class scaling_factors(spatial):
@@ -1697,170 +1778,196 @@
         grid step size ``[x, y]``
     shape: tuple
         dimensions of spatial object
     ndim: int
         number of dimensions of spatial object
     filename: str
         input or output filename
-
     """
     np.seterr(invalid='ignore')
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
         self.error = None
         self.magnitude = None
 
-    def from_ascii(self, filename, date=True, **kwargs):
+    def from_ascii(self, filename, **kwargs):
         """
         Read a ``scaling_factors`` object from an ascii file
 
         Parameters
         ----------
         filename: str
             full path of input ascii file
-        date: bool, default True
-            ascii file has date information
         compression: str or NoneType, default None
             file compression type
 
                 - ``'gzip'``
                 - ``'zip'``
                 - ``'bytes'``
+        spacing: list, default [None,None]
+            grid step size ``[longitude,latitude]``
+        extent: list, default [None,None,None,None]
+            spatial grid bounds
+            ``[minimum longitude, maximum longitude,
+            minimum latitude, maximum latitude]``
+        nlat: int or NoneType, default None
+            length of latitude dimension
+        nlon: int or NoneType, default None
+            length of longitude dimension
         columns: list, default ['lon','lat','kfactor','error','magnitude']
             variable names for each column
         header: int, default 0
             Number of rows of header lines to skip
         verbose: bool, default False
             print file and variable information
         """
         # set filename
         self.case_insensitive_filename(filename)
         # set default parameters
         kwargs.setdefault('verbose',False)
         kwargs.setdefault('compression',None)
+        kwargs.setdefault('spacing',[None,None])
+        kwargs.setdefault('nlat',None)
+        kwargs.setdefault('nlon',None)
+        kwargs.setdefault('extent',[None]*4)
         default_columns = ['lon','lat','kfactor','error','magnitude']
         kwargs.setdefault('columns',default_columns)
         kwargs.setdefault('header',0)
         # open the ascii file and extract contents
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         if (kwargs['compression'] == 'gzip'):
             # read input ascii data from gzip compressed file and split lines
-            with gzip.open(self.filename,'r') as f:
+            with gzip.open(self.filename, mode='r') as f:
                 file_contents = f.read().decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'zip'):
             # read input ascii data from zipped file and split lines
-            base,_ = os.path.splitext(self.filename)
+            stem = self.filename.stem
             with zipfile.ZipFile(self.filename) as z:
-                file_contents = z.read(base).decode('ISO-8859-1').splitlines()
+                file_contents = z.read(stem).decode('ISO-8859-1').splitlines()
         elif (kwargs['compression'] == 'bytes'):
             # read input file object and split lines
             file_contents = self.filename.read().splitlines()
         else:
             # read input ascii file (.txt, .asc) and split lines
-            with open(self.filename, mode='r', encoding='utf8') as f:
+            with self.filename.open(mode='r', encoding='utf8') as f:
                 file_contents = f.read().splitlines()
         # compile regular expression operator for extracting numerical values
         # from input ascii files of spatial data
         regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[EeD][+-]?\d+)?'
         rx = re.compile(regex_pattern, re.VERBOSE)
         # output spatial dimensions
-        if (None not in self.extent):
-            self.lat = np.linspace(self.extent[3],self.extent[2],self.shape[0])
-            self.lon = np.linspace(self.extent[0],self.extent[1],self.shape[1])
+        if (None not in kwargs['extent']) and kwargs['nlat'] and kwargs['nlon']:
+            extent = kwargs.get('extent')
+            self.lat = np.linspace(extent[3], extent[2], kwargs['nlat'])
+            self.lon = np.linspace(extent[0], extent[1], kwargs['nlon'])
+            dlon = np.abs(self.lon[1] - self.lon[0])
+            dlat = np.abs(self.lat[1] - self.lat[0])
+        elif (None not in kwargs['extent']) and (None not in kwargs['spacing']):
+            extent = kwargs.get('extent')
+            dlon, dlat = kwargs.get('spacing')
+            self.lat = np.arange(extent[3], extent[2] - dlat, dlat)
+            self.lon = np.arange(extent[0], extent[1] + dlon, dlon)
+        elif kwargs['nlat'] and kwargs['nlon'] and (None not in kwargs['spacing']):
+            dlon, dlat = kwargs.get('spacing')
+            self.lat = np.zeros((kwargs['nlat']))
+            self.lon = np.zeros((kwargs['nlon']))
         else:
-            self.lat = np.zeros((self.shape[0]))
-            self.lon = np.zeros((self.shape[1]))
+            raise ValueError('Unknown dimensions for input ``spatial`` object')
+        # get spatial dimensions
+        nlat = len(self.lat)
+        nlon = len(self.lon)
         # output spatial data
-        self.data = np.zeros((self.shape[0],self.shape[1]))
-        self.mask = np.zeros((self.shape[0],self.shape[1]),dtype=bool)
+        self.data = np.zeros((nlat, nlon))
+        self.mask = np.zeros((nlat, nlon), dtype=bool)
         # remove time from list of column names if not date
         columns = [c for c in kwargs['columns'] if (c != 'time')]
         # extract spatial data array and convert to matrix
         # for each line in the file
         header = kwargs['header']
         for line in file_contents[header:]:
             # extract columns of interest and assign to dict
             # convert fortran exponentials if applicable
             d = {c:r.replace('D','E') for c,r in zip(columns,rx.findall(line))}
             # convert line coordinates to integers
-            ilon = np.int64(np.float64(d['lon'])/self.spacing[0])
-            ilat = np.int64((90.0-np.float64(d['lat']))//self.spacing[1])
+            ilon = np.int64(np.float64(d['lon'])/dlon)
+            ilat = np.int64((90.0-np.float64(d['lat']))//dlat)
             # get scaling factor, error and magnitude
             self.data[ilat,ilon] = np.float64(d['data'])
             self.error[ilat,ilon] = np.float64(d['error'])
             self.magnitude[ilat,ilon] = np.float64(d['magnitude'])
             # set mask
             self.mask[ilat,ilon] = False
             # set latitude and longitude
             self.lon[ilon] = np.float64(d['lon'])
             self.lat[ilat] = np.float64(d['lat'])
-        # get spacing and dimensions
-        self.update_spacing()
-        self.update_extents()
-        self.update_dimensions()
+        # update mask
         self.update_mask()
         return self
 
     def to_ascii(self, filename, **kwargs):
         """
         Write a ``scaling_factors`` object to ascii file
 
         Parameters
         ----------
         filename: str
             full path of output ascii file
         verbose: bool, default False
             Output file and variable information
         """
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         # set default verbosity and parameters
         kwargs.setdefault('verbose',False)
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         # open the output file
-        fid = open(self.filename, mode='w', encoding='utf8')
+        fid = self.filename.open(mode='w', encoding='utf8')
         # write to file for each valid latitude and longitude
         ii,jj = np.nonzero((self.data != self.fill_value) & (~self.mask))
         for i,j in zip(ii,jj):
             print((f'{self.lon[j]:10.4f} {self.lat[i]:10.4f} '
                   f'{self.data[i,j]:12.4f} {self.error[i,j]:12.4f} '
                   f'{self.magnitude[i,j]:12.4f}'), file=fid)
         # close the output file
         fid.close()
 
     def kfactor(self, var):
         """
         Calculate the scaling factor and scaling factor errors
         from two ``spatial`` or ``scaling_factors`` objects
-        following [Landerer2012]_
+        following [Landerer2012]_ and [Hsu2017]_
 
         Parameters
         ----------
-        var: float
+        var: obj
             ``spatial`` object to used for scaling
 
         Returns
         -------
         temp: obj
             scaling factor, scaling error and magnitude
 
         References
         ----------
         .. [Landerer2012] F. W. Landerer and S. C. Swenson,
             "Accuracy of scaled GRACE terrestrial water storage estimates",
             *Water Resources Research*, 48(W04531), (2012).
             `doi: 10.1029/2011WR011453 <https://doi.org/10.1029/2011WR011453>`_
+        .. [Hsu2017] C.-W. Hsu and I. Velicogna, "Detection of Sea Level
+            Fingerprints derived from GRACE gravity data",
+            *Geophysical Research Letters*, 44, 8953--8961, (2017).
+            `doi: 10.1002/2017GL074070 <https://doi.org/10.1002/2017GL074070>`_
         """
         # copy to not modify original inputs
         temp1 = self.copy()
         temp2 = var.copy()
         # expand dimensions and replace invalid values with 0
         temp1.expand_dims().replace_invalid(0.0)
         temp2.expand_dims().replace_invalid(0.0)
         # dimensions of input spatial object
-        nlat,nlon,nt = temp1.shape
+        nlat, nlon, nt = temp1.shape
         # allocate for scaling factor and scaling factor error
         temp = scaling_factors(nlat=nlat, nlon=nlon, fill_value=0.0)
         temp.data = np.zeros((nlat, nlon))
         temp.error = np.zeros((nlat, nlon))
         # copy latitude and longitude variables
         temp.lon = np.copy(temp1.lon)
         temp.lat = np.copy(temp1.lat)
@@ -1873,18 +1980,14 @@
         temp.data[indy,indx] = val1/val2
         # calculate difference between scaled and original
         variance = temp1.scale(temp.data).offset(-temp2.data)
         # calculate scaling factor errors as RMS of variance
         temp.error = np.sqrt((variance.sum(power=2).data)/nt)
         # calculate magnitude of original data
         temp.magnitude = temp2.sum(power=2.0).power(0.5).data[:]
-        # get spacing and dimensions
-        temp.update_spacing()
-        temp.update_extents()
-        temp.update_dimensions()
         # update mask
         temp.update_mask()
         # return the scaling factors and scaling factor errors
         return temp
 
     def update_mask(self):
         """
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/time_series/amplitude.py` & `gravity-toolkit-1.2.1/gravity_toolkit/time_series/amplitude.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/time_series/piecewise.py` & `gravity-toolkit-1.2.1/gravity_toolkit/time_series/piecewise.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 piecewise.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (04/2023)
 
 Fits a synthetic signal to data over a time period by ordinary or weighted
     least-squares for breakpoint analysis
 
 Derivation of Sharp Breakpoint Piecewise Regression:
     https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/02-0472
         y = beta_0 + beta_1*t + e (for x <= alpha)
@@ -47,23 +47,25 @@
     BREAK_TIME: breakpoint time for piecewise regression
     BREAKPOINT: breakpoint indice of piecewise regression
     DATA_ERR: data precision
         single value if equal
         array if unequal for weighted least squares
     WEIGHT: Set if measurement errors for use in weighted least squares
     CYCLES: list of cyclical terms (0.5=semi-annual, 1=annual)
+    TERMS: list of extra terms
     STDEV: standard deviation of output error
     CONF: confidence interval of output error
     AICc: use second order AIC
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
     scipy: Scientific Tools for Python (https://docs.scipy.org/doc/)
 
 UPDATE HISTORY:
+    Updated 04/2023: option to include extra fit terms in the design matrix
     Updated 01/2023: refactored time series analysis functions
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 01/2021: added function docstrings
     Updated 10/2019: changing Y/N flags to True/False
     Updated 01/2019: added option S2 to include 161-day tidal aliasing terms
     Updated 12/2018: put transpose of design matrix within FIT_TYPE if statement
@@ -91,16 +93,16 @@
     Written 10/2011
 """
 import numpy as np
 import scipy.stats
 import scipy.special
 
 def piecewise(t_in, d_in, BREAK_TIME=None, BREAKPOINT=None,
-    CYCLES=[0.5,1.0], DATA_ERR=0, WEIGHT=False, STDEV=0, CONF=0,
-    AICc=False):
+    CYCLES=[0.5,1.0], TERMS=[], DATA_ERR=0, WEIGHT=False,
+    STDEV=0, CONF=0, AICc=False):
     """
     Fits a synthetic signal to data over a time period by ordinary or
     weighted least-squares for breakpoint analysis [Toms2003]_
 
     Parameters
     ----------
     t_in: float
@@ -109,14 +111,16 @@
         input data array
     BREAK_TIME: float or NoneType, default None
         breakpoint time for piecewise regression
     BREAKPOINT: int or NoneType, default None
         breakpoint indice of piecewise regression
     CYCLES: list, default [0.5, 1.0]
         list of cyclical terms in fractions of year
+    TERMS: list, default []
+        list of extra fit terms
     DATA_ERR: float or list
         data precision
 
             - single value if equal
             - array if unequal for weighted least squares
     WEIGHT: bool, default False
         Use weighted least squares with measurement errors
@@ -201,14 +205,17 @@
     P_x1 = np.zeros((nmax))
     P_x1[nco:] = t_in[nco:] - tco
     DMAT.append(P_x1)
     # add cyclical terms (0.5=semi-annual, 1=annual)
     for c in CYCLES:
         DMAT.append(np.sin(2.0*np.pi*t_in/np.float64(c)))
         DMAT.append(np.cos(2.0*np.pi*t_in/np.float64(c)))
+    # add additional terms to the design matrix
+    for t in TERMS:
+        DMAT.append(t)
     # take the transpose of the design matrix
     DMAT = np.transpose(DMAT)
 
     # Calculating Least-Squares Coefficients
     if WEIGHT:
         # Weighted Least-Squares fitting
         if (np.ndim(DATA_ERR) == 0):
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/time_series/regress.py` & `gravity-toolkit-1.2.1/gravity_toolkit/time_series/regress.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 regress.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (04/2023)
 
 Fits a synthetic signal to data over a time period by ordinary or weighted
     least-squares
 
 Fit significance derivations are based on Burnham and Anderson (2002)
     Model Selection and Multimodel Inference
 
@@ -42,23 +42,25 @@
     DATA_ERR: data precision
         single value if equal
         array if unequal for weighted least squares
     WEIGHT: Set if measurement errors for use in weighted least squares
     RELATIVE: relative period
     ORDER: maximum polynomial order in fit (0=constant, 1=linear, 2=quadratic)
     CYCLES: list of cyclical terms (0.5=semi-annual, 1=annual)
+    TERMS: list of extra terms
     STDEV: standard deviation of output error
     CONF: confidence interval of output error
     AICc: use second order AIC
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
     scipy: Scientific Tools for Python (https://docs.scipy.org/doc/)
 
 UPDATE HISTORY:
+    Updated 04/2023: option to include extra fit terms in the design matrix
     Updated 01/2023: refactored time series analysis functions
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 07/2020: added function docstrings
     Updated 10/2019: changing Y/N flags to True/False
     Updated 12/2018: put transpose of design matrix within FIT_TYPE if statement
     Updated 08/2018: import packages before function definition
@@ -98,16 +100,17 @@
     Updated 01/2012: added std weighting for a error weighted least-squares
     Written 10/2011
 """
 import numpy as np
 import scipy.stats
 import scipy.special
 
-def regress(t_in, d_in, ORDER=1, CYCLES=[0.5,1.0], DATA_ERR=0,
-    WEIGHT=False, RELATIVE=Ellipsis, STDEV=0, CONF=0, AICc=True):
+def regress(t_in, d_in, ORDER=1, CYCLES=[0.5,1.0], TERMS=[],
+    DATA_ERR=0, WEIGHT=False, RELATIVE=Ellipsis, STDEV=0, CONF=0,
+    AICc=True):
     """
     Fits a synthetic signal to data over a time period by
     ordinary or weighted least-squares
 
     Parameters
     ----------
     t_in: float
@@ -118,14 +121,16 @@
         maximum polynomial order in fit
 
             * ``0``: constant
             * ``1``: linear
             * ``2``: quadratic
     CYCLES: list, default [0.5, 1.0]
         list of cyclical terms
+    TERMS: list, default []
+        list of extra fit terms
     DATA_ERR: float or list
         Data precision
 
             - single value if equal
             - array if unequal for weighted least squares
     WEIGHT: bool, default False
         Use weighted least squares with measurement errors
@@ -197,22 +202,26 @@
         t_rel = t_in[RELATIVE].mean()
     elif isinstance(RELATIVE, (float, int, np.float_, np.int_)):
         t_rel = np.copy(RELATIVE)
     elif (RELATIVE == Ellipsis):
         t_rel = t_in[RELATIVE].mean()
 
     # create design matrix based on polynomial order and harmonics
+    # with any additional fit terms
     DMAT = []
     # add polynomial orders (0=constant, 1=linear, 2=quadratic)
     for o in range(ORDER+1):
         DMAT.append((t_in-t_rel)**o)
     # add cyclical terms (0.5=semi-annual, 1=annual)
     for c in CYCLES:
         DMAT.append(np.sin(2.0*np.pi*t_in/np.float64(c)))
         DMAT.append(np.cos(2.0*np.pi*t_in/np.float64(c)))
+    # add additional terms to the design matrix
+    for t in TERMS:
+        DMAT.append(t)
     # take the transpose of the design matrix
     DMAT = np.transpose(DMAT)
 
     # Calculating Least-Squares Coefficients
     if WEIGHT:
         # Weighted Least-Squares fitting
         if (np.ndim(DATA_ERR) == 0):
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/time_series/savitzky_golay.py` & `gravity-toolkit-1.2.1/gravity_toolkit/time_series/savitzky_golay.py`

 * *Files 4% similar despite different names*

```diff
@@ -52,15 +52,15 @@
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 07/2020: added function docstrings
     Updated 08/2019: importing factorial from scipy.special.factorial
     Updated 11/2018: using future division for python3 compatibility
     Updated 08/2015: changed sys.exit to raise ValueError
     Written 06/2014
 """
-from __future__ import print_function, division
+from __future__ import division
 
 import numpy as np
 import scipy.special
 
 def savitzky_golay(t_in, y_in, WINDOW=None, ORDER=2, DERIV=0,
     RATE=1, DATA_ERR=0):
     """
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/time_series/smooth.py` & `gravity-toolkit-1.2.1/gravity_toolkit/time_series/smooth.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/tools.py` & `gravity-toolkit-1.2.1/gravity_toolkit/tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 tools.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Jupyter notebook, user interface and plotting tools
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
     scipy: Scientific Tools for Python
@@ -23,25 +23,29 @@
 PROGRAM DEPENDENCIES:
     grace_find_months.py: finds available months for a GRACE/GRACE-FO dataset
     grace_date.py: reads GRACE index file and calculates dates for each month
     spatial.py: spatial data class for reading, writing and processing data
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: add wrap longitudes function to change convention
+        improve typing for variables in docstrings
     Updated 06/2022: place matplotlib imports within try/except
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 06/2022: place ipython and tkinter imports within try/except
     Updated 05/2022: adjusted mask oceans function to be able to output mask
     Updated 04/2022: updated docstrings to numpy documentation format
     Updated 12/2021: added custom colormap function for some common scales
     Written 09/2021
 """
 import os
 import re
 import copy
+import pathlib
 import colorsys
 import warnings
 import numpy as np
 import scipy.interpolate
 from gravity_toolkit.spatial import spatial
 from gravity_toolkit.utilities import get_data_path
 from gravity_toolkit.grace_find_months import grace_find_months
@@ -74,15 +78,15 @@
 
 # widgets for Jupyter notebooks
 class widgets:
     def __init__(self, **kwargs):
         """Widgets and functions for running GRACE/GRACE-FO analyses
         """
         # set default keyword arguments
-        kwargs.setdefault('directory', os.getcwd())
+        kwargs.setdefault('directory', pathlib.Path.cwd())
         kwargs.setdefault('defaults', ['CSR','RL06','GSM',60])
         kwargs.setdefault('style', {})
         # set style
         self.style = copy.copy(kwargs['style'])
         # run directory
         self.select_directory(**kwargs)
 
@@ -99,15 +103,15 @@
             `Tkinter file dialog <https://docs.python.org/3/library/dialog.html>`_
         update: obj
             Checkbox widget for updating GRACE/GRACE-FO data in directory
         """
 
         # set the directory with GRACE/GRACE-FO data
         self.directory_label = ipywidgets.Text(
-            value=kwargs['directory'],
+            value=str(kwargs['directory']),
             description='Directory:',
             disabled=False,
             style=self.style,
         )
         # button and label for directory selection
         self.directory_button = ipywidgets.Button(
             description="Directory select",
@@ -692,15 +696,15 @@
             multiple=True)
         self.remove_files.extend(b.files)
         self.set_removelabel()
 
     def set_removefile(self, sender):
         """function for updating removed file list
         """
-        if self.remove_file.value:
+        if self.remove_label.value:
             self.remove_files = self.remove_label.value.split(',')
         else:
             self.remove_files = []
 
     def set_removelabel(self):
         """function for updating removed file label
         """
@@ -740,27 +744,27 @@
             style=self.style,
         )
 
     @property
     def base_directory(self):
         """Returns the data directory
         """
-        return os.path.expanduser(self.directory_label.value)
+        return pathlib.Path(self.directory_label.value).expanduser().absolute()
 
     @property
     def GIA_model(self):
         """Returns the GIA model file
         """
-        return os.path.expanduser(self.GIA_label.value)
+        return pathlib.Path(self.GIA_label.value).expanduser().absolute()
 
     @property
     def landmask(self):
         """Returns the land-sea mask file
         """
-        return os.path.expanduser(self.mask_label.value)
+        return pathlib.Path(self.mask_label.value).expanduser().absolute()
 
     @property
     def unit_index(self):
         """Returns the index for output spatial units
         """
         return self.units.index + 1
 
@@ -917,18 +921,19 @@
     use_extremes: bool, default True
         use the under, over and bad values from the cpt file
     **kwargs: dict
         optional arguments for LinearSegmentedColormap
     """
 
     # read the cpt file and get contents
-    with open(filename, mode='r', encoding='utf8') as f:
+    filename = pathlib.Path(filename).expanduser().absolute()
+    with filename.open(mode='r', encoding='utf8') as f:
         file_contents = f.read().splitlines()
     # extract basename from cpt filename
-    name = re.sub(r'\.cpt','',os.path.basename(filename),flags=re.I)
+    name = re.sub(r'\.cpt', '', filename.name, flags=re.I)
 
     # compile regular expression operator to find numerical instances
     rx = re.compile(r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?')
 
     # create list objects for x, r, g, b
     x,r,g,b = ([],[],[],[])
     # assume RGB color model
@@ -1090,38 +1095,52 @@
     # create colormap for use in matplotlib
     cmap = colors.LinearSegmentedColormap(map_name, cdict, **kwargs)
     # register colormap to be recognizable by cm.get_cmap()
     cm.register_cmap(name=map_name, cmap=cmap)
     # return the colormap
     return cmap
 
+# PURPOSE: adjusts longitudes to be -180:180
+def wrap_longitudes(lon):
+    """
+    Wraps longitudes to range from -180 to +180
+
+    Parameters
+    ----------
+    lon: np.ndarray
+        longitude (degrees east)
+    """
+    phi = np.arctan2(np.sin(lon*np.pi/180.0), np.cos(lon*np.pi/180.0))
+    # convert phi from radians to degrees
+    return phi*180.0/np.pi
+
 # PURPOSE: parallels the matplotlib basemap shiftgrid function
 def shift_grid(lon0, data, lon, CYCLIC=360.0):
     """
     Shift global grid east or west to a new base longitude
 
     Parallels the ``mpl_toolkits.basemap.shiftgrid`` function
 
     Parameters
     ----------
-    lon0: float
+    lon0: np.ndarray
         Starting longitude for shifted grid
         lon0 (_type_): _description_
-    data: float
+    data: np.ndarray
         data grid to be shifted
-    lon: float
+    lon: np.ndarray
         longitude array to be shifted
     CYCLIC: float, default 360.0
         width of periodic domain
 
     Returns
     -------
-    shift_data: float
+    shift_data: np.ndarray
         shifted data grid
-    shift_lon: float
+    shift_lon: np.ndarray
         shifted longitude array
     """
     start_idx = 0 if (np.fabs(lon[-1]-lon[0]-CYCLIC) > 1.e-4) else 1
     i0 = np.argmin(np.fabs(lon-lon0))
     # shift longitudinal values
     if np.ma.isMA(lon):
         shift_lon = np.ma.zeros(lon.shape,lon.dtype)
@@ -1144,33 +1163,33 @@
     """
     Interpolate gridded data to a new grid
 
     Parallels the ``mpl_toolkits.basemap.interp`` function
 
     Parameters
     ----------
-    datain: float
+    datain: np.ndarray
         input data grid to be interpolated
-    xin: float
+    xin: np.ndarray
         input x-coordinate array (monotonically increasing)
-    yin: float
+    yin: np.ndarray
         input y-coordinate array (monotonically increasing)
-    xout: float
+    xout: np.ndarray
         output x-coordinate array
-    yout: float
+    yout: np.ndarray
         output y-coordinate array
     order: int, default 0
         interpolation order
 
             - ``0``: nearest-neighbor interpolation
             - ``k``: bivariate spline interpolation of degree k
 
     Returns
     -------
-    interp_data: float
+    interp_data: np.ndarray
         interpolated data grid
     """
     if (order == 0):
         # interpolate with nearest-neighbors
         xcoords = (len(xin)-1)*(xout-xin[0])/(xin[-1]-xin[0])
         ycoords = (len(yin)-1)*(yout-yin[0])/(yin[-1]-yin[0])
         xcoords = np.clip(xcoords,0,len(xin)-1)
@@ -1193,19 +1212,19 @@
     """
     Mask a data grid over global ocean and water points
 
     Parallels the ``mpl_toolkits.basemap.maskoceans`` function
 
     Parameters
     ----------
-    xin: float
+    xin: np.ndarray
         input x-coordinate array (monotonically increasing)
-    yin: float
+    yin: np.ndarray
         input y-coordinate array (monotonically increasing)
-    data: float or NoneType
+    data: np.ndarray or NoneType
         input data grid to be masked
     order: int, default 0
         interpolation order
 
             - ``0``: nearest-neighbor interpolation
             - ``k``: bivariate spline interpolation of degree k
     lakes: bool, default False
@@ -1217,17 +1236,17 @@
 
             - ``'1d'``: 1-degree spacing
             - ``'hd'``: 0.5-degree spacing
             - ``'qd'``: 0.25-degree spacing
 
     Returns
     -------
-    mask: bool
+    mask: np.ndarray
         mask grid
-    datain: float
+    datain: np.ndarray
         masked data grid
     """
     # read in land/sea mask
     lsmask = get_data_path(['data',f'landsea_{resolution}.nc'])
     # Land-Sea Mask with Antarctica from Rignot (2017) and Greenland from GEUS
     # 0=Ocean, 1=Land, 2=Lake, 3=Small Island, 4=Ice Shelf
     # Open the land-sea NetCDF file for reading
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit/utilities.py` & `gravity-toolkit-1.2.1/gravity_toolkit/utilities.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,23 @@
 #!/usr/bin/env python
 u"""
 utilities.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (06/2023)
 Download and management utilities for syncing time and auxiliary files
 
 PYTHON DEPENDENCIES:
     lxml: processing XML and HTML in Python
         https://pypi.python.org/pypi/lxml
 
 UPDATE HISTORY:
+    Updated 06/2023: add functions to retrieve and revoke Earthdata tokens
+        add TN11e.txt file to list of CSR SLR downloads
+    Updated 05/2023: add reify decorator for evaluation of properties
+        use pathlib to define and operate on paths
+    Updated 04/2023: use release-03 GFZ GravIS SLR and geocenter files
     Updated 03/2023: place boto3 import within try/except statement
     Updated 01/2023: add default ssl context attribute with protocol
     Updated 12/2022: add variables for NASA DAAC and s3 providers
         add functions for managing and maintaining git repositories
     Updated 11/2022: add CMR queries for collection metadata
         exposed GSFC SLR url for weekly 5x5 harmonics as an option
     Updated 08/2022: add regular expression function for finding files
@@ -38,15 +43,15 @@
         added username and password to ftp functions. added ftp connection check
     Updated 09/2020: copy from http and https to bytesIO object in chunks
         use netrc credentials if not entered from PO.DAAC functions
         generalize build opener function for different Earthdata instances
     Updated 08/2020: add PO.DAAC Drive opener, login and download functions
     Written 08/2020
 """
-from __future__ import print_function, division
+from __future__ import print_function, division, annotations
 
 import sys
 import os
 import re
 import io
 import ssl
 import json
@@ -55,14 +60,15 @@
 import shutil
 import base64
 import socket
 import getpass
 import inspect
 import hashlib
 import logging
+import pathlib
 import builtins
 import dateutil
 import warnings
 import posixpath
 import lxml.etree
 import subprocess
 import calendar,time
@@ -81,104 +87,129 @@
 except (ImportError, ModuleNotFoundError) as exc:
     warnings.filterwarnings("module")
     warnings.warn("boto3 not available", ImportWarning)
 # ignore warnings
 warnings.filterwarnings("ignore")
 
 # PURPOSE: get absolute path within a package from a relative path
-def get_data_path(relpath):
+def get_data_path(relpath: list | str | pathlib.Path):
     """
     Get the absolute path within a package from a relative path
 
     Parameters
     ----------
-    relpath: str,
+    relpath: list, str or pathlib.Path
         relative path
     """
     # current file path
     filename = inspect.getframeinfo(inspect.currentframe()).filename
-    filepath = os.path.dirname(os.path.abspath(filename))
-    if isinstance(relpath,list):
+    filepath = pathlib.Path(filename).absolute().parent
+    if isinstance(relpath, list):
         # use *splat operator to extract from list
-        return os.path.join(filepath,*relpath)
-    elif isinstance(relpath,str):
-        return os.path.join(filepath,relpath)
+        return filepath.joinpath(*relpath)
+    elif isinstance(relpath, str):
+        return filepath.joinpath(relpath)
+
+class reify(object):
+    """Class decorator that puts the result of the method it
+    decorates into the instance"""
+    def __init__(self, wrapped):
+        self.wrapped = wrapped
+        self.__name__ = wrapped.__name__
+        self.__doc__ = wrapped.__doc__
+
+    def __get__(self, inst, objtype=None):
+        if inst is None:
+            return self
+        val = self.wrapped(inst)
+        setattr(inst, self.wrapped.__name__, val)
+        return val
 
 # PURPOSE: get the hash value of a file
-def get_hash(local, algorithm='MD5'):
+def get_hash(
+        local: str | io.IOBase | pathlib.Path,
+        algorithm: str = 'MD5'
+    ):
     """
     Get the hash value from a local file or ``BytesIO`` object
 
     Parameters
     ----------
-    local: obj or str
+    local: obj, str or pathlib.Path
         BytesIO object or path to file
     algorithm: str, default 'MD5'
         hashing algorithm for checksum validation
 
             - ``'MD5'``: Message Digest
             - ``'sha1'``: Secure Hash Algorithm
     """
     # check if open file object or if local file exists
     if isinstance(local, io.IOBase):
         if (algorithm == 'MD5'):
             return hashlib.md5(local.getvalue()).hexdigest()
         elif (algorithm == 'sha1'):
             return hashlib.sha1(local.getvalue()).hexdigest()
-    elif os.access(os.path.expanduser(local),os.F_OK):
+    elif isinstance(local, (str, pathlib.Path)):
         # generate checksum hash for local file
+        local = pathlib.Path(local).expanduser()
+        # if file currently doesn't exist, return empty string
+        if not local.exists():
+            return ''
         # open the local_file in binary read mode
-        with open(os.path.expanduser(local), 'rb') as local_buffer:
+        with local.open(mode='rb') as local_buffer:
             # generate checksum hash for a given type
             if (algorithm == 'MD5'):
                 return hashlib.md5(local_buffer.read()).hexdigest()
             elif (algorithm == 'sha1'):
                 return hashlib.sha1(local_buffer.read()).hexdigest()
     else:
         return ''
 
 # PURPOSE: get the git hash value
-def get_git_revision_hash(refname='HEAD', short=False):
+def get_git_revision_hash(
+        refname: str = 'HEAD',
+        short: bool = False
+    ):
     """
     Get the ``git`` hash value for a particular reference
 
     Parameters
     ----------
     refname: str, default HEAD
         Symbolic reference name
     short: bool, default False
         Return the shorted hash value
     """
     # get path to .git directory from current file path
     filename = inspect.getframeinfo(inspect.currentframe()).filename
-    basepath = os.path.dirname(os.path.dirname(os.path.abspath(filename)))
-    gitpath = os.path.join(basepath,'.git')
+    basepath = pathlib.Path(filename).absolute().parent.parent
+    gitpath = basepath.joinpath('.git')
     # build command
     cmd = ['git', f'--git-dir={gitpath}', 'rev-parse']
     cmd.append('--short') if short else None
     cmd.append(refname)
     # get output
     with warnings.catch_warnings():
         return str(subprocess.check_output(cmd), encoding='utf8').strip()
 
 # PURPOSE: get the current git status
 def get_git_status():
     """Get the status of a ``git`` repository as a boolean value
     """
     # get path to .git directory from current file path
     filename = inspect.getframeinfo(inspect.currentframe()).filename
-    basepath = os.path.dirname(os.path.dirname(os.path.abspath(filename)))
-    gitpath = os.path.join(basepath,'.git')
+    basepath = pathlib.Path(filename).absolute().parent.parent
+    gitpath = basepath.joinpath('.git')
     # build command
     cmd = ['git', f'--git-dir={gitpath}', 'status', '--porcelain']
     with warnings.catch_warnings():
         return bool(subprocess.check_output(cmd))
 
 # PURPOSE: recursively split a url path
-def url_split(s):
+def url_split(s: str):
     """
     Recursively split a url path into a list
 
     Parameters
     ----------
     s: str
         url string
@@ -203,15 +234,18 @@
     # remove commented lines and after argument comments
     for arg in re.sub(r'\#(.*?)$',r'',arg_line).split():
         if not arg.strip():
             continue
         yield arg
 
 # PURPOSE: returns the Unix timestamp value for a formatted date string
-def get_unix_time(time_string, format='%Y-%m-%d %H:%M:%S'):
+def get_unix_time(
+        time_string: str,
+        format: str = '%Y-%m-%d %H:%M:%S'
+    ):
     """
     Get the Unix timestamp value for a formatted date string
 
     Parameters
     ----------
     time_string: str
         formatted time string to parse
@@ -229,15 +263,15 @@
         parsed_time = dateutil.parser.parse(time_string.rstrip())
     except (TypeError, ValueError):
         return None
     else:
         return parsed_time.timestamp()
 
 # PURPOSE: output a time string in isoformat
-def isoformat(time_string):
+def isoformat(time_string: str):
     """
     Reformat a date string to ISO formatting
 
     Parameters
     ----------
     time_string: str
         formatted time string to parse
@@ -247,88 +281,98 @@
         parsed_time = dateutil.parser.parse(time_string.rstrip())
     except (TypeError, ValueError):
         return None
     else:
         return parsed_time.isoformat()
 
 # PURPOSE: rounds a number to an even number less than or equal to original
-def even(value):
+def even(value: float):
     """
     Rounds a number to an even number less than or equal to original
 
     Parameters
     ----------
     value: float
         number to be rounded
     """
     return 2*int(value//2)
 
 # PURPOSE: rounds a number upward to its nearest integer
-def ceil(value):
+def ceil(value: float):
     """
     Rounds a number upward to its nearest integer
 
     Parameters
     ----------
     value: float
         number to be rounded upward
     """
     return -int(-value//1)
 
 # PURPOSE: make a copy of a file with all system information
-def copy(source, destination, move=False, **kwargs):
+def copy(
+        source: str | pathlib.Path,
+        destination: str | pathlib.Path,
+        move: bool = False,
+        **kwargs
+    ):
     """
     Copy or move a file with all system information
 
     Parameters
     ----------
-    source: str
+    source: str or pathlib.Path
         source file
-    destination: str
+    destination: str or pathlib.Path
         copied destination file
     move: bool, default False
         remove the source file
     """
-    source = os.path.abspath(os.path.expanduser(source))
-    destination = os.path.abspath(os.path.expanduser(destination))
+    source = pathlib.Path(source).expanduser().absolute()
+    destination = pathlib.Path(destination).expanduser().absolute()
     # log source and destination
-    logging.info(f'{source} -->\n\t{destination}')
+    logging.info(f'{str(source)} -->\n\t{str(destination)}')
     shutil.copyfile(source, destination)
     shutil.copystat(source, destination)
+    # remove the original file if moving
     if move:
-        os.remove(source)
+        source.unlink()
 
 # PURPOSE: open a unique file adding a numerical instance if existing
-def create_unique_file(filename):
+def create_unique_file(filename: str | pathlib.Path):
     """
     Open a unique file adding a numerical instance if existing
 
     Parameters
     ----------
-    filename: str
+    filename: str or pathlib.Path
         full path to output file
     """
-    # split filename into fileBasename and fileExtension
-    fileBasename, fileExtension = os.path.splitext(filename)
+    filename = pathlib.Path(filename)
     # create counter to add to the end of the filename if existing
     counter = 1
     while counter:
         try:
             # open file descriptor only if the file doesn't exist
-            fd = os.open(filename, os.O_CREAT | os.O_EXCL | os.O_RDWR)
+            fd = filename.open(mode='xb')
         except OSError:
             pass
         else:
-            return os.fdopen(fd, 'w+')
-        # new filename adds counter the between fileBasename and fileExtension
-        filename = f'{fileBasename}_{counter:d}{fileExtension}'
+            # return the file descriptor
+            return fd
+        # new filename adds a counter before the file extension
+        filename = filename.with_name(f'{filename.stem}_{counter:d}{filename.suffix}')
         counter += 1
 
 # PURPOSE: check ftp connection
-def check_ftp_connection(HOST, username=None, password=None):
+def check_ftp_connection(
+        HOST: str,
+        username: str | None = None,
+        password: str | None = None
+    ):
     """
     Check internet connection with ftp host
 
     Parameters
     ----------
     HOST: str
         remote ftp host
@@ -346,16 +390,23 @@
         raise RuntimeError('Check internet connection')
     except ftplib.error_perm:
         raise RuntimeError('Check login credentials')
     else:
         return True
 
 # PURPOSE: list a directory on a ftp host
-def ftp_list(HOST, username=None, password=None, timeout=None,
-    basename=False, pattern=None, sort=False):
+def ftp_list(
+        HOST: str | list,
+        username: str | None = None,
+        password: str | None = None,
+        timeout: int | None = None,
+        basename: bool = False,
+        pattern: str | None = None,
+        sort: bool = False
+    ):
     """
     List a directory on a ftp host
 
     Parameters
     ----------
     HOST: str or list
         remote ftp host path split as list
@@ -381,15 +432,15 @@
     """
     # verify inputs for remote ftp host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try to connect to ftp host
     try:
         ftp = ftplib.FTP(HOST[0],timeout=timeout)
-    except (socket.gaierror,IOError) as exc:
+    except (socket.gaierror,IOError):
         raise RuntimeError(f'Unable to connect to {HOST[0]}')
     else:
         ftp.login(username,password)
         # list remote path
         output = ftp.nlst(posixpath.join(*HOST[1:]))
         # get last modified date of ftp files and convert into unix time
         mtimes = [None]*len(output)
@@ -418,34 +469,43 @@
             i = [i for i,j in sorted(enumerate(output), key=lambda i: i[1])]
             # sort list of listed items and last modified times
             output = [output[indice] for indice in i]
             mtimes = [mtimes[indice] for indice in i]
         # close the ftp connection
         ftp.close()
         # return the list of items and last modified times
-        return (output,mtimes)
+        return (output, mtimes)
 
 # PURPOSE: download a file from a ftp host
-def from_ftp(HOST, username=None, password=None, timeout=None,
-    local=None, hash='', chunk=8192, verbose=False, fid=sys.stdout,
-    mode=0o775):
+def from_ftp(
+        HOST: str | list,
+        username: str | None = None,
+        password: str | None = None,
+        timeout: int | None = None,
+        local: str | pathlib.Path | None = None,
+        hash: str = '',
+        chunk: int = 8192,
+        verbose: bool = False,
+        fid=sys.stdout,
+        mode: oct = 0o775
+    ):
     """
     Download a file from a ftp host
 
     Parameters
     ----------
     HOST: str or list
         remote ftp host path
     username: str or NoneType
         ftp username
     password: str or NoneType
         ftp password
     timeout: int or NoneType, default None
         timeout in seconds for blocking operations
-    local: str or NoneType, default None
+    local: str, pathlib.Path or NoneType, default None
         path to local file
     hash: str, default ''
         MD5 hash of local file
     chunk: int, default 8192
         chunk size for transfer encoding
     verbose: bool, default False
         print file transfer information
@@ -464,16 +524,16 @@
     logging.basicConfig(stream=fid, level=loglevel)
     # verify inputs for remote ftp host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try downloading from ftp
     try:
         # try to connect to ftp host
-        ftp = ftplib.FTP(HOST[0],timeout=timeout)
-    except (socket.gaierror,IOError) as exc:
+        ftp = ftplib.FTP(HOST[0], timeout=timeout)
+    except (socket.gaierror,IOError):
         raise RuntimeError(f'Unable to connect to {HOST[0]}')
     else:
         ftp.login(username,password)
         # remote path
         ftp_remote_path = posixpath.join(*HOST[1:])
         # copy remote file contents to bytesIO object
         remote_buffer = io.BytesIO()
@@ -486,40 +546,39 @@
         remote_hash = hashlib.md5(remote_buffer.getvalue()).hexdigest()
         # get last modified date of remote file and convert into unix time
         mdtm = ftp.sendcmd(f'MDTM {ftp_remote_path}')
         remote_mtime = get_unix_time(mdtm[4:], format="%Y%m%d%H%M%S")
         # compare checksums
         if local and (hash != remote_hash):
             # convert to absolute path
-            local = os.path.abspath(local)
+            local = pathlib.Path(local).expanduser().absolute()
             # create directory if non-existent
-            if not os.access(os.path.dirname(local), os.F_OK):
-                os.makedirs(os.path.dirname(local), mode)
+            local.parent.mkdir(mode=mode, parents=True, exist_ok=True)
             # print file information
-            args = (posixpath.join(*HOST),local)
+            args = (posixpath.join(*HOST), str(local))
             logging.info('{0} -->\n\t{1}'.format(*args))
             # store bytes to file using chunked transfer encoding
             remote_buffer.seek(0)
-            with open(os.path.expanduser(local), 'wb') as f:
+            with local.open(mode='wb') as f:
                 shutil.copyfileobj(remote_buffer, f, chunk)
             # change the permissions mode
-            os.chmod(local,mode)
+            local.chmod(mode)
             # keep remote modification time of file and local access time
-            os.utime(local, (os.stat(local).st_atime, remote_mtime))
+            os.utime(local, (local.stat().st_atime, remote_mtime))
         # close the ftp connection
         ftp.close()
         # return the bytesIO object
         remote_buffer.seek(0)
         return remote_buffer
 
 # default ssl context
 _default_ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS)
 
 # PURPOSE: check internet connection
-def check_connection(HOST, context=_default_ssl_context):
+def check_connection(HOST: str, context=_default_ssl_context):
     """
     Check internet connection with http host
 
     Parameters
     ----------
     HOST: str
         remote http host
@@ -531,17 +590,23 @@
         urllib2.urlopen(HOST, timeout=20, context=context)
     except urllib2.URLError as exc:
         raise RuntimeError('Check internet connection') from exc
     else:
         return True
 
 # PURPOSE: list a directory on an Apache http Server
-def http_list(HOST, timeout=None, context=_default_ssl_context,
-    parser=lxml.etree.HTMLParser(), format='%Y-%m-%d %H:%M',
-    pattern='', sort=False):
+def http_list(
+        HOST: str | list,
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        parser = lxml.etree.HTMLParser(),
+        format: str = '%Y-%m-%d %H:%M',
+        pattern: str = '',
+        sort: bool = False
+    ):
     """
     List a directory on an Apache http Server
 
     Parameters
     ----------
     HOST: str or list
         remote http host path
@@ -567,58 +632,64 @@
     """
     # verify inputs for remote http host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try listing from http
     try:
         # Create and submit request.
-        request=urllib2.Request(posixpath.join(*HOST))
-        response=urllib2.urlopen(request,timeout=timeout,context=context)
-    except (urllib2.HTTPError, urllib2.URLError) as exc:
+        request = urllib2.Request(posixpath.join(*HOST))
+        response = urllib2.urlopen(request, timeout=timeout, context=context)
+    except (urllib2.HTTPError, urllib2.URLError):
         raise Exception('List error from {0}'.format(posixpath.join(*HOST)))
     else:
         # read and parse request for files (column names and modified times)
-        tree = lxml.etree.parse(response,parser)
+        tree = lxml.etree.parse(response, parser)
         colnames = tree.xpath('//tr/td[not(@*)]//a/@href')
         # get the Unix timestamp value for a modification time
         collastmod = [get_unix_time(i,format=format)
             for i in tree.xpath('//tr/td[@align="right"][1]/text()')]
         # reduce using regular expression pattern
         if pattern:
-            i = [i for i,f in enumerate(colnames) if re.search(pattern,f)]
+            i = [i for i,f in enumerate(colnames) if re.search(pattern, f)]
             # reduce list of column names and last modified times
             colnames = [colnames[indice] for indice in i]
             collastmod = [collastmod[indice] for indice in i]
         # sort the list
         if sort:
             i = [i for i,j in sorted(enumerate(colnames), key=lambda i: i[1])]
             # sort list of column names and last modified times
             colnames = [colnames[indice] for indice in i]
             collastmod = [collastmod[indice] for indice in i]
         # return the list of column names and last modified times
-        return (colnames,collastmod)
+        return (colnames, collastmod)
 
 # PURPOSE: download a file from a http host
-def from_http(HOST, timeout=None, context=_default_ssl_context,
-    local=None, hash='', chunk=16384, verbose=False, fid=sys.stdout,
-    mode=0o775):
+def from_http(
+        HOST: str | list,
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        local: str | pathlib.Path | None = None,
+        hash: str = '',
+        chunk: int = 16384,
+        verbose: bool = False,
+        fid = sys.stdout,
+        mode: oct = 0o775
+    ):
     """
     Download a file from a http host
 
     Parameters
     ----------
     HOST: str or list
         remote http host path split as list
     timeout: int or NoneType, default None
         timeout in seconds for blocking operations
     context: obj, default ssl.SSLContext(ssl.PROTOCOL_TLS)
         SSL context for ``urllib`` opener object
-    timeout: int or NoneType, default None
-        timeout in seconds for blocking operations
-    local: str or NoneType, default None
+    local: str, pathlib.Path or NoneType, default None
         path to local file
     hash: str, default ''
         MD5 hash of local file
     chunk: int, default 16384
         chunk size for transfer encoding
     verbose: bool, default False
         print file transfer information
@@ -638,50 +709,55 @@
     # verify inputs for remote http host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try downloading from http
     try:
         # Create and submit request.
         request = urllib2.Request(posixpath.join(*HOST))
-        response = urllib2.urlopen(request,timeout=timeout,context=context)
-    except (urllib2.HTTPError, urllib2.URLError) as exc:
+        response = urllib2.urlopen(request, timeout=timeout, context=context)
+    except:
         raise Exception('Download error from {0}'.format(posixpath.join(*HOST)))
     else:
         # copy remote file contents to bytesIO object
         remote_buffer = io.BytesIO()
         shutil.copyfileobj(response, remote_buffer, chunk)
         remote_buffer.seek(0)
         # save file basename with bytesIO object
         remote_buffer.filename = HOST[-1]
         # generate checksum hash for remote file
         remote_hash = hashlib.md5(remote_buffer.getvalue()).hexdigest()
         # compare checksums
         if local and (hash != remote_hash):
             # convert to absolute path
-            local = os.path.abspath(local)
+            local = pathlib.Path(local).expanduser().absolute()
             # create directory if non-existent
-            if not os.access(os.path.dirname(local), os.F_OK):
-                os.makedirs(os.path.dirname(local), mode)
+            local.parent.mkdir(mode=mode, parents=True, exist_ok=True)
             # print file information
-            args = (posixpath.join(*HOST),local)
+            args = (posixpath.join(*HOST), str(local))
             logging.info('{0} -->\n\t{1}'.format(*args))
             # store bytes to file using chunked transfer encoding
             remote_buffer.seek(0)
-            with open(os.path.expanduser(local), 'wb') as f:
+            with local.open(mode='wb') as f:
                 shutil.copyfileobj(remote_buffer, f, chunk)
             # change the permissions mode
-            os.chmod(local,mode)
+            local.chmod(mode)
         # return the bytesIO object
         remote_buffer.seek(0)
         return remote_buffer
 
 # PURPOSE: attempt to build an opener with netrc
-def attempt_login(urs, context=_default_ssl_context,
-    password_manager=True, get_ca_certs=False, redirect=False,
-    authorization_header=True, **kwargs):
+def attempt_login(
+        urs: str,
+        context=_default_ssl_context,
+        password_manager: bool = True,
+        get_ca_certs: bool = False,
+        redirect: bool = False,
+        authorization_header: bool = True,
+        **kwargs
+    ):
     """
     Attempt to build a ``urllib`` opener for NASA Earthdata
 
     Parameters
     ----------
     urs: str
         Earthdata login URS 3 host
@@ -709,20 +785,22 @@
     opener: obj
         OpenerDirector instance
     """
     # set default keyword arguments
     kwargs.setdefault('username', os.environ.get('EARTHDATA_USERNAME'))
     kwargs.setdefault('password', os.environ.get('EARTHDATA_PASSWORD'))
     kwargs.setdefault('retries', 5)
-    kwargs.setdefault('netrc', os.path.expanduser('~/.netrc'))
+    kwargs.setdefault('netrc', pathlib.Path.home().joinpath('.netrc'))
     try:
+        # verify permissions level of netrc file
         # only necessary on jupyterhub
-        os.chmod(kwargs['netrc'], 0o600)
+        nc = pathlib.Path(kwargs['netrc']).expanduser().absolute()
+        nc.chmod(mode=0o600)
         # try retrieving credentials from netrc
-        username, _, password = netrc.netrc(kwargs['netrc']).authenticators(urs)
+        username, _, password = netrc.netrc(nc).authenticators(urs)
     except Exception as exc:
         # try retrieving credentials from environmental variables
         username, password = (kwargs['username'], kwargs['password'])
         pass
     # if username or password are not available
     if not username:
         username = builtins.input(f'Username for {urs}: ')
@@ -750,17 +828,24 @@
         # reattempt login
         username = builtins.input(f'Username for {urs}: ')
         password = getpass.getpass(prompt=prompt)
     # reached end of available retries
     raise RuntimeError('End of Retries: Check NASA Earthdata credentials')
 
 # PURPOSE: "login" to NASA Earthdata with supplied credentials
-def build_opener(username, password, context=_default_ssl_context,
-    password_manager=False, get_ca_certs=False, redirect=False,
-    authorization_header=True, urs='https://urs.earthdata.nasa.gov'):
+def build_opener(
+        username: str,
+        password: str,
+        context=_default_ssl_context,
+        password_manager: bool = False,
+        get_ca_certs: bool = False,
+        redirect: bool = False,
+        authorization_header: bool = True,
+        urs: str = 'https://urs.earthdata.nasa.gov'
+    ):
     """
     Build ``urllib`` opener for NASA Earthdata with supplied credentials
 
     Parameters
     ----------
     username: str or NoneType, default None
         NASA Earthdata username
@@ -786,15 +871,15 @@
     """
     # https://docs.python.org/3/howto/urllib2.html#id5
     handler = []
     # create a password manager
     if password_manager:
         password_mgr = urllib2.HTTPPasswordMgrWithDefaultRealm()
         # Add the username and password for NASA Earthdata Login system
-        password_mgr.add_password(None,urs,username,password)
+        password_mgr.add_password(None, urs, username, password)
         handler.append(urllib2.HTTPBasicAuthHandler(password_mgr))
     # Create cookie jar for storing cookies. This is used to store and return
     # the session cookie given to use by the data server (otherwise will just
     # keep sending us back to Earthdata Login to authenticate).
     cookie_jar = CookieJar()
     handler.append(urllib2.HTTPCookieProcessor(cookie_jar))
     # SSL context handler
@@ -806,22 +891,181 @@
         handler.append(urllib2.HTTPRedirectHandler())
     # create "opener" (OpenerDirector instance)
     opener = urllib2.build_opener(*handler)
     # Encode username/password for request authorization headers
     # add Authorization header to opener
     if authorization_header:
         b64 = base64.b64encode(f'{username}:{password}'.encode())
-        opener.addheaders = [("Authorization","Basic {0}".format(b64.decode()))]
+        opener.addheaders = [("Authorization", f"Basic {b64.decode()}")]
     # Now all calls to urllib2.urlopen use our opener.
     urllib2.install_opener(opener)
     # All calls to urllib2.urlopen will now use handler
     # Make sure not to include the protocol in with the URL, or
     # HTTPPasswordMgrWithDefaultRealm will be confused.
     return opener
 
+# PURPOSE: generate a NASA Earthdata user token
+def get_token(
+        HOST: str = 'https://urs.earthdata.nasa.gov/api/users/token',
+        username: str | None = None,
+        password: str | None = None,
+        build: bool = True,
+        urs: str = 'urs.earthdata.nasa.gov',
+    ):
+    """
+    Generate a NASA Earthdata User Token
+
+    Parameters
+    ----------
+    HOST: str or list
+        NASA Earthdata token API host
+    username: str or NoneType, default None
+        NASA Earthdata username
+    password: str or NoneType, default None
+        NASA Earthdata password
+    build: bool, default True
+        Build opener and check WebDAV credentials
+    timeout: int or NoneType, default None
+        timeout in seconds for blocking operations
+    urs: str, default 'urs.earthdata.nasa.gov'
+        NASA Earthdata URS 3 host
+
+    Returns
+    -------
+    token: dict
+        JSON response with NASA Earthdata User Token
+    """
+    # attempt to build urllib2 opener and check credentials
+    if build:
+        attempt_login(urs,
+            username=username,
+            password=password,
+            password_manager=False,
+            get_ca_certs=False,
+            redirect=False,
+            authorization_header=True)
+    # create post response with Earthdata token API
+    try:
+        request = urllib2.Request(HOST, method='POST')
+        response = urllib2.urlopen(request)
+    except urllib2.HTTPError as exc:
+        logging.debug(exc.code)
+        raise RuntimeError(exc.reason) from exc
+    except urllib2.URLError as exc:
+        logging.debug(exc.reason)
+        raise RuntimeError('Check internet connection') from exc
+    # read and return JSON response
+    return json.loads(response.read())
+
+# PURPOSE: generate a NASA Earthdata user token
+def list_tokens(
+        HOST: str = 'https://urs.earthdata.nasa.gov/api/users/tokens',
+        username: str | None = None,
+        password: str | None = None,
+        build: bool = True,
+        urs: str = 'urs.earthdata.nasa.gov',
+    ):
+    """
+    List the current associated NASA Earthdata User Tokens
+
+    Parameters
+    ----------
+    HOST: str
+        NASA Earthdata list token API host
+    username: str or NoneType, default None
+        NASA Earthdata username
+    password: str or NoneType, default None
+        NASA Earthdata password
+    build: bool, default True
+        Build opener and check WebDAV credentials
+    timeout: int or NoneType, default None
+        timeout in seconds for blocking operations
+    urs: str, default 'urs.earthdata.nasa.gov'
+        NASA Earthdata URS 3 host
+
+    Returns
+    -------
+    tokens: list
+        JSON response with NASA Earthdata User Tokens
+    """
+    # attempt to build urllib2 opener and check credentials
+    if build:
+        attempt_login(urs,
+            username=username,
+            password=password,
+            password_manager=False,
+            get_ca_certs=False,
+            redirect=False,
+            authorization_header=True)
+    # create get response with Earthdata list tokens API
+    try:
+        request = urllib2.Request(HOST)
+        response = urllib2.urlopen(request)
+    except urllib2.HTTPError as exc:
+        logging.debug(exc.code)
+        raise RuntimeError(exc.reason) from exc
+    except urllib2.URLError as exc:
+        logging.debug(exc.reason)
+        raise RuntimeError('Check internet connection') from exc
+    # read and return JSON response
+    return json.loads(response.read())
+
+# PURPOSE: revoke a NASA Earthdata user token
+def revoke_token(
+        token: str,
+        HOST: str = f'https://urs.earthdata.nasa.gov/api/users/revoke_token',
+        username: str | None = None,
+        password: str | None = None,
+        build: bool = True,
+        urs: str = 'urs.earthdata.nasa.gov',
+    ):
+    """
+    Generate a NASA Earthdata User Token
+
+    Parameters
+    ----------
+    token: str
+        NASA Earthdata token to be revoked
+    HOST: str
+        NASA Earthdata revoke token API host
+    username: str or NoneType, default None
+        NASA Earthdata username
+    password: str or NoneType, default None
+        NASA Earthdata password
+    build: bool, default True
+        Build opener and check WebDAV credentials
+    timeout: int or NoneType, default None
+        timeout in seconds for blocking operations
+    urs: str, default 'urs.earthdata.nasa.gov'
+        NASA Earthdata URS 3 host
+    """
+    # attempt to build urllib2 opener and check credentials
+    if build:
+        attempt_login(urs,
+            username=username,
+            password=password,
+            password_manager=False,
+            get_ca_certs=False,
+            redirect=False,
+            authorization_header=True)
+    # full path for NASA Earthdata revoke token API
+    url = f'{HOST}?token={token}'
+    # create post response with Earthdata revoke tokens API
+    try:
+        request = urllib2.Request(url, method='POST')
+        response = urllib2.urlopen(request)
+    except urllib2.HTTPError as exc:
+        logging.debug(exc.code)
+        raise RuntimeError(exc.reason) from exc
+    except urllib2.URLError as exc:
+        logging.debug(exc.reason)
+        raise RuntimeError('Check internet connection') from exc
+    # verbose response
+    logging.debug(f'Token Revoked: {token}')
+
 # NASA on-prem DAAC providers
 _daac_providers = {
     'gesdisc': 'GES_DISC',
     'ghrcdaac': 'GHRC_DAAC',
     'lpdaac': 'LPDAAC_ECS',
     'nsidc': 'NSIDC_ECS',
     'ornldaac': 'ORNL_DAAC',
@@ -856,16 +1100,19 @@
     'nsidc': 'nsidc-cumulus-prod-protected',
     'ornldaac': 'ornl-cumulus-prod-protected',
     'podaac': 'podaac-ops-cumulus-protected',
     'podaac-doc': 'podaac-ops-cumulus-docs'
 }
 
 # PURPOSE: get AWS s3 client for PO.DAAC Cumulus
-def s3_client(HOST=_s3_endpoints['podaac'], timeout=None,
-    region_name='us-west-2'):
+def s3_client(
+        HOST: str = _s3_endpoints['podaac'],
+        timeout: int | None = None,
+        region_name: str = 'us-west-2'
+    ):
     """
     Get AWS s3 client for PO.DAAC Cumulus
 
     Parameters
     ----------
     HOST: str
         PO.DAAC or ECCO AWS S3 credential host
@@ -888,15 +1135,15 @@
         aws_secret_access_key=cumulus['secretAccessKey'],
         aws_session_token=cumulus['sessionToken'],
         region_name=region_name)
     # return the AWS client for region
     return client
 
 # PURPOSE: get a s3 bucket name from a presigned url
-def s3_bucket(presigned_url):
+def s3_bucket(presigned_url: str) -> str:
     """
     Get a s3 bucket name from a presigned url
 
     Parameters
     ----------
     presigned_url: str
         s3 presigned url
@@ -907,15 +1154,15 @@
         s3 bucket name
     """
     host = url_split(presigned_url)
     bucket = re.sub(r's3:\/\/', r'', host[0], re.IGNORECASE)
     return bucket
 
 # PURPOSE: get a s3 bucket key from a presigned url
-def s3_key(presigned_url):
+def s3_key(presigned_url: str) -> str:
     """
     Get a s3 bucket key from a presigned url
 
     Parameters
     ----------
     presigned_url: str
         s3 presigned url
@@ -926,15 +1173,15 @@
         s3 bucket key for object
     """
     host = url_split(presigned_url)
     key = posixpath.join(*host[1:])
     return key
 
 # PURPOSE: check that entered NASA Earthdata credentials are valid
-def check_credentials(HOST='https://podaac-tools.jpl.nasa.gov/drive/files'):
+def check_credentials(HOST: str = _s3_endpoints['podaac']):
     """
     Check that entered NASA Earthdata credentials are valid
 
     HOST: str
         full url to protected credential website
     """
     try:
@@ -944,17 +1191,25 @@
         raise RuntimeError('Check your NASA Earthdata credentials')
     except urllib2.URLError:
         raise RuntimeError('Check internet connection')
     else:
         return True
 
 # PURPOSE: list a directory on JPL PO.DAAC/ECCO Drive https server
-def drive_list(HOST, username=None, password=None, build=True,
-    timeout=None, urs='podaac-tools.jpl.nasa.gov',
-    parser=lxml.etree.HTMLParser(), pattern='', sort=False):
+def drive_list(
+        HOST: str | list,
+        username: str | None = None,
+        password: str | None = None,
+        build: bool = True,
+        timeout: int | None = None,
+        urs: str = 'podaac-tools.jpl.nasa.gov',
+        parser=lxml.etree.HTMLParser(),
+        pattern: str = '',
+        sort: bool = False
+    ):
     """
     List a directory on
     `JPL PO.DAAC <https://podaac-tools.jpl.nasa.gov/drive>`_ or
     `ECCO Drive <https://ecco.jpl.nasa.gov/drive/>`_
 
     Parameters
     ----------
@@ -996,15 +1251,15 @@
     # verify inputs for remote https host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try listing from https
     try:
         # Create and submit request.
         request = urllib2.Request(posixpath.join(*HOST))
-        tree = lxml.etree.parse(urllib2.urlopen(request,timeout=timeout),parser)
+        tree = lxml.etree.parse(urllib2.urlopen(request, timeout=timeout),parser)
     except (urllib2.HTTPError, urllib2.URLError) as exc:
         raise Exception('List error from {0}'.format(posixpath.join(*HOST)))
     else:
         # read and parse request for files (column names and modified times)
         colnames = tree.xpath('//tr/td//a[@class="text-left"]/text()')
         # get the Unix timestamp value for a modification time
         collastmod = [get_unix_time(i) for i in tree.xpath('//tr/td[3]/text()')]
@@ -1020,17 +1275,28 @@
             # sort list of column names and last modified times
             colnames = [colnames[indice] for indice in i]
             collastmod = [collastmod[indice] for indice in i]
         # return the list of column names and last modified times
         return (colnames,collastmod)
 
 # PURPOSE: download a file from a PO.DAAC/ECCO Drive https server
-def from_drive(HOST, username=None, password=None, build=True,
-    timeout=None, urs='podaac-tools.jpl.nasa.gov', local=None,
-    hash='', chunk=16384, verbose=False, fid=sys.stdout, mode=0o775):
+def from_drive(
+        HOST: str | list,
+        username: str | None = None,
+        password: str | None = None,
+        build: bool = True,
+        timeout: int | None = None,
+        urs: str = 'podaac-tools.jpl.nasa.gov',
+        local: str | pathlib.Path | None = None,
+        hash: str = '',
+        chunk: int = 16384,
+        verbose: bool = False,
+        fid = sys.stdout,
+        mode: oct = 0o775
+    ):
     """
     Download a file from a
     `JPL PO.DAAC <https://podaac-tools.jpl.nasa.gov/drive>`_ or
     `ECCO Drive <https://ecco.jpl.nasa.gov/drive/>`_ https server
 
     Parameters
     ----------
@@ -1079,48 +1345,53 @@
     # verify inputs for remote https host
     if isinstance(HOST, str):
         HOST = url_split(HOST)
     # try downloading from https
     try:
         # Create and submit request.
         request = urllib2.Request(posixpath.join(*HOST))
-        response = urllib2.urlopen(request,timeout=timeout)
+        response = urllib2.urlopen(request, timeout=timeout)
     except (urllib2.HTTPError, urllib2.URLError) as exc:
         raise Exception('Download error from {0}'.format(posixpath.join(*HOST)))
     else:
         # copy remote file contents to bytesIO object
         remote_buffer = io.BytesIO()
         shutil.copyfileobj(response, remote_buffer, chunk)
         remote_buffer.seek(0)
         # save file basename with bytesIO object
         remote_buffer.filename = HOST[-1]
         # generate checksum hash for remote file
         remote_hash = hashlib.md5(remote_buffer.getvalue()).hexdigest()
         # compare checksums
         if local and (hash != remote_hash):
             # convert to absolute path
-            local = os.path.abspath(local)
+            local = pathlib.Path(local).expanduser().absolute()
             # create directory if non-existent
-            if not os.access(os.path.dirname(local), os.F_OK):
-                os.makedirs(os.path.dirname(local), mode)
+            local.parent.mkdir(mode=mode, parents=True, exist_ok=True)
             # print file information
-            args = (posixpath.join(*HOST),local)
+            args = (posixpath.join(*HOST), str(local))
             logging.info('{0} -->\n\t{1}'.format(*args))
             # store bytes to file using chunked transfer encoding
             remote_buffer.seek(0)
-            with open(os.path.expanduser(local), 'wb') as f:
+            with local.open(mode='wb') as f:
                 shutil.copyfileobj(remote_buffer, f, chunk)
             # change the permissions mode
-            os.chmod(local,mode)
+            local.chmod(mode=mode)
         # return the bytesIO object
         remote_buffer.seek(0)
         return remote_buffer
 
 # PURPOSE: retrieve shortnames for GRACE/GRACE-FO products
-def cmr_product_shortname(mission, center, release, level='L2', version='0'):
+def cmr_product_shortname(
+        mission: str,
+        center: str,
+        release: str,
+        level: str = 'L2',
+        version: str = '0'
+    ):
     """
     Create a list of product shortnames for NASA Common Metadata
     Repository (CMR) queries
 
     Parameters
     ----------
     mission: str
@@ -1211,15 +1482,20 @@
     try:
         cmr_shortnames = cmr_shortname[mission][level][center][release]
     except Exception as exc:
         raise Exception('NASA CMR shortname not found')
     else:
         return cmr_shortnames
 
-def cmr_readable_granules(product, level='L2', solution='BA01', version='0'):
+def cmr_readable_granules(
+        product: str,
+        level: str = 'L2',
+        solution: str = 'BA01',
+        version: str = '0'
+    ):
     """
     Create readable granule names pattern for NASA Common Metadata
     Repository (CMR) queries
 
     Parameters
     ----------
     product: str
@@ -1256,15 +1532,18 @@
         pattern = '{0}-2_???????-???????_????_?????_{1}_???{2}*'.format(*args)
     else:
         pattern = '*'
     # return readable granules pattern
     return pattern
 
 # PURPOSE: filter the CMR json response for desired data files
-def cmr_filter_json(search_results, endpoint="data"):
+def cmr_filter_json(
+        search_results: dict,
+        endpoint: str = 'data'
+    ):
     """
     Filter the NASA Common Metadata Repository (CMR) json
     response for desired data files
 
     Parameters
     ----------
     search_results: dict
@@ -1304,15 +1583,18 @@
             if (link['rel'] == rel[endpoint]):
                 granule_urls.append(link['href'])
                 break
     # return the list of urls, granule ids and modified times
     return (granule_names,granule_urls,granule_mtimes)
 
 # PURPOSE: filter the CMR json response for desired metadata files
-def cmr_metadata_json(search_results, endpoint="data"):
+def cmr_metadata_json(
+        search_results: dict,
+        endpoint: str = 'data'
+    ):
     """
     Filter the NASA Common Metadata Repository (CMR) json response
     for desired metadata files
 
     Parameters
     ----------
     search_results: dict
@@ -1344,46 +1626,58 @@
         for link in entry['links']:
             if (link['rel'] == rel[endpoint]):
                 collection_urls.append(link['href'])
     # return the list of urls
     return collection_urls
 
 # PURPOSE: cmr queries for GRACE/GRACE-FO products
-def cmr(mission=None, center=None, release=None, level='L2', product=None,
-    solution='BA01', version='0', start_date=None, end_date=None,
-    provider='POCLOUD', endpoint='data', verbose=False, fid=sys.stdout):
+def cmr(
+        mission: str | None = None,
+        center: str | None = None,
+        release: str | None = None,
+        level: str | None = 'L2',
+        product: str | None = None,
+        solution: str | None = 'BA01',
+        version: str | None = '0',
+        start_date: str | None = None,
+        end_date: str | None = None,
+        provider: str | None = 'POCLOUD',
+        endpoint: str | None = 'data',
+        verbose: bool = False,
+        fid = sys.stdout
+    ):
     """
     Query the NASA Common Metadata Repository (CMR) for GRACE/GRACE-FO data
 
     Parameters
     ----------
     mission: str or NoneType, default None
         GRACE (``'grace'``) or GRACE Follow-On (``'grace-fo'``)
     center: str or NoneType, default None
         GRACE/GRACE-FO processing center
     release: str or NoneType, default None
         GRACE/GRACE-FO data release
-    level: str, default 'L2'
+    level: str or NoneType, default 'L2'
         GRACE/GRACE-FO product level
     product: str or NoneType, default None
         GRACE/GRACE-FO data product
-    solution: str, default 'BA01'
+    solution: str or NoneType, default 'BA01'
         monthly gravity field solution for Release-06
-    version: str, default '0'
+    version: str or NoneType, default '0'
         GRACE/GRACE-FO Level-2 data version
     start_date: str or NoneType, default None
         starting date for CMR product query
     end_date: str or NoneType, default None
         ending date for CMR product query
-    provider: str, default 'POCLOUD'
+    provider: str or NoneType, default 'POCLOUD'
         CMR data provider
 
             - ``'PODAAC'``: PO.DAAC Drive
             - ``'POCLOUD'``: PO.DAAC Cumulus
-    endpoint: str, default 'data'
+    endpoint: str or NoneType, default 'data'
         url endpoint type
 
             - ``'data'``: PO.DAAC https archive
             - ``'s3'``: PO.DAAC Cumulus AWS S3 bucket
     verbose: bool, default False
         print CMR query information
     fid: obj, default sys.stdout
@@ -1465,17 +1759,26 @@
         granule_names.extend(ids)
         granule_urls.extend(urls)
         granule_mtimes.extend(mtimes)
     # return the list of granule ids, urls and modification times
     return (granule_names, granule_urls, granule_mtimes)
 
 # PURPOSE: cmr queries for GRACE/GRACE-FO auxiliary data and documentation
-def cmr_metadata(mission=None, center=None, release=None, level='L2',
-    version='0', provider='POCLOUD', endpoint='data', pattern='',
-    verbose=False, fid=sys.stdout):
+def cmr_metadata(
+        mission: str | None = None,
+        center: str | None = None,
+        release: str | None = None,
+        level: str | None = 'L2',
+        version: str | None = '0',
+        provider: str | None = 'POCLOUD',
+        endpoint: str | None = 'data',
+        pattern: str | None = '',
+        verbose: bool = False,
+        fid = sys.stdout
+    ):
     """
     Query the NASA Common Metadata Repository (CMR) for GRACE/GRACE-FO
     auxiliary data and documentation
 
     Parameters
     ----------
     mission: str or NoneType, default None
@@ -1551,16 +1854,22 @@
         i = [i for i,f in enumerate(collection_urls) if re.search(pattern,f)]
         # reduce list of collection_urls
         collection_urls = [collection_urls[indice] for indice in i]
     # return the list of collection urls
     return collection_urls
 
 # PURPOSE: create and compile regular expression operator to find GRACE files
-def compile_regex_pattern(PROC, DREL, DSET, mission=None,
-    solution=r'BA01', version=r'\d+'):
+def compile_regex_pattern(
+        PROC: str,
+        DREL: str,
+        DSET: str,
+        mission: str | None = None,
+        solution: str | None = r'BA01',
+        version: str | None = r'\d+'
+    ):
     """
     Compile regular expressor operators for finding a specified
     subset of GRACE/GRACE-FO Level-2 spherical harmonic files
 
     Parameters
     ----------
     PROC: str
@@ -1654,17 +1963,25 @@
         pattern = r'{0}-2_([a-zA-Z0-9_\-]+)(\.gz)?$'
     # return the compiled regular expression operator
     return re.compile(pattern.format(*args), re.VERBOSE)
 
 # PURPOSE: download geocenter files from Sutterley and Velicogna (2019)
 # https://doi.org/10.3390/rs11182108
 # https://doi.org/10.6084/m9.figshare.7388540
-def from_figshare(directory, article='7388540', timeout=None,
-    context=_default_ssl_context, chunk=16384, verbose=False, fid=sys.stdout,
-    pattern=r'(CSR|GFZ|JPL)_(RL\d+)_(.*?)_SLF_iter.txt$', mode=0o775):
+def from_figshare(
+        directory: str | pathlib.Path,
+        article: str = '7388540',
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        chunk: int | None = 16384,
+        verbose: bool = False,
+        fid = sys.stdout,
+        pattern: str = r'(CSR|GFZ|JPL)_(RL\d+)_(.*?)_SLF_iter.txt$',
+        mode: oct = 0o775
+    ):
     """
     Download [Sutterley2019]_ geocenter files from
     `figshare <https://doi.org/10.6084/m9.figshare.7388540>`_
 
     Parameters
     ----------
     directory: str
@@ -1692,38 +2009,53 @@
         "Improved Estimates of Geocenter Variability from Time-Variable Gravity
         and Ocean Model Outputs", *Remote Sensing*, 11(18), 2108, (2019).
         `doi: 10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
     """
     # figshare host
     HOST=['https://api.figshare.com','v2','articles',article]
     # recursively create directory if non-existent
-    directory = os.path.abspath(os.path.expanduser(directory))
-    if not os.access(os.path.join(directory,'geocenter'), os.F_OK):
-        os.makedirs(os.path.join(directory,'geocenter'), mode)
+    directory = pathlib.Path(directory).expanduser().absolute()
+    local_dir = directory.joinpath('geocenter')
+    local_dir.mkdir(mode=mode, parents=True, exist_ok=True)
     # Create and submit request.
     request = urllib2.Request(posixpath.join(*HOST))
-    response = urllib2.urlopen(request,timeout=timeout,context=context)
+    response = urllib2.urlopen(request, timeout=timeout,context=context)
     resp = json.loads(response.read())
     # reduce list of geocenter files
     geocenter_files = [f for f in resp['files'] if re.match(pattern,f['name'])]
     for f in geocenter_files:
         # download geocenter file
-        original_md5 = get_hash(os.path.join(directory,'geocenter',f['name']))
-        from_http(url_split(f['download_url']),timeout=timeout,context=context,
-            local=os.path.join(directory,'geocenter',f['name']),
-            hash=original_md5,chunk=chunk,verbose=verbose,fid=fid,mode=mode)
+        local_file = local_dir.joinpath(f['name'])
+        original_md5 = get_hash(local_file)
+        from_http(f['download_url'],
+            timeout=timeout,
+            context=context,
+            local=local_file,
+            hash=original_md5,
+            chunk=chunk,
+            verbose=verbose,
+            fid=fid,
+            mode=mode)
         # verify MD5 checksums
-        computed_md5 = get_hash(os.path.join(directory,'geocenter',f['name']))
+        computed_md5 = get_hash(local_file)
         if (computed_md5 != f['supplied_md5']):
-            raise Exception('Checksum mismatch: {0}'.format(f['download_url']))
+            raise Exception(f'Checksum mismatch: {f["download_url"]}')
 
 # PURPOSE: send files to figshare using secure FTP uploader
-def to_figshare(files, username=None, password=None, directory=None,
-    timeout=None, context=ssl.SSLContext(ssl.PROTOCOL_TLS),
-    get_ca_certs=False, verbose=False, chunk=8192):
+def to_figshare(
+        files: list,
+        username: str | None = None,
+        password: str | None = None,
+        directory: str | None | pathlib.Path = None,
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        get_ca_certs: bool = False,
+        verbose: bool = False,
+        chunk: int = 8192
+    ):
     """
     Send files to figshare using secure `FTP uploader
     <https://help.figshare.com/article/upload-large-datasets-and-
     bulk-upload-using-the-ftp-uploader-desktop-uploader-or-api>`_
 
     Parameters
     ----------
@@ -1763,27 +2095,36 @@
     try:
         # will only create the directory if non-existent
         ftps.mkd(posixpath.join('data',directory))
     except:
         pass
     # upload each file
     for local_file in files:
+        # local file
+        local_file = pathlib.Path(local_file).expanduser().absolute()
         # remote ftp file
         ftp_remote_path = posixpath.join('data',directory,
-            os.path.basename(local_file))
+            local_file.name)
         # open local file and send bytes
-        with open(os.path.expanduser(local_file),'rb') as fp:
+        with local_file.open(mode='rb') as fp:
             ftps.storbinary(f'STOR {ftp_remote_path}', fp,
                 blocksize=chunk, callback=None, rest=None)
 
 # PURPOSE: download satellite laser ranging files from CSR
 # http://download.csr.utexas.edu/pub/slr/geocenter/GCN_L1_L2_30d_CF-CM.txt
 # http://download.csr.utexas.edu/outgoing/cheng/gct2est.220_5s
-def from_csr(directory, timeout=None, context=_default_ssl_context,
-    chunk=16384, verbose=False, fid=sys.stdout, mode=0o775):
+def from_csr(
+        directory: str | pathlib.Path,
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        chunk: int | None = 16384,
+        verbose: bool = False,
+        fid = sys.stdout,
+        mode: oct = 0o775
+    ):
     """
     Download `satellite laser ranging (SLR)
     <http://download.csr.utexas.edu/pub/slr/>`_
     files from the University of Texas Center for Space Research (UTCSR)
 
     Parameters
     ----------
@@ -1801,48 +2142,67 @@
         open file object to print if verbose
     mode: oct, default 0o775
         permissions mode of output local file
     """
     # CSR download http server
     HOST = 'http://download.csr.utexas.edu'
     # recursively create directory if non-existent
-    directory = os.path.abspath(os.path.expanduser(directory))
-    if not os.access(os.path.join(directory,'geocenter'), os.F_OK):
-        os.makedirs(os.path.join(directory,'geocenter'), mode)
+    directory = pathlib.Path(directory).expanduser().absolute()
+    local_dir = directory.joinpath('geocenter')
+    local_dir.mkdir(mode=mode, parents=True, exist_ok=True)
     # download SLR 5x5, figure axis and azimuthal dependence files
     FILES = []
     FILES.append([HOST,'pub','slr','degree_5',
         'CSR_Monthly_5x5_Gravity_Harmonics.txt'])
     FILES.append([HOST,'pub','slr','degree_2','C20_RL06.txt'])
     FILES.append([HOST,'pub','slr','degree_2','C21_S21_RL06.txt'])
     FILES.append([HOST,'pub','slr','degree_2','C22_S22_RL06.txt'])
+    FILES.append([HOST,'pub','slr','TN11E','TN11E.txt'])
     # for each SLR file
     for FILE in FILES:
-        original_md5 = get_hash(os.path.join(directory,FILE[-1]))
-        from_http(FILE,timeout=timeout,context=context,
-            local=os.path.join(directory,FILE[-1]),
-            hash=original_md5,chunk=chunk,verbose=verbose,
-            fid=fid,mode=mode)
+        local_file = directory.joinpath(FILE[-1])
+        original_md5 = get_hash(local_file)
+        from_http(FILE,
+            timeout=timeout,
+            context=context,
+            local=local_file,
+            hash=original_md5,
+            chunk=chunk,
+            verbose=verbose,
+            fid=fid,
+            mode=mode)
     # download CF-CM SLR and updated SLR geocenter files from Minkang Cheng
     FILES = []
     FILES.append([HOST,'pub','slr','geocenter','GCN_L1_L2_30d_CF-CM.txt'])
     FILES.append([HOST,'outgoing','cheng','gct2est.220_5s'])
     # for each SLR geocenter file
     for FILE in FILES:
-        original_md5 = get_hash(os.path.join(directory,'geocenter',FILE[-1]))
-        from_http(FILE,timeout=timeout,context=context,
-            local=os.path.join(directory,'geocenter',FILE[-1]),
-            hash=original_md5,chunk=chunk,verbose=verbose,
-            fid=fid,mode=mode)
+        local_file = local_dir.joinpath(FILE[-1])
+        original_md5 = get_hash(local_file)
+        from_http(FILE,
+            timeout=timeout,
+            context=context,
+            local=local_file,
+            hash=original_md5,
+            chunk=chunk,
+            verbose=verbose,
+            fid=fid,
+            mode=mode)
 
 # PURPOSE: download GravIS and satellite laser ranging files from GFZ
 # ftp://isdcftp.gfz-potsdam.de/grace/Level-2/GFZ/RL06_SLR_C20/
 # ftp://isdcftp.gfz-potsdam.de/grace/GravIS/GFZ/Level-2B/aux_data/
-def from_gfz(directory, timeout=None, chunk=8192, verbose=False,
-    fid=sys.stdout, mode=0o775):
+def from_gfz(
+        directory: str | pathlib.Path,
+        timeout: int | None = None,
+        chunk: int | None = 8192,
+        verbose: bool = False,
+        fid = sys.stdout,
+        mode: oct = 0o775
+    ):
     """
     Download GravIS and satellite laser ranging (SLR) files from the
     German Research Centre for Geosciences (GeoForschungsZentrum, GFZ)
 
     Parameters
     ----------
     directory: str
@@ -1855,41 +2215,60 @@
         print file transfer information
     fid: obj, default sys.stdout
         open file object to print if verbose
     mode: oct, default 0o775
         permissions mode of output local file
     """
     # recursively create directories if non-existent
-    directory = os.path.abspath(os.path.expanduser(directory))
-    if not os.access(os.path.join(directory,'geocenter'), os.F_OK):
-        os.makedirs(os.path.join(directory,'geocenter'), mode)
+    directory = pathlib.Path(directory).expanduser().absolute()
+    local_dir = directory.joinpath('geocenter')
+    local_dir.mkdir(mode=mode, parents=True, exist_ok=True)
     # SLR oblateness and combined low-degree harmonic files
     FILES = []
     FILES.append(['isdcftp.gfz-potsdam.de','grace','Level-2','GFZ',
         'RL06_SLR_C20','GFZ_RL06_C20_SLR.dat'])
     FILES.append(['isdcftp.gfz-potsdam.de','grace','GravIS','GFZ',
-        'Level-2B','aux_data','GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0002.dat'])
+        'Level-2B','aux_data','GRAVIS-2B_GFZOP_GRACE+SLR_LOW_DEGREES_0003.dat'])
     # get each file
     for FILE in FILES:
-        local = os.path.join(directory,FILE[-1])
-        from_ftp(FILE,timeout=timeout,local=local,hash=get_hash(local),
-            chunk=chunk,verbose=verbose,fid=fid,mode=mode)
+        local_file = directory.joinpath(FILE[-1])
+        from_ftp(FILE,
+            timeout=timeout,
+            local=local_file,
+            hash=get_hash(local_file),
+            chunk=chunk,
+            verbose=verbose,
+            fid=fid,
+            mode=mode)
     # GravIS geocenter file
     FILE = ['isdcftp.gfz-potsdam.de','grace','GravIS','GFZ','Level-2B',
-        'aux_data','GRAVIS-2B_GFZOP_GEOCENTER_0002.dat']
-    local = os.path.join(directory,'geocenter',FILE[-1])
-    from_ftp(FILE,timeout=timeout,local=local,hash=get_hash(local),
-        chunk=chunk,verbose=verbose,fid=fid,mode=mode)
+        'aux_data','GRAVIS-2B_GFZOP_GEOCENTER_0003.dat']
+    local_file = local_dir.joinpath(FILE[-1])
+    from_ftp(FILE,
+        timeout=timeout,
+        local=local_file,
+        hash=get_hash(local_file),
+        chunk=chunk,
+        verbose=verbose,
+        fid=fid,
+        mode=mode)
 
 # PURPOSE: download satellite laser ranging files from GSFC
 # https://earth.gsfc.nasa.gov/geo/data/slr
-def from_gsfc(directory,
-    host='https://earth.gsfc.nasa.gov/sites/default/files/geo/slr-weekly',
-    timeout=None, context=_default_ssl_context, chunk=16384, verbose=False,
-    fid=sys.stdout, copy=False, mode=0o775):
+def from_gsfc(
+        directory: str | pathlib.Path,
+        host: str = 'https://earth.gsfc.nasa.gov/sites/default/files/geo/slr-weekly',
+        timeout: int | None = None,
+        context = _default_ssl_context,
+        chunk: int | None = 16384,
+        verbose: bool = False,
+        fid = sys.stdout,
+        copy: bool = False,
+        mode: oct = 0o775
+    ):
     """
     Download `satellite laser ranging (SLR) <https://earth.gsfc.nasa.gov/geo/data/slr/>`_
     files from NASA Goddard Space Flight Center (GSFC)
 
     Parameters
     ----------
     directory: str
@@ -1908,45 +2287,52 @@
         open file object to print if verbose
     copy: bool, default False
         create a copy of file for archival purposes
     mode: oct, default 0o775
         permissions mode of output local file
     """
     # recursively create directory if non-existent
-    directory = os.path.abspath(os.path.expanduser(directory))
-    if not os.access(directory, os.F_OK):
-        os.makedirs(directory, mode)
+    directory = pathlib.Path(directory).expanduser().absolute()
+    directory.mkdir(mode=mode, parents=True, exist_ok=True)
     # download GSFC SLR 5x5 file
     FILE = 'gsfc_slr_5x5c61s61.txt'
-    original_md5 = get_hash(os.path.join(directory,FILE))
+    local_file = directory.joinpath(FILE)
+    original_md5 = get_hash(local_file)
     fileID = from_http(posixpath.join(host,FILE),
-        timeout=timeout, context=context,
-        local=os.path.join(directory,FILE),
-        hash=original_md5, chunk=chunk, verbose=verbose,
-        fid=fid, mode=mode)
+        timeout=timeout,
+        context=context,
+        local=local_file,
+        hash=original_md5,
+        chunk=chunk,
+        verbose=verbose,
+        fid=fid,
+        mode=mode)
     # create a dated copy for archival purposes
     if copy:
         # create copy of file for archiving
         # read file and extract data date span
         file_contents = fileID.read().decode('utf-8').splitlines()
         data_span, = [l for l in file_contents if l.startswith('Data span:')]
         # extract start and end of data date span
         span_start,span_end = re.findall(r'\d+[\s+]\w{3}[\s+]\d{4}', data_span)
         # create copy of file with date span in filename
-        COPY = 'GSFC_SLR_5x5c61s61_{0}_{1}.txt'.format(
-            time.strftime('%Y%m', time.strptime(span_start, '%d %b %Y')),
-            time.strftime('%Y%m', time.strptime(span_end, '%d %b %Y')))
-        shutil.copyfile(os.path.join(directory,FILE), os.path.join(directory,COPY))
+        YM1 = time.strftime('%Y%m', time.strptime(span_start, '%d %b %Y'))
+        YM2 = time.strftime('%Y%m', time.strptime(span_end, '%d %b %Y'))
+        COPY = f'GSFC_SLR_5x5c61s61_{YM1}_{YM2}.txt'
+        shutil.copyfile(local_file, directory.joinpath(COPY))
         # copy modification times and permissions for archive file
-        shutil.copystat(os.path.join(directory,FILE), os.path.join(directory,COPY))
+        shutil.copystat(local_file, directory.joinpath(COPY))
 
 # PURPOSE: list a directory on the GFZ ICGEM https server
 # http://icgem.gfz-potsdam.de
-def icgem_list(host='http://icgem.gfz-potsdam.de/tom_longtime',
-    timeout=None, parser=lxml.etree.HTMLParser()):
+def icgem_list(
+        host: str = 'http://icgem.gfz-potsdam.de/tom_longtime',
+        timeout: int | None = None,
+        parser=lxml.etree.HTMLParser()
+    ):
     """
     Parse the table of static gravity field models on the GFZ
     `International Centre for Global Earth Models (ICGEM) <http://icgem.gfz-potsdam.de/>`_
     server
 
     Parameters
     ----------
@@ -1962,15 +2348,15 @@
     colfiles: dict
         Static gravity field file urls mapped by field name
     """
     # try listing from https
     try:
         # Create and submit request.
         request = urllib2.Request(host)
-        tree = lxml.etree.parse(urllib2.urlopen(request,timeout=timeout),parser)
+        tree = lxml.etree.parse(urllib2.urlopen(request, timeout=timeout),parser)
     except:
         raise Exception(f'List error from {host}')
     else:
         # read and parse request for files
         colfiles = tree.xpath('//td[@class="tom-cell-modelfile"]//a/@href')
         # reduce list of files to find gfc files
         # return the dict of model files mapped by name
```

### Comparing `gravity-toolkit-1.2.0/gravity_toolkit.egg-info/PKG-INFO` & `gravity-toolkit-1.2.1/gravity_toolkit.egg-info/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: gravity-toolkit
-Version: 1.2.0
+Version: 1.2.1
 Summary: Python tools for obtaining and working with spherical harmoniccoefficients from the NASA/DLR GRACE and NASA/GFZ GRACE Follow-on missions
 Home-page: https://github.com/tsutterley/gravity-toolkit
 Author: Tyler Sutterley
 Author-email: tsutterl@uw.edu
 License: MIT
 Keywords: GRACE,GRACE-FO,Gravity,satellite geodesy,spherical harmonics
 Classifier: Development Status :: 3 - Alpha
@@ -24,36 +24,28 @@
 gravity-toolkit
 ===============
 
 |Language|
 |License|
 |PyPI Version|
 |Documentation Status|
-|Binder|
-|Pangeo|
 |zenodo|
 
 .. |Language| image:: https://img.shields.io/pypi/pyversions/gravity-toolkit?color=green
    :target: https://www.python.org/
 
 .. |License| image:: https://img.shields.io/github/license/tsutterley/gravity-toolkit
    :target: https://github.com/tsutterley/gravity-toolkit/blob/main/LICENSE
 
 .. |PyPI Version| image:: https://img.shields.io/pypi/v/gravity-toolkit.svg
    :target: https://pypi.python.org/pypi/gravity-toolkit/
 
 .. |Documentation Status| image:: https://readthedocs.org/projects/gravity-toolkit/badge/?version=latest
    :target: https://gravity-toolkit.readthedocs.io/en/latest/?badge=latest
 
-.. |Binder| image:: https://mybinder.org/badge_logo.svg
-   :target: https://mybinder.org/v2/gh/tsutterley/gravity-toolkit/main
-
-.. |Pangeo| image:: https://img.shields.io/static/v1.svg?logo=Jupyter&label=PangeoBinderAWS&message=us-west-2&color=orange
-   :target: https://aws-uswest2-binder.pangeo.io/v2/gh/tsutterley/gravity-toolkit/main?urlpath=lab
-
 .. |zenodo| image:: https://zenodo.org/badge/107323776.svg
    :target: https://zenodo.org/badge/latestdoi/107323776
 
 Python tools for obtaining and working with Level-2 spherical harmonic coefficients from the NASA/DLR Gravity Recovery and Climate Experiment (GRACE) and the NASA/GFZ Gravity Recovery and Climate Experiment Follow-On (GRACE-FO) missions
 
 Resources
 #########
@@ -82,19 +74,19 @@
 - `geoid-toolkit: Python utilities for calculating geoid heights from static gravity field coefficients <https://github.com/tsutterley/geoid-toolkit/>`_
 
 References
 ##########
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Self-Consistent Ice Mass Balance
     and Regional Sea Level From Time-Variable Gravity", *Earth and Space Science*, 7,
-    (2020). `doi:10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
+    (2020). `doi: 10.1029/2019EA000860 <https://doi.org/10.1029/2019EA000860>`_
 
     T. C. Sutterley and I. Velicogna, "Improved estimates of geocenter variability
     from time-variable gravity and ocean model outputs", *Remote Sensing*, 11(18),
-    2108, (2019). `doi:10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
+    2108, (2019). `doi: 10.3390/rs11182108 <https://doi.org/10.3390/rs11182108>`_
 
     J. Wahr, S. C. Swenson, and I. Velicogna, "Accuracy of GRACE mass estimates",
     *Geophysical Research Letters*, 33(6), L06401, (2006).
     `doi: 10.1029/2005GL025305 <https://doi.org/10.1029/2005GL025305>`_
 
     J. Wahr, M. Molenaar, and F. Bryan, "Time variability of the Earth's gravity
     field: Hydrological and oceanic effects and their possible detection using
@@ -107,19 +99,19 @@
     `doi: 10.1111/j.1365-246X.1995.tb01819.x <https://doi.org/10.1111/j.1365-246X.1995.tb01819.x>`_
 
 Data Repositories
 #################
 
     T. C. Sutterley, I. Velicogna, and C.-W. Hsu, "Ice Mass and Regional Sea Level
     Estimates from Time-Variable Gravity", (2020).
-    `doi:10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
+    `doi: 10.6084/m9.figshare.9702338 <https://doi.org/10.6084/m9.figshare.9702338>`_
 
     T. C. Sutterley and I. Velicogna, "Geocenter Estimates from Time-Variable
     Gravity and Ocean Model Outputs", (2019).
-    `doi:10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
+    `doi: 10.6084/m9.figshare.7388540 <https://doi.org/10.6084/m9.figshare.7388540>`_
 
 Download
 ########
 
 | The program homepage is:
 | https://github.com/tsutterley/gravity-toolkit
 | A zip archive of the latest version is available directly at:
```

### Comparing `gravity-toolkit-1.2.0/scripts/aod1b_geocenter.py` & `gravity-toolkit-1.2.1/scripts/aod1b_geocenter.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 #!/usr/bin/env python
 u"""
 aod1b_geocenter.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte (03/2021)
 
 Reads GRACE/GRACE-FO level-1b dealiasing data files for a specific product
     atm: atmospheric loading from ECMWF
     ocn: oceanic loading from OMCT/MPIOM
     glo: global atmospheric and oceanic loading
     oba: ocean bottom pressure from OMCT/MPIOM
 
 Creates monthly files of geocenter variations at 3 or 6 hour intervals
 
-NOTE: this reads the GFZ AOD1B files downloaded from PO.DAAC
-https://podaac-uat.jpl.nasa.gov/drive/files/allData/grace/L1B/GFZ/AOD1B/RL06/
-
 CALLING SEQUENCE:
     python aod1b_geocenter.py --release RL06 --product atm ocn glo oba
 
 COMMAND LINE OPTIONS:
     -D X, --directory X: Working Data Directory
     -r X, --release X: GRACE/GRACE-FO Data Release (RL05 or RL06)
     -p X, --product X: GRACE/GRACE-FO dealiasing product (atm, ocn, glo, oba)
@@ -30,14 +27,16 @@
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 PROGRAM DEPENDENCIES:
     geocenter.py: converts degree 1 spherical harmonics to geocenter variations
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: debug-level logging of member names and header lines
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
     Updated 11/2021: use gravity_toolkit geocenter class for operations
     Updated 10/2021: using python logging for handling verbose output
     Updated 07/2021: can use default argument files to define options
@@ -57,18 +56,18 @@
         do not extract tar files to temp, extract contents of files to memory
     Updated 05-06/2016: oba=ocean bottom pressure, absolute import of shutil
     Written 05/2016
 """
 from __future__ import print_function, division
 
 import sys
-import os
 import re
 import gzip
 import logging
+import pathlib
 import tarfile
 import argparse
 import numpy as np
 import gravity_toolkit as gravtk
 
 # program module to read the degree 1 coefficients of the AOD1b data
 def aod1b_geocenter(base_dir,
@@ -108,15 +107,16 @@
     # will extract the data from the file
     regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?'
     rx = re.compile(regex_pattern, re.VERBOSE)
     # output formatting string
     fstr = '{0:4d}-{1:02d}-{2:02d}T{3:02d}:00:00 {4:12.8f} {5:12.8f} {6:12.8f}'
 
     # set number of hours in a file
-    # set the ocean model for a given release
+    # set the atmospheric and ocean model for a given release
+    # set the maximum degree and order of a release
     if DREL in ('RL01','RL02','RL03','RL04','RL05'):
         # for 00, 06, 12 and 18
         n_time = 4
         ATMOSPHERE = 'ECMWF'
         OCEAN_MODEL = 'OMCT'
         LMAX = 100
     elif DREL in ('RL06',):
@@ -134,85 +134,90 @@
     product = {}
     product['atm'] = f'Atmospheric loading from {ATMOSPHERE}'
     product['ocn'] = f'Oceanic loading from {OCEAN_MODEL}'
     product['glo'] = 'Global atmospheric and oceanic loading'
     product['oba'] = f'Ocean bottom pressure from {OCEAN_MODEL}'
 
     # AOD1B directory and output geocenter directory
-    grace_dir = os.path.join(base_dir,'AOD1B',DREL)
-    output_dir = os.path.join(grace_dir,'geocenter')
-    if not os.access(output_dir, os.F_OK):
-        os.mkdir(output_dir, MODE)
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath('AOD1B',DREL)
+    output_dir = grace_dir.joinpath('geocenter')
+    output_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # finding all of the tar files in the AOD1b directory
-    input_tar_files = [tf for tf in os.listdir(grace_dir) if tx.match(tf)]
+    input_tar_files = [tf for tf in grace_dir.iterdir() if tx.match(tf.name)]
 
     # for each tar file
-    for i in sorted(input_tar_files):
+    for input_file in sorted(input_tar_files):
         # extract the year and month from the file
-        YY,MM,SFX = tx.findall(i).pop()
-        YY,MM = np.array([YY,MM], dtype=np.int64)
+        YY,MM,SFX = tx.findall(input_file.name).pop()
+        YY,MM = np.array([YY, MM], dtype=np.int64)
         # output monthly geocenter file
         FILE = f'AOD1B_{DREL}_{DSET}_{YY:4d}_{MM:02d}.txt'
+        output_file = output_dir.joinpath(FILE)
         # if output file exists: check if input tar file is newer
         TEST = False
         OVERWRITE = ' (clobber)'
         # check if output file exists
-        if os.access(os.path.join(output_dir,FILE), os.F_OK):
+        if output_file.exists():
             # check last modification time of input and output files
-            input_mtime = os.stat(os.path.join(grace_dir,i)).st_mtime
-            output_mtime = os.stat(os.path.join(output_dir,FILE)).st_mtime
+            input_mtime = input_file.stat().st_mtime
+            output_mtime = output_file.stat().st_mtime
             # if input tar file is newer: overwrite the output file
             if (input_mtime > output_mtime):
                 TEST = True
                 OVERWRITE = ' (overwrite)'
         else:
             TEST = True
             OVERWRITE = ' (new)'
         # As there are so many files.. this will only read the new files
         # or will rewrite if CLOBBER is set (if wanting something changed)
         if TEST or CLOBBER:
             # if verbose: output information about the geocenter file
-            logging.info('{0}{1}'.format(os.path.join(output_dir,FILE),OVERWRITE))
+            logging.info(f'{str(output_file)}{OVERWRITE}')
             # open output monthly geocenter file
-            f = open(os.path.join(output_dir,FILE), mode='w', encoding='utf8')
-            args = ('Geocenter time series',DREL,DSET)
+            f = output_file.open(mode='w', encoding='utf8')
+            args = ('Geocenter time series', DREL, DSET)
             print('# {0} from {1} AOD1b {2} Product'.format(*args), file=f)
             print('# {0}'.format(product[DSET]), file=f)
             args = ('ISO-Time','X','Y','Z')
             print('# {0:^15}    {1:^12} {2:^12} {3:^12}'.format(*args), file=f)
 
             # open the AOD1B monthly tar file
-            tar = tarfile.open(name=os.path.join(grace_dir,i), mode='r:gz')
+            tar = tarfile.open(name=str(input_file), mode='r:gz')
 
             # Iterate over every member within the tar file
             for member in tar.getmembers():
+                # track tar file members
+                logging.debug(member.name)
                 # get calendar day from file
                 DD,SFX = fx.findall(member.name).pop()
                 DD = np.int64(DD)
                 # open data file for day
                 if (SFX == '.gz'):
                     fid = gzip.GzipFile(fileobj=tar.extractfile(member))
                 else:
                     fid = tar.extractfile(member)
                 # degree 1 spherical harmonics for day and hours
                 DEG1 = gravtk.geocenter()
                 DEG1.C10 = np.zeros((n_time))
                 DEG1.C11 = np.zeros((n_time))
                 DEG1.S11 = np.zeros((n_time))
-                hours = np.zeros((n_time),dtype=np.int64)
+                hours = np.zeros((n_time), dtype=np.int64)
 
                 # create counter for hour in dataset
                 c = 0
                 # while loop ends when dataset is read
                 while (c < n_time):
                     # read line
                     file_contents = fid.readline().decode('ISO-8859-1')
                     # find file header for data product
                     if bool(hx.search(file_contents)):
+                        # track file header lines
+                        logging.debug(file_contents)
                         # extract hour from header and convert to float
                         HH, = re.findall(r'(\d+):\d+:\d+',file_contents)
                         hours[c] = np.int64(HH)
                         # read each line of spherical harmonics
                         for k in range(0,n_harm):
                             file_contents = fid.readline().decode('ISO-8859-1')
                             # find numerical instances in the data line
@@ -236,30 +241,29 @@
                     print(fstr.format(YY,MM,DD,h,X,Y,Z), file=f)
 
             # close the tar file
             tar.close()
             # close the output file
             f.close()
             # set the permissions mode of the output file
-            os.chmod(os.path.join(output_dir,FILE), MODE)
+            output_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Creates monthly files of geocenter variations
             at 3 or 6-hour intervals
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str, default='',
         help='GRACE/GRACE-FO Data Release')
     # GRACE/GRACE-FO level-1b dealiasing product
     parser.add_argument('--product','-p',
```

### Comparing `gravity-toolkit-1.2.0/scripts/aod1b_oblateness.py` & `gravity-toolkit-1.2.1/scripts/aod1b_oblateness.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,21 @@
 #!/usr/bin/env python
 u"""
 aod1b_oblateness.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Contributions by Hugo Lecomte (03/2021)
 
 Reads GRACE/GRACE-FO level-1b dealiasing data files for a specific product
     atm: atmospheric loading from ECMWF
     ocn: oceanic loading from OMCT
     glo: global atmospheric and oceanic loading
     oba: ocean bottom pressure from OMCT
 
 Creates monthly files of oblateness (C20) variations at 3 or 6 hour intervals
 
-NOTE: this reads the GFZ AOD1B files downloaded from PO.DAAC
-https://podaac-uat.jpl.nasa.gov/drive/files/allData/grace/L1B/GFZ/AOD1B/RL05/
-
 CALLING SEQUENCE:
     python aod1b_oblateness.py --release RL06 atm ocn glo oba
 
 INPUTS:
     GRACE/GRACE-FO AOD1B dataset (atm, ocn, glo, oba)
 
 COMMAND LINE OPTIONS:
@@ -32,14 +29,16 @@
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python (https://numpy.org)
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: debug-level logging of member names and header lines
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
     Updated 10/2021: using python logging for handling verbose output
     Updated 07/2021: can use default argument files to define options
         Add 3-hour interval depending on Release
@@ -57,18 +56,18 @@
         do not extract tar files to temp, extract contents of files to memory
     Updated 05-06/2016: oba=ocean bottom pressure, absolute import of shutil
     Written 05/2016
 """
 from __future__ import print_function, division
 
 import sys
-import os
 import re
 import gzip
 import logging
+import pathlib
 import tarfile
 import argparse
 import numpy as np
 import gravity_toolkit as gravtk
 
 # program module to read the C20 coefficients of the AOD1b data
 def aod1b_oblateness(base_dir,
@@ -109,15 +108,16 @@
     # will extract the data from the file
     regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?'
     rx = re.compile(regex_pattern, re.VERBOSE)
     # output formatting string
     fstr = '{0:4d}-{1:02d}-{2:02d}T{3:02d}:00:00 {4:+16.8E}'
 
     # set number of hours in a file
-    # set the ocean model for a given release
+    # set the atmospheric and ocean model for a given release
+    # set the maximum degree and order of a release
     if DREL in ('RL01','RL02','RL03','RL04','RL05'):
         # for 00, 06, 12 and 18
         n_time = 4
         ATMOSPHERE = 'ECMWF'
         OCEAN_MODEL = 'OMCT'
         LMAX = 100
     elif DREL in ('RL06',):
@@ -135,61 +135,64 @@
     product = {}
     product['atm'] = f'Atmospheric loading from {ATMOSPHERE}'
     product['ocn'] = f'Oceanic loading from {OCEAN_MODEL}'
     product['glo'] = 'Global atmospheric and oceanic loading'
     product['oba'] = f'Ocean bottom pressure from {OCEAN_MODEL}'
 
     # AOD1B directory and output oblateness directory
-    grace_dir = os.path.join(base_dir,'AOD1B',DREL)
-    output_dir = os.path.join(grace_dir,'oblateness')
-    if not os.access(output_dir, os.F_OK):
-        os.mkdir(output_dir, MODE)
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath('AOD1B',DREL)
+    output_dir = grace_dir.joinpath('oblateness')
+    output_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # finding all of the tar files in the AOD1b directory
-    input_tar_files = [tf for tf in os.listdir(grace_dir) if tx.match(tf)]
+    input_tar_files = [tf for tf in grace_dir.iterdir() if tx.match(tf.name)]
 
     # for each tar file
-    for i in sorted(input_tar_files):
+    for input_file in sorted(input_tar_files):
         # extract the year and month from the file
-        YY,MM,SFX = tx.findall(i).pop()
-        YY,MM = np.array([YY,MM], dtype=np.int64)
+        YY,MM,SFX = tx.findall(input_file.name).pop()
+        YY,MM = np.array([YY, MM], dtype=np.int64)
         # output monthly oblateness file
         FILE = f'AOD1B_{DREL}_{DSET}_{YY:4d}_{MM:02d}.txt'
+        output_file = output_dir.joinpath(FILE)
         # if output file exists: check if input tar file is newer
         TEST = False
         OVERWRITE = ' (clobber)'
         # check if output file exists
-        if os.access(os.path.join(output_dir,FILE), os.F_OK):
+        if output_file.exists():
             # check last modification time of input and output files
-            input_mtime = os.stat(os.path.join(grace_dir,i)).st_mtime
-            output_mtime = os.stat(os.path.join(output_dir,FILE)).st_mtime
+            input_mtime = input_file.stat().st_mtime
+            output_mtime = output_file.stat().st_mtime
             # if input tar file is newer: overwrite the output file
             if (input_mtime > output_mtime):
                 TEST = True
                 OVERWRITE = ' (overwrite)'
         else:
             TEST = True
             OVERWRITE = ' (new)'
         # As there are so many files.. this will only read the new files
         # or will rewrite if CLOBBER is set (if wanting something changed)
         if TEST or CLOBBER:
             # if verbose: output information about the oblateness file
-            logging.info('{0}{1}'.format(os.path.join(output_dir,FILE),OVERWRITE))
+            logging.info(f'{str(output_file)}{OVERWRITE}')
             # open output monthly oblateness file
-            f = open(os.path.join(output_dir,FILE), mode='w', encoding='utf8')
+            f = output_file.open(mode='w', encoding='utf8')
             args = ('Oblateness time series',DREL,DSET)
             print('# {0} from {1} AOD1b {2} Product'.format(*args), file=f)
             print('# {0}'.format(product[DSET]), file=f)
             print('# {0:^15}    {1:^15}'.format('ISO-Time','C20'), file=f)
 
             # open the AOD1B monthly tar file
-            tar = tarfile.open(name=os.path.join(grace_dir,i), mode='r:gz')
+            tar = tarfile.open(name=str(input_file), mode='r:gz')
 
             # Iterate over every member within the tar file
             for member in tar.getmembers():
+                # track tar file members
+                logging.debug(member.name)
                 # get calendar day from file
                 DD,SFX = fx.findall(member.name).pop()
                 DD = np.int64(DD)
                 # open datafile for day
                 if (SFX == '.gz'):
                     fid = gzip.GzipFile(fileobj=tar.extractfile(member))
                 else:
@@ -202,14 +205,16 @@
                 c = 0
                 # while loop ends when dataset is read
                 while (c < n_time):
                     # read line
                     file_contents = fid.readline().decode('ISO-8859-1')
                     # find file header for data product
                     if bool(hx.search(file_contents)):
+                        # track file header lines
+                        logging.debug(file_contents)
                         # extract hour from header and convert to float
                         HH, = re.findall(r'(\d+):\d+:\d+',file_contents)
                         hours[c] = np.int64(HH)
                         # read each line of spherical harmonics
                         for k in range(0,n_harm):
                             file_contents = fid.readline().decode('ISO-8859-1')
                             # find numerical instances in the data line
@@ -228,30 +233,29 @@
                     print(fstr.format(YY,MM,DD,hours[h],C20[h]),file=f)
 
             # close the tar file
             tar.close()
             # close the output file
             f.close()
             # set the permissions mode of the output file
-            os.chmod(os.path.join(output_dir,FILE), MODE)
+            output_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Creates monthly files of oblateness (C20)
             variations at 3 or 6-hour intervals
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str, default='',
         help='GRACE/GRACE-FO Data Release')
     # GRACE/GRACE-FO level-1b dealiasing product
     parser.add_argument('--product','-p',
```

### Comparing `gravity-toolkit-1.2.0/scripts/calc_degree_one.py` & `gravity-toolkit-1.2.1/scripts/calc_degree_one.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 calc_degree_one.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates degree 1 variations using GRACE coefficients of degree 2 and greater,
     and ocean bottom pressure variations from ECCO and OMCT/MPIOM
 
 Relation between Geocenter Motion in mm and Normalized Geoid Coefficients
     X = sqrt(3)*a*C11
     Y = sqrt(3)*a*S11
@@ -97,26 +97,34 @@
         ascii
         netCDF4
         HDF5
     --ocean-file X: Index file for ocean model harmonics
     --mean-file X: GRACE/GRACE-FO mean file to remove from the harmonic data
     --mean-format X: Input data format for GRACE/GRACE-FO mean file
     --iterative: Iterate degree one solutions
+    -s X, --solver X: Least squares solver for degree one solutions
+        inv: matrix inversion
+        lstsq: least squares solution
+        gelsy: complete orthogonal factorization
+        gelss: singular value decomposition (SVD)
+        gelsd: singular value decomposition (SVD) with divide and conquer method
     --fingerprint: Redistribute land-water flux using sea level fingerprints
     -e X, --expansion X: Spherical harmonic expansion for sea level fingerprints
     --mask X: Land-sea mask for calculating ocean mass and land water flux
     -p, --plot: Create output plots for components and iterations
     -C, --copy: Copy output files for distribution and archival
     --log: Output log of files created for each job
     -V, --verbose: Verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
+    scipy: Scientific Tools for Python
+        https://scipy.org
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
     matplotlib: Python 2D plotting library
@@ -155,14 +163,16 @@
 
     S Swenson, D Chambers and J Wahr, "Estimating geocenter variations
         from a combination of GRACE and ocean model output,"
         Journal of Geophysical Research: Solid Earth, 113(B08410), (2008).
         https://doi.org/10.1029/2007JB005338
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: add options for least-squares solver
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 02/2023: use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
     Updated 08/2022: set default land-sea mask file in arguments
@@ -238,18 +248,20 @@
 import sys
 import os
 import re
 import time
 import copy
 import shutil
 import logging
+import pathlib
 import argparse
 import warnings
 import traceback
 import numpy as np
+import scipy.linalg
 import gravity_toolkit as gravtk
 
 # attempt imports
 try:
     import matplotlib.pyplot as plt
     import matplotlib.cm as cm
     import matplotlib.offsetbox
@@ -263,15 +275,15 @@
     warnings.filterwarnings("module")
     warnings.warn("netCDF4 not available", ImportWarning)
 # ignore warnings
 warnings.filterwarnings("ignore")
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: import GRACE/GRACE-FO GSM files for a given months range
@@ -362,26 +374,27 @@
     SLR_C40=None,
     SLR_C50=None,
     DATAFORM=None,
     MEAN_FILE=None,
     MEANFORM=None,
     MODEL_INDEX=None,
     ITERATIVE=False,
+    SOLVER=None,
     FINGERPRINT=False,
     EXPANSION=None,
     LANDMASK=None,
     PLOT=False,
     COPY=False,
     MODE=0o775):
 
     # output directory
-    DIRECTORY = os.path.join(base_dir,'geocenter')
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    DIRECTORY = base_dir.joinpath('geocenter')
     # create output directory if non-existent
-    if not os.access(DIRECTORY, os.F_OK):
-        os.makedirs(DIRECTORY, mode=MODE)
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
     # list object of output files for file logs (full path)
     output_files = []
 
     # output flag for using sea level fingerprints
     slf_str = '_SLF' if FINGERPRINT else ''
     # output flag for low-degree harmonic replacements
     if SLR_21 in ('CSR','GFZ','GSFC'):
@@ -432,23 +445,23 @@
     # Read Smoothed Ocean and Land Functions
     # Open the land-sea NetCDF file for reading
     landsea = gravtk.spatial().from_netCDF4(LANDMASK,
         date=False, varname='LSMASK')
     # degree spacing and grid dimensions
     # will create GRACE spatial fields with same dimensions
     dlon,dlat = landsea.spacing
-    nlat,nlon = landsea.shape
+    nlat, nlon = landsea.shape
     # spatial parameters in radians
     dphi = dlon*np.pi/180.0
     dth = dlat*np.pi/180.0
     # longitude and colatitude in radians
     phi = landsea.lon[np.newaxis,:]*np.pi/180.0
     th = (90.0 - np.squeeze(landsea.lat))*np.pi/180.0
     # create land function
-    land_function = np.zeros((nlon,nlat),dtype=np.float64)
+    land_function = np.zeros((nlon, nlat),dtype=np.float64)
     # extract land function from file
     # combine land and island levels for land function
     indx,indy = np.nonzero((landsea.data.T >= 1) & (landsea.data.T <= 3))
     land_function[indx,indy] = 1.0
     # calculate ocean function from land function
     ocean_function = 1.0 - land_function
 
@@ -545,21 +558,17 @@
         ATM_Ylms.mean(apply=True)
     # truncate to degree and order LMAX/MMAX
     ATM_Ylms = ATM_Ylms.truncate(lmax=LMAX, mmax=MMAX)
     # save geocenter coefficients of the atmospheric jump corrections
     atm = gravtk.geocenter().from_harmonics(ATM_Ylms)
 
     # read bottom pressure model if applicable
-    # ECCO_kf080i: https://ecco.jpl.nasa.gov/drive/files/NearRealTime/KalmanFilter/
-    # ECCO_dr080i: https://ecco.jpl.nasa.gov/drive/files/NearRealTime/Smoother/
-    # ECCO_V4r3: https://ecco.jpl.nasa.gov/drive/files/Version4/Release3/interp_monthly/
-    # ECCO_V4r4: https://ecco.jpl.nasa.gov/drive/files/Version4/Release4/interp_monthly/
     if MODEL not in ('OMCT','MPIOM'):
         # read input data files for ascii (txt), netCDF4 (nc) or HDF5 (H5)
-        MODEL_INDEX = os.path.expanduser(MODEL_INDEX)
+        MODEL_INDEX = pathlib.Path(MODEL_INDEX).expanduser().absolute()
         OBP_Ylms = gravtk.harmonics().from_index(MODEL_INDEX,
             format=DATAFORM)
         # reduce to GRACE/GRACE-FO months and truncate to degree and order
         OBP_Ylms = OBP_Ylms.subset(GSM_Ylms.month).truncate(lmax=LMAX,mmax=MMAX)
         # filter ocean bottom pressure coefficients
         if DESTRIPE:
             OBP_Ylms = OBP_Ylms.destripe()
@@ -578,15 +587,15 @@
     ssin = np.sin(np.dot(m,phi))
 
     # Legendre polynomials for degree 1
     P10 = np.squeeze(PLM[1,0,:])
     P11 = np.squeeze(PLM[1,1,:])
     # PLM for spherical harmonic degrees 2+ up to LMAX
     # converted into mass and smoothed if specified
-    plmout = np.zeros((LMAX+1,MMAX+1,nlat))
+    plmout = np.zeros((LMAX+1, MMAX+1, nlat))
     for l in range(1,LMAX+1):
         m = np.arange(0,np.min([l,MMAX])+1)
         # convert to smoothed coefficients of mass
         # Convolving plms with degree dependent factor and smoothing
         plmout[l,m,:] = PLM[l,m,:]*dfactor[l]*wt[l]
 
     # Initializing 3x3 I-Parameter matrix
@@ -653,15 +662,15 @@
         Ylms = GSM_Ylms.index(t)
         Ylms.subtract(GIA_Ylms.index(t))
         Ylms.subtract(ATM_Ylms.index(t))
         for i in range(0, nlat):
             l = np.arange(2,LMAX+1)
             pcos[:,i] = np.sum(plmout[l,:,i]*Ylms.clm[l,:], axis=0)
             psin[:,i] = np.sum(plmout[l,:,i]*Ylms.slm[l,:], axis=0)
-        # Multiplying by c/s(phi#m) to get surface density in cmH2Oeq (lon,lat)
+        # Multiplying by c/s(phi#m) to get surface density in cmwe (lon,lat)
         # ccos/ssin are mXphi, pcos/psin are mXtheta: resultant matrices are phiXtheta
         # The summation over spherical harmonic order is in this multiplication
         rmass = np.dot(np.transpose(ccos),pcos) + np.dot(np.transpose(ssin),psin)
         # calculate G matrix parameters through a summation of each latitude
         for i in range(0,nlat):
             # summation of integration factors, Legendre polynomials,
             # (convolution of order and harmonics) and the ocean mass at t
@@ -765,23 +774,27 @@
                 GAD = np.array([GAD.C10[t], GAD.C11[t], GAD.S11[t]])
                 OBP = np.array([OBP.C10[t], OBP.C11[t], OBP.S11[t]])
                 # effectively adding back OMCT/MPIOM and then removing ECCO
                 CMAT += OBP - GAD
 
             # G Matrix for time t
             GMAT = np.array([G.C10[t], G.C11[t], G.S11[t]])
-            # calculate inversion for degree 1 solutions
+            # calculate degree 1 solution for iteration
             # this is mathematically equivalent to an iterative procedure
             # whereby the initial degree one coefficients are used to update
             # the G Matrix until (C10, C11, S11) converge
             # for OMCT/MPIOM: min(eustatic from land - measured ocean)
             # for ECCO: min((OBP-GAD) + eustatic from land - measured ocean)
-            DMAT[:,t] = np.dot(np.linalg.inv(IMAT), (CMAT-GMAT))
-            # could also use pseudo-inverse in least-squares
-            #DMAT[:,t] = np.linalg.lstsq(IMAT,(CMAT-GMAT),rcond=-1)[0]
+            if (SOLVER == 'inv'):
+                DMAT[:,t] = np.dot(np.linalg.inv(IMAT), (CMAT-GMAT))
+            elif (SOLVER == 'lstsq'):
+                DMAT[:,t] = np.linalg.lstsq(IMAT, (CMAT-GMAT), rcond=-1)[0]
+            elif SOLVER in ('gelsd', 'gelsy', 'gelss'):
+                DMAT[:,t], res, rnk, s = scipy.linalg.lstsq(IMAT, (CMAT-GMAT),
+                    lapack_driver=SOLVER)
             # save geocenter for iteration and time t after restoring GIA+ATM
             iteration.C10[t,n_iter] = DMAT[0,t]/dfactor[1]+gia.C10[t]+atm.C10[t]
             iteration.C11[t,n_iter] = DMAT[1,t]/dfactor[1]+gia.C11[t]+atm.C11[t]
             iteration.S11[t,n_iter] = DMAT[2,t]/dfactor[1]+gia.S11[t]+atm.S11[t]
         # remove mean of each solution for iteration
         iteration.C10[:,n_iter] -= iteration.C10[:,n_iter].mean()
         iteration.C11[:,n_iter] -= iteration.C11[:,n_iter].mean()
@@ -815,16 +828,16 @@
     file_format = '{0}_{1}_{2}{3}{4}{5}{6}{7}{8}.{9}'
     output_format = '{0:11.4f}{1:14.6e}{2:14.6e}{3:14.6e} {4:03d}\n'
     # public file format in fully normalized spherical harmonics
     # before and after restoring the atmospheric and oceanic dealiasing
     for AOD in ['','_wAOD']:
         # local version with all descriptor flags
         a1=(PROC,DREL,MODEL,slf_str,iter_str,slr_str,gia_str,AOD,ds_str,'txt')
-        FILE1 = os.path.join(DIRECTORY,file_format.format(*a1))
-        fid1 = open(FILE1, mode='w', encoding='utf8')
+        FILE1 = DIRECTORY.joinpath(file_format.format(*a1))
+        fid1 = FILE1.open(mode='w', encoding='utf8')
         # print headers for cases with and without dealiasing
         print_header(fid1)
         print_harmonic(fid1,LOVE.kl[1])
         print_global(fid1,PROC,DREL,MODEL.replace('_',' '),AOD,GIA_Ylms_rate,
             SLR_C20,SLR_21,GSM_Ylms.month)
         print_variables(fid1,'single precision','fully normalized')
         # for each GRACE/GRACE-FO month
@@ -836,37 +849,37 @@
                 args=(tdec[t],DEG1.C10[t],DEG1.C11[t],DEG1.S11[t],mon)
             # output geocenter coefficients to file
             fid1.write(output_format.format(*args))
         # close the output file
         fid1.close()
         output_files.append(FILE1)
         # set the permissions mode of the output file
-        os.chmod(FILE1, MODE)
+        FILE1.chmod(mode=MODE)
         # create public and archival copies of data
         if COPY:
             # create symbolic link for public distribution without flags
             a2=(PROC,DREL,MODEL,slf_str,iter_str,'','',AOD,'','txt')
-            FILE2 = os.path.join(DIRECTORY,file_format.format(*a2))
-            os.symlink(FILE1,FILE2) if not os.access(FILE2,os.F_OK) else None
+            FILE2 = DIRECTORY.joinpath(file_format.format(*a2))
+            os.symlink(FILE1,FILE2) if not FILE2.exists() else None
             output_files.append(FILE2)
             # create copy of file with date for archiving
-            today=time.strftime('_%Y-%m-%d',time.localtime())
+            today = time.strftime('_%Y-%m-%d',time.localtime())
             a3=(PROC,DREL,MODEL,slf_str,iter_str,'','',AOD,today,'txt')
-            FILE3 = os.path.join(DIRECTORY,file_format.format(*a3))
+            FILE3 = DIRECTORY.joinpath(file_format.format(*a3))
             shutil.copyfile(FILE1,FILE3)
             # copy modification times and permissions for archive file
             shutil.copystat(FILE1,FILE3)
             output_files.append(FILE3)
 
     # save iterations to netCDF4 file
     if ITERATIVE:
         # output all degree 1 coefficients as a netCDF4 file
         a4=(PROC,DREL,MODEL,slf_str,iter_str,slr_str,gia_str,'',ds_str,'nc')
-        FILE4=os.path.join(DIRECTORY,file_format.format(*a4))
-        fileID=netCDF4.Dataset(FILE4,'w')
+        FILE4 = DIRECTORY.joinpath(file_format.format(*a4))
+        fileID = netCDF4.Dataset(FILE4, mode='w')
         # Defining the NetCDF4 dimensions
         fileID.createDimension('iteration', n_iter)
         fileID.createDimension('time', n_files)
         # defining the NetCDF4 variables
         nc = {}
         nc['time'] = fileID.createVariable('time',GSM_Ylms.time.dtype,('time',))
         nc['month'] = fileID.createVariable('month',GSM_Ylms.month.dtype,('time',))
@@ -895,15 +908,15 @@
         nc['S11'].units = 'fully_normalized'
         nc['S11'].long_name = 'sine_spherical_harmonic_of_degree_1,_order_1'
         # define global attributes
         fileID.date_created = time.strftime('%Y-%m-%d',time.localtime())
         # close the output file
         fileID.close()
         # set the permissions mode of the output file
-        os.chmod(FILE4, MODE)
+        FILE4.chmod(mode=MODE)
         output_files.append(FILE4)
 
     # create plot similar to Figure 1 of Swenson et al (2008)
     if PLOT:
         # 3 row plot (C10, C11 and S11)
         # with ECCO OBP geocenter, OMCT/MPIOM OBP geocenter, eustatic geocenter
         # and the G matrix geocenter components
@@ -958,20 +971,21 @@
             # adjust ticks
             ax[i].get_xaxis().set_tick_params(which='both', direction='in')
             ax[i].get_yaxis().set_tick_params(which='both', direction='in')
         # adjust locations of subplots and save to file
         fig.subplots_adjust(left=0.1,right=0.96,bottom=0.06,top=0.98,hspace=0.1)
         args = (PROC,DREL,MODEL,slf_str,iter_str,slr_str,gia_str,ds_str)
         FILE = 'Swenson_Figure_1_{0}_{1}_{2}{3}{4}{5}{6}{7}.pdf'.format(*args)
-        plt.savefig(os.path.join(DIRECTORY,FILE), format='pdf',
-            metadata={'Title':os.path.basename(sys.argv[0])})
+        PLOT1 = DIRECTORY.joinpath(FILE)
+        plt.savefig(PLOT1, format='pdf',
+            metadata={'Title':pathlib.path(sys.argv[0]).name})
         plt.clf()
         # set the permissions mode of the output files
-        os.chmod(os.path.join(DIRECTORY,FILE), MODE)
-        output_files.append(os.path.join(DIRECTORY,FILE))
+        PLOT.chmod(mode=MODE)
+        output_files.append(PLOT1)
 
     # if ITERATIVE: create plot showing iteration solutions
     if PLOT and ITERATIVE:
         # 3 row plot (C10, C11 and S11)
         ax = {}
         fig, (ax[0], ax[1], ax[2]) = plt.subplots(num=1, nrows=3, sharex=True,
             figsize=(6,9))
@@ -1010,20 +1024,21 @@
             # adjust ticks
             ax[i].get_xaxis().set_tick_params(which='both', direction='in')
             ax[i].get_yaxis().set_tick_params(which='both', direction='in')
         # adjust locations of subplots and save to file
         fig.subplots_adjust(left=0.12,right=0.94,bottom=0.06,top=0.98,hspace=0.1)
         args = (PROC,DREL,MODEL,slf_str,slr_str,gia_str,ds_str)
         FILE = 'Geocenter_Iterative_{0}_{1}_{2}{3}{4}{5}{6}.pdf'.format(*args)
-        plt.savefig(os.path.join(DIRECTORY,FILE), format='pdf',
-            metadata={'Title':os.path.basename(sys.argv[0])})
+        PLOT2 = DIRECTORY.joinpath(FILE)
+        plt.savefig(PLOT2, format='pdf',
+            metadata={'Title':pathlib.path(sys.argv[0]).name})
         plt.clf()
         # set the permissions mode of the output files
-        os.chmod(os.path.join(DIRECTORY,FILE), MODE)
-        output_files.append(os.path.join(DIRECTORY,FILE))
+        PLOT.chmod(mode=MODE)
+        output_files.append(PLOT2)
 
     # return the list of output files and the number of iterations
     return (output_files, n_iter)
 
 # PURPOSE: print YAML header to top of file
 def print_header(fid):
     # print header
@@ -1113,15 +1128,15 @@
     ack.append(('Work was supported by an appointment to the NASA Postdoctoral '
         'Program at NASA Goddard Space Flight Center, administered by '
         'Universities Space Research Association under contract with NASA'))
     ack.append('GRACE is a joint mission of NASA (USA) and DLR (Germany)')
     if (DREL == 'RL06'):
         ack.append('GRACE-FO is a joint mission of NASA (USA) and GFZ (Germany)')
     fid.write('    {0:22}: {1}\n'.format('acknowledgement','.  '.join(ack)))
-    PRODUCT_VERSION = 'Release-{0}'.format(DREL[2:])
+    PRODUCT_VERSION = f'Release-{DREL[2:]}'
     fid.write('    {0:22}: {1}\n'.format('product_version',PRODUCT_VERSION))
     fid.write('    {0:22}:\n'.format('references'))
     reference = []
     # geocenter citations
     reference.append(('T. C. Sutterley, and I. Velicogna, "Improved estimates '
         'of geocenter variability from time-variable gravity and ocean model '
         'outputs", Remote Sensing, 11(18), 2108, (2019). '
@@ -1237,17 +1252,17 @@
     fid.write('\n\n# End of YAML header\n')
 
 # PURPOSE: print a file log for the GRACE degree one analysis
 def output_log_file(input_arguments, output_files, n_iter):
     # format: calc_degree_one_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_degree_one_run_{0}_PID-{1:d}.log'.format(*args)
-    DIRECTORY = os.path.join(input_arguments.directory,'geocenter')
+    DIRECTORY = pathlib.Path(input_arguments.directory).joinpath('geocenter')
     # create a unique log and open the log file
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print number of iterations used in calculation
     if arguments.iterative:
@@ -1260,17 +1275,17 @@
     fid.close()
 
 # PURPOSE: print a error file log for the GRACE degree one analysis
 def output_error_log_file(input_arguments):
     # format: calc_degree_one_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_degree_one_failed_run_{0}_PID-{1:d}.log'.format(*args)
-    DIRECTORY = os.path.join(input_arguments.directory,'geocenter')
+    DIRECTORY = pathlib.Path(input_arguments.directory).joinpath('geocenter')
     # create a unique log and open the log file
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -1288,16 +1303,15 @@
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = \
         gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
@@ -1358,15 +1372,15 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', default='AW13-ICE6G', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # correct for pole tide drift follow Wahr et al. (2015)
     parser.add_argument('--pole-tide',
@@ -1406,39 +1420,44 @@
         help='Ocean model to use')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input data format for ocean models')
     # index file for ocean model harmonics
     parser.add_argument('--ocean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Index file for ocean model harmonics')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # run with iterative scheme
     parser.add_argument('--iterative',
         default=False, action='store_true',
         help='Iterate degree one solutions')
+    # least squares solver
+    choices = ('inv','lstsq','gelsd', 'gelsy', 'gelss')
+    parser.add_argument('--solver','-s',
+        type=str, default='lstsq', choices=choices,
+        help='Least squares solver for degree one solutions')
     # run with sea level fingerprints
     parser.add_argument('--fingerprint',
         default=False, action='store_true',
         help='Redistribute land-water flux using sea level fingerprints')
     parser.add_argument('--expansion','-e',
         type=int, default=240,
         help='Spherical harmonic expansion for sea level fingerprints')
     # land-sea mask for calculating ocean mass and land water flux
     land_mask_file = gravtk.utilities.get_data_path(['data','land_fcn_300km.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         default=land_mask_file,
         help='Land-sea mask for calculating ocean mass and land water flux')
     # create output plots
     parser.add_argument('--plot','-p',
         default=False, action='store_true',
         help='Create output plots for components and iterations')
     # copy output files
@@ -1501,14 +1520,15 @@
             SLR_C40=args.slr_c40,
             SLR_C50=args.slr_c50,
             DATAFORM=args.format,
             MODEL_INDEX=args.ocean_file,
             MEAN_FILE=args.mean_file,
             MEANFORM=args.mean_format,
             ITERATIVE=args.iterative,
+            SOLVER=args.solver,
             FINGERPRINT=args.fingerprint,
             EXPANSION=args.expansion,
             LANDMASK=args.mask,
             PLOT=args.plot,
             COPY=args.copy,
             MODE=args.mode)
     except Exception as exc:
```

### Comparing `gravity-toolkit-1.2.0/scripts/calc_harmonic_resolution.py` & `gravity-toolkit-1.2.1/scripts/calc_harmonic_resolution.py`

 * *Files 1% similar despite different names*

```diff
@@ -32,15 +32,14 @@
     Updated 04/2022: use argparse descriptions within documentation
     Updated 09/2020: using argparse to set parameters
     Updated 10/2019: changing Y/N flags to True/False
     Updated 02/2014: minor update to if statement
     Updated 08/2013: changed SPH_CAP option to (Y/N)
     Written 01/2013
 """
-import sys
 import argparse
 import numpy as np
 
 # PURPOSE: Calculates minimum spatial resolution that can be resolved
 # from spherical harmonics of a maximum degree
 def calc_harmonic_resolution(LMAX, RADIUS=6371.0008, SPH_CAP=False):
     """
```

### Comparing `gravity-toolkit-1.2.0/scripts/calc_mascon.py` & `gravity-toolkit-1.2.1/scripts/calc_mascon.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 calc_mascon.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates a time-series of regional mass anomalies through a least-squares
     mascon procedure from GRACE/GRACE-FO time-variable gravity data
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -D X, --directory X: Working data directory
@@ -89,14 +89,20 @@
         ascii
         netCDF4
         HDF5
     --redistribute-mascons: redistribute mascon mass over the ocean
     --fit-method X: method for fitting sensitivity kernel to harmonics
         1: mass coefficients
         2: geoid coefficients
+    -s X, --solver X: Least squares solver for sensitivity kernels
+        inv: matrix inversion
+        lstsq: least squares solution
+        gelsy: complete orthogonal factorization
+        gelss: singular value decomposition (SVD)
+        gelsd: singular value decomposition (SVD) with divide and conquer method
     --remove-file X: Monthly files to be removed from the GRACE/GRACE-FO data
     --remove-format X: Input data format for files to be removed
         ascii
         netCDF4
         HDF5
         index-ascii
         index-netCDF4
@@ -108,14 +114,16 @@
     -V, --verbose: Verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
+    scipy: Scientific Tools for Python
+        https://scipy.org
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
     future: Compatibility layer between Python 2 and Python 3
@@ -154,14 +162,16 @@
         https://doi.org/10.1029/2009GL039401
 
     J Wahr, S C Swenson, and I Velicogna, "Accuracy of GRACE mass estimates",
         Geophysical Research Letters, 33(6), L06401, (2006).
         https://doi.org/10.1029/2005GL025305
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: add options for least-squares solver
     Updated 02/2023: use love numbers class with additional attributes
     Updated 01/2023: refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
     Updated 04/2022: use wrapper function for reading load Love numbers
         include utf-8 encoding in reads to be windows compliant
@@ -240,22 +250,24 @@
 from __future__ import print_function, division
 
 import sys
 import os
 import re
 import time
 import logging
+import pathlib
 import argparse
-import numpy as np
 import traceback
+import numpy as np
+import scipy.linalg
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: calculate a regional time-series through a least
@@ -285,26 +297,29 @@
     DATAFORM=None,
     MEAN_FILE=None,
     MEANFORM=None,
     MASCON_FILE=None,
     MASCON_FORMAT=None,
     REDISTRIBUTE_MASCONS=False,
     FIT_METHOD=0,
+    SOLVER=None,
     REMOVE_FILES=None,
     REMOVE_FORMAT=None,
     REDISTRIBUTE_REMOVED=False,
     RECONSTRUCT=False,
     RECONSTRUCT_FILE=None,
     LANDMASK=None,
     OUTPUT_DIRECTORY=None,
     MODE=0o775):
 
-    # recursively create output Directory if not currently existing
-    if (not os.access(OUTPUT_DIRECTORY, os.F_OK)):
-        os.makedirs(OUTPUT_DIRECTORY, mode=MODE, exist_ok=True)
+    # directory setup
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    # recursively create output directory if not currently existing
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # list object of output files for file logs (full path)
     output_files = []
 
     # file information
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
     # file parser for reading index files
@@ -375,15 +390,15 @@
         # destriping GRACE/GRACE-FO coefficients
         ds_str = '_FL'
         GRACE_Ylms = GRACE_Ylms.destripe()
     else:
         # using standard GRACE/GRACE-FO harmonics
         ds_str = ''
     # full path to directory for specific GRACE/GRACE-FO product
-    GRACE_Ylms.directory = Ylms['directory']
+    GRACE_Ylms.directory = pathlib.Path(Ylms['directory']).expanduser().absolute()
     # date information of GRACE/GRACE-FO coefficients
     n_files = len(GRACE_Ylms.time)
 
     # input GIA spherical harmonic datafiles
     GIA_Ylms_rate = gravtk.gia(lmax=LMAX).from_GIA(GIA_FILE, GIA=GIA, mmax=MMAX)
     gia_str = f'_{GIA_Ylms_rate.title}' if GIA else ''
     # monthly GIA calculated by gia_rate*time elapsed
@@ -438,15 +453,16 @@
 
     # input reconstructed spherical harmonic datafiles
     construct_Ylms = GRACE_Ylms.zeros_like()
     construct_Ylms.time[:] = np.copy(GRACE_Ylms.time)
     construct_Ylms.month[:] = np.copy(GRACE_Ylms.month)
     if RECONSTRUCT:
         # input index for reconstructed spherical harmonic datafiles
-        with open(RECONSTRUCT_FILE, mode='r', encoding='utf8') as f:
+        RECONSTRUCT_FILE = pathlib.Path(RECONSTRUCT_FILE).expanduser().absolute()
+        with RECONSTRUCT_FILE.open(mode='r', encoding='utf8') as f:
             file_list = [l for l in f.read().splitlines() if parser.match(l)]
         # for each valid file in the index (iterate over mascons)
         for reconstruct_file in file_list:
             # read reconstructed spherical harmonics
             Ylms = gravtk.harmonics().from_file(reconstruct_file,
                 format=DATAFORM)
             # truncate clm and slm matrices to LMAX/MMAX
@@ -458,27 +474,28 @@
         # set flag for removing reconstructed coefficients
         construct_str = '_LEAKAGE'
     else:
         # set flag for not removing the reconstructed coefficients
         construct_str = ''
 
     # input mascon spherical harmonic datafiles
-    with open(MASCON_FILE, mode='r', encoding='utf8') as f:
+    MASCON_FILE = pathlib.Path(MASCON_FILE).expanduser().absolute()
+    with MASCON_FILE.open(mode='r', encoding='utf8') as f:
         mascon_files = [l for l in f.read().splitlines() if parser.match(l)]
     # number of mascons
     n_mas = len(mascon_files)
     # spatial area of the mascon
     total_area = np.zeros((n_mas))
     # name of each mascon
     mascon_name = []
     # for each valid file in the index (iterate over mascons)
     mascon_list = []
     for k,fi in enumerate(mascon_files):
         # read mascon spherical harmonics
-        Ylms = gravtk.harmonics().from_file(os.path.expanduser(fi),
+        Ylms = gravtk.harmonics().from_file(fi,
             format=MASCON_FORMAT, date=False)
         # Calculating the total mass of each mascon (1 cmwe uniform)
         total_area[k] = 4.0*np.pi*(rad_e**3)*rho_e*Ylms.clm[0,0]/3.0
         # distribute mascon mass uniformly over the ocean
         if REDISTRIBUTE_MASCONS:
             # calculate ratio between total mascon mass and
             # a uniformly distributed cm of water over the ocean
@@ -488,43 +505,41 @@
                 for l in range(m,LMAX+1):# LMAX+1 to include LMAX
                     # remove ratio*ocean Ylms from mascon Ylms
                     # note: x -= y is equivalent to x = x - y
                     Ylms.clm[l,m] -= ratio*ocean_Ylms.clm[l,m]
                     Ylms.slm[l,m] -= ratio*ocean_Ylms.slm[l,m]
         # truncate mascon spherical harmonics to d/o LMAX/MMAX and add to list
         mascon_list.append(Ylms.truncate(lmax=LMAX, mmax=MMAX))
-        # mascon base is the file without directory or suffix
-        mascon_base = os.path.basename(mascon_files[k])
-        mascon_base = os.path.splitext(mascon_base)[0]
-        # if lower case, will capitalize
-        mascon_base = mascon_base.upper()
-        # if mascon name contains degree and order info, remove
-        mascon_name.append(mascon_base.replace(f'_L{LMAX:d}', ''))
+        # stem is the mascon file without directory or suffix
+        # if lower case: will capitalize
+        # if mascon name contains degree and order info: scrub from string
+        stem = re.sub(r'_L(\d+)(M\d+)?', r'', Ylms.filename.stem.upper())
+        mascon_name.append(stem)
     # create single harmonics object from list
     mascon_Ylms = gravtk.harmonics().from_list(mascon_list, date=False)
     # clear mascon list variable
     del mascon_list
 
     # calculating GRACE/GRACE-FO error (Wahr et al. 2006)
     # output GRACE error file (for both LMAX==MMAX and LMAX != MMAX cases)
     fargs = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GRACE_Ylms.month[0],
         GRACE_Ylms.month[-1], suffix[DATAFORM])
     delta_format = '{0}_{1}_{2}_DELTA_CLM_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
-    DELTA_FILE = os.path.join(GRACE_Ylms.directory,delta_format.format(*fargs))
+    DELTA_FILE = GRACE_Ylms.directory.joinpath(delta_format.format(*fargs))
     # check full path of the GRACE directory for delta file
     # if file was previously calculated: will read file
     # else: will calculate the GRACE/GRACE-FO error
-    if not os.access(DELTA_FILE, os.F_OK):
+    if not DELTA_FILE.exists():
         # add output delta file to list object
         output_files.append(DELTA_FILE)
 
         # Delta coefficients of GRACE time series (Error components)
-        delta_Ylms = gravtk.harmonics(lmax=LMAX,mmax=MMAX)
-        delta_Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-        delta_Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+        delta_Ylms = gravtk.harmonics(lmax=LMAX, mmax=MMAX)
+        delta_Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+        delta_Ylms.slm = np.zeros((LMAX+1, MMAX+1))
         # Smoothing Half-Width (CNES is a 10-day solution)
         # All other solutions are monthly solutions (HFWTH for annual = 6)
         if ((PROC == 'CNES') and (DREL in ('RL01','RL02'))):
             HFWTH = 19
         else:
             HFWTH = 6
         # Equal to the noise of the smoothed time-series
@@ -546,21 +561,21 @@
                     # variance of data-(smoothed+annual+semi)
                     val2 = getattr(delta_Ylms, csharm)
                     val2[l,m] = np.sqrt(np.sum(smth['noise']**2)/nsmth)
 
         # attributes for output files
         attributes = {}
         attributes['title'] = 'GRACE/GRACE-FO Spherical Harmonic Errors'
-        attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+        attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
         # save GRACE/GRACE-FO delta harmonics to file
         delta_Ylms.time = np.copy(tsmth)
         delta_Ylms.month = np.int64(nsmth)
         delta_Ylms.to_file(DELTA_FILE, format=DATAFORM, **attributes)
         # set the permissions mode of the output harmonics file
-        os.chmod(DELTA_FILE, MODE)
+        DELTA_FILE.chmod(mode=MODE)
         # append delta harmonics file to output files list
         output_files.append(DELTA_FILE)
     else:
         # read GRACE/GRACE-FO delta harmonics from file
         delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE,
             format=DATAFORM)
         # truncate GRACE/GRACE-FO delta clm and slm to d/o LMAX/MMAX
@@ -570,40 +585,37 @@
 
     # Calculating the number of cos and sin harmonics between LMIN and LMAX
     # taking into account MMAX (if MMAX == LMAX then LMAX-MMAX=0)
     n_harm=np.int64(LMAX**2 - LMIN**2 + 2*LMAX + 1 - (LMAX-MMAX)**2 - (LMAX-MMAX))
 
     # Initialing harmonics for least squares fitting
     # mascon kernel
-    M_lm = np.zeros((n_harm,n_mas))
+    M_lm = np.zeros((n_harm, n_mas))
     # mascon kernel converted to output unit
-    MA_lm = np.zeros((n_harm,n_mas))
+    MA_lm = np.zeros((n_harm, n_mas))
     # corrected clm and slm
-    Y_lm = np.zeros((n_harm,n_files))
+    Y_lm = np.zeros((n_harm, n_files))
     # sensitivity kernel
-    A_lm = np.zeros((n_harm,n_mas))
+    A_lm = np.zeros((n_harm, n_mas))
     # Satellite error harmonics
     delta_lm = np.zeros((n_harm))
     # Initializing output Mascon time-series
-    mascon = np.zeros((n_mas,n_files))
+    mascon = np.zeros((n_mas, n_files))
     # Mascon satellite error component
     M_delta = np.zeros((n_mas))
     # Initializing conversion factors
     # factor for converting to coefficients of mass
     fact = np.zeros((n_harm))
     # smoothing factor
     wt_lm = np.zeros((n_harm))
 
     # ii is a counter variable for building the mascon column array
     ii = 0
     # Creating column array of clm/slm coefficients
     # Order is [C00...C6060,S11...S6060]
-    # Calculating factor to convert geoid spherical harmonic coefficients
-    # to coefficients of mass (Wahr, 1998)
-    coeff = rho_e*rad_e/3.0
     # Switching between Cosine and Sine Stokes
     for cs,csharm in enumerate(['clm','slm']):
         # copy cosine and sin harmonics
         mascon_harm = getattr(mascon_Ylms, csharm)
         grace_harm = getattr(GRACE_Ylms, csharm)
         GIA_harm = getattr(GIA_Ylms, csharm)
         remove_harm = getattr(remove_Ylms, csharm)
@@ -635,30 +647,36 @@
     # Converting mascon coefficients to fit method
     if (FIT_METHOD == 1):
         # Fitting Sensitivity Kernel as mass coefficients
         # converting M_lm to mass coefficients of the kernel
         for i in range(n_harm):
             MA_lm[i,:] = M_lm[i,:]*wt_lm[i]*fact[i]
         fit_factor = wt_lm*fact
-    else:
+    elif (FIT_METHOD == 2):
         # Fitting Sensitivity Kernel as geoid coefficients
         for i in range(n_harm):
             MA_lm[:,:] = M_lm[i,:]*wt_lm[i]
         fit_factor = wt_lm*np.ones((n_harm))
 
     # Fitting the sensitivity kernel from the input kernel
     for i in range(n_harm):
         # setting kern_i equal to 1 for d/o
         kern_i = np.zeros((n_harm))
         # converting to mass coefficients if specified
         kern_i[i] = 1.0*fit_factor[i]
         # spherical harmonics solution for the
         # mascon sensitivity kernels
-        # Least Squares Solutions: Inv(X'.X).(X'.Y)
-        kern_lm = np.linalg.lstsq(MA_lm,kern_i,rcond=-1)[0]
+        if (SOLVER == 'inv'):
+            kern_lm = np.dot(np.linalg.inv(MA_lm), kern_i)
+        elif (SOLVER == 'lstsq'):
+            kern_lm = np.linalg.lstsq(MA_lm, kern_i, rcond=-1)[0]
+        elif SOLVER in ('gelsd', 'gelsy', 'gelss'):
+            kern_lm, res, rnk, s = scipy.linalg.lstsq(MA_lm, kern_i,
+                lapack_driver=SOLVER)
+        # calculate the sensitivity kernel for each mascon
         for k in range(n_mas):
             A_lm[i,k] = kern_lm[k]*total_area[k]
 
     # for each mascon
     for k in range(n_mas):
         # Multiply the Satellite error (noise of a smoothed time-series
         # with annual and semi-annual components) by the sensitivity kernel
@@ -668,50 +686,50 @@
         # output filename format (for both LMAX==MMAX and LMAX != MMAX cases):
         # mascon name, GRACE dataset, GIA model, LMAX, (MMAX,)
         # Gaussian smoothing, filter flag, remove reconstructed fields flag
         # output GRACE error file
         fargs = (mascon_name[k], dset_str, gia_str.upper(), atm_str, ocean_str,
             LMAX, order_str, gw_str, ds_str, construct_str)
         file_format = '{0}{1}{2}{3}{4}_L{5:d}{6}{7}{8}{9}.txt'
-        output_file = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
+        output_file = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
 
         # Output mascon datafiles
         # Will output each mascon time series
         # month, date, mascon mass [Gt], satellite error [Gt], mascon area [km^2]
         # open output mascon time-series file
-        fid = open(output_file, mode='w', encoding='utf8')
+        fid = output_file.open(mode='w', encoding='utf8')
         # for each date
         formatting_string = '{0:03d} {1:12.4f} {2:16.10f} {3:16.10f} {4:16.5f}'
         for t,mon in enumerate(GRACE_Ylms.month):
             # Summing over all spherical harmonics for mascon k, and time t
             # multiplies by the degree dependent factor to convert
             # the harmonics into mass coefficients
             # Converting mascon mass time-series from g to gigatonnes
             mascon[k,t] = np.sum(A_lm[:,k]*Y_lm[:,t])/1e15
             # output to file
             args=(mon,GRACE_Ylms.time[t],mascon[k,t],M_delta[k],total_area[k]/1e10)
             print(formatting_string.format(*args), file=fid)
         # close the output file
         fid.close()
         # change the permissions mode
-        os.chmod(output_file, MODE)
+        output_file.chmod(mode=MODE)
         # add output files to list object
         output_files.append(output_file)
 
     # return the list of output files
     return output_files
 
 # PURPOSE: print a file log for the GRACE mascon analysis
 def output_log_file(input_arguments, output_files):
     # format: calc_mascon_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_mascon_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
@@ -722,16 +740,16 @@
 
 # PURPOSE: print a error file log for the GRACE mascon analysis
 def output_error_log_file(input_arguments):
     # format: calc_mascon_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_mascon_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -748,20 +766,18 @@
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for mascon files')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
@@ -832,15 +848,15 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # correct for pole tide drift follow Wahr et al. (2015)
     parser.add_argument('--pole-tide',
@@ -858,15 +874,15 @@
     # GFZ: GRACE/GRACE-FO coefficients from GFZ GravIS
     #     http://gravis.gfz-potsdam.de/corrections
     parser.add_argument('--geocenter',
         metavar='DEG1', type=str,
         choices=['Tellus','SLR','SLF','UCI','Swenson','GFZ'],
         help='Update Degree 1 coefficients with SLR or derived values')
     parser.add_argument('--geocenter-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Specific geocenter file if not default')
     parser.add_argument('--interpolate-geocenter',
         default=False, action='store_true',
         help='Least-squares model missing Degree 1 coefficients')
     # replace low degree harmonics with values from Satellite Laser Ranging
     parser.add_argument('--slr-c20',
         type=str, default=None, choices=['CSR','GFZ','GSFC'],
@@ -888,38 +904,43 @@
         help='Replace C50 coefficients with SLR values')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input data format for auxiliary files')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # mascon index file and parameters
     parser.add_argument('--mascon-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Index file of mascons spherical harmonics')
     parser.add_argument('--mascon-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input data format for mascon files')
     parser.add_argument('--redistribute-mascons',
         default=False, action='store_true',
         help='Redistribute mascon mass over the ocean')
     # 1: mass coefficients
     # 2: geoid coefficients
     parser.add_argument('--fit-method',
         type=int, default=1, choices=(1,2),
         help='Method for fitting sensitivity kernel to harmonics')
+    # least squares solver
+    choices = ('inv','lstsq','gelsd', 'gelsy', 'gelss')
+    parser.add_argument('--solver','-s',
+        type=str, default='lstsq', choices=choices,
+        help='Least squares solver for sensitivity kernel solutions')
     # monthly files to be removed from the GRACE/GRACE-FO data
     parser.add_argument('--remove-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='+',
+        type=pathlib.Path, nargs='+',
         help='Monthly files to be removed from the GRACE/GRACE-FO data')
     choices = []
     choices.extend(['ascii','netCDF4','HDF5'])
     choices.extend(['index-ascii','index-netCDF4','index-HDF5'])
     parser.add_argument('--remove-format',
         type=str, nargs='+', choices=choices,
         help='Input data format for files to be removed')
@@ -927,20 +948,20 @@
         default=False, action='store_true',
         help='Redistribute removed mass fields over the ocean')
     # mascon reconstruct parameters
     parser.add_argument('--remove-reconstruct',
         default=False, action='store_true',
         help='Remove reconstructed mascon time series fields')
     parser.add_argument('--reconstruct-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Reconstructed mascon time series file to be removed')
     # land-sea mask for redistributing mascon mass and land water flux
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing mascon mass and land water flux')
     # Output log file for each job in forms
     # calc_mascon_run_2002-04-01_PID-00000.log
     # calc_mascon_failed_run_2002-04-01_PID-00000.log
     parser.add_argument('--log',
         default=False, action='store_true',
         help='Output log file for each job')
@@ -1000,14 +1021,15 @@
             DATAFORM=args.format,
             MEAN_FILE=args.mean_file,
             MEANFORM=args.mean_format,
             MASCON_FILE=args.mascon_file,
             MASCON_FORMAT=args.mascon_format,
             REDISTRIBUTE_MASCONS=args.redistribute_mascons,
             FIT_METHOD=args.fit_method,
+            SOLVER=args.solver,
             REMOVE_FILES=args.remove_file,
             REMOVE_FORMAT=args.remove_format,
             REDISTRIBUTE_REMOVED=args.redistribute_removed,
             RECONSTRUCT=args.remove_reconstruct,
             RECONSTRUCT_FILE=args.reconstruct_file,
             LANDMASK=args.mask,
             OUTPUT_DIRECTORY=args.output_directory,
```

### Comparing `gravity-toolkit-1.2.0/scripts/calc_sensitivity_kernel.py` & `gravity-toolkit-1.2.1/scripts/calc_sensitivity_kernel.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 calc_sensitivity_kernel.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates spatial sensitivity kernels through a least-squares mascon procedure
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -O X, --output-directory X: output directory for mascon files
     --lmin X: minimum spherical harmonic degree
@@ -28,29 +28,37 @@
         HDF5
     --mask X: Land-sea mask for redistributing mascon mass and land water flux
     --mascon-file X: index file of mascons spherical harmonics
     --redistribute-mascons: redistribute mascon mass over the ocean
     --fit-method X: method for fitting sensitivity kernel to harmonics
         1: mass coefficients
         2: geoid coefficients
-    -s, --spatial: Output spatial grid file for each mascon
+    -s X, --solver X: Least squares solver for sensitivity kernels
+        inv: matrix inversion
+        lstsq: least squares solution
+        gelsy: complete orthogonal factorization
+        gelss: singular value decomposition (SVD)
+        gelsd: singular value decomposition (SVD) with divide and conquer method
+    -o, --spatial: Output spatial grid file for each mascon
     -S X, --spacing X: spatial resolution of output data (dlon,dlat)
     -I X, --interval X: Output grid interval
         1: global
         2: centered global
         3: non-global
     -B X, --bounds X: non-global grid bounding box (minlon,maxlon,minlat,maxlat)
     --log: Output log of files created for each job
     -V, --verbose: Verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
+    scipy: Scientific Tools for Python
+        https://scipy.org
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
     future: Compatibility layer between Python 2 and Python 3
         https://python-future.org/
 
@@ -81,14 +89,16 @@
 
     V M Tiwari, J Wahr, S and Swenson, "Dwindling groundwater resources in
         northern India, from satellite gravity observations",
         Geophysical Research Letters, 36(18), L18401, (2009).
         https://doi.org/10.1029/2009GL039401
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: add options for least-squares solver
     Updated 02/2023: use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 07/2022: create mask for output gridded variables
         made creating the spatial outputs optional to improve compute time
     Updated 04/2022: use wrapper function for reading load Love numbers
@@ -138,22 +148,24 @@
 from __future__ import print_function, division
 
 import sys
 import os
 import re
 import time
 import logging
+import pathlib
 import argparse
-import numpy as np
 import traceback
+import numpy as np
+import scipy.linalg
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: calculate a regional time-series through a least
@@ -163,14 +175,15 @@
     MMAX=None,
     LOVE_NUMBERS=0,
     REFERENCE=None,
     DATAFORM=None,
     MASCON_FILE=None,
     REDISTRIBUTE_MASCONS=False,
     FIT_METHOD=0,
+    SOLVER=None,
     LANDMASK=None,
     SPATIAL=False,
     DDEG=None,
     INTERVAL=None,
     BOUNDS=None,
     OUTPUT_DIRECTORY=None,
     MODE=0o775):
@@ -178,17 +191,17 @@
     # file information
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')[DATAFORM]
     # file parser for reading index files
     # removes commented lines (can comment out files in the index)
     # removes empty lines (if there are extra empty lines)
     parser = re.compile(r'^(?!\#|\%|$)', re.VERBOSE)
 
-    # Create output Directory if not currently existing
-    if (not os.access(OUTPUT_DIRECTORY,os.F_OK)):
-        os.mkdir(OUTPUT_DIRECTORY)
+    # recursively create output directory if not currently existing
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # list object of output files for file logs (full path)
     output_files = []
 
     # read arrays of kl, hl, and ll Love Numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
         REFERENCE=REFERENCE, FORMAT='class')
@@ -220,27 +233,28 @@
             MMAX=MMAX, LOVE=LOVE)
         ocean_str = '_OCN'
     else:
         # not distributing uniformly over ocean
         ocean_str = ''
 
     # input mascon spherical harmonic datafiles
-    with open(MASCON_FILE, mode='r', encoding='utf8') as f:
+    MASCON_FILE = pathlib.Path(MASCON_FILE).expanduser().absolute()
+    with MASCON_FILE.open(mode='r', encoding='utf8') as f:
         mascon_files = [l for l in f.read().splitlines() if parser.match(l)]
     # number of mascons
     n_mas = len(mascon_files)
     # spatial area of the mascon
     total_area = np.zeros((n_mas))
     # name of each mascon
     mascon_name = []
     # for each valid file in the index (iterate over mascons)
     mascon_list = []
     for k,fi in enumerate(mascon_files):
         # read mascon spherical harmonics
-        Ylms = gravtk.harmonics().from_file(os.path.expanduser(fi),
+        Ylms = gravtk.harmonics().from_file(fi,
             format=DATAFORM, date=False)
         # Calculating the total mass of each mascon (1 cmwe uniform)
         total_area[k] = 4.0*np.pi*(rad_e**3)*rho_e*Ylms.clm[0,0]/3.0
         # distribute mascon mass uniformly over the ocean
         if REDISTRIBUTE_MASCONS:
             # calculate ratio between total mascon mass and
             # a uniformly distributed cm of water over the ocean
@@ -250,53 +264,47 @@
                 for l in range(m,LMAX+1):# LMAX+1 to include LMAX
                     # remove ratio*ocean Ylms from mascon Ylms
                     # note: x -= y is equivalent to x = x - y
                     Ylms.clm[l,m] -= ratio*ocean_Ylms.clm[l,m]
                     Ylms.slm[l,m] -= ratio*ocean_Ylms.slm[l,m]
         # truncate mascon spherical harmonics to d/o LMAX/MMAX and add to list
         mascon_list.append(Ylms.truncate(lmax=LMAX, mmax=MMAX))
-        # mascon base is the file without directory or suffix
-        mascon_base = os.path.basename(mascon_files[k])
-        mascon_base = os.path.splitext(mascon_base)[0]
-        # if lower case, will capitalize
-        mascon_base = mascon_base.upper()
-        # if mascon name contains degree and order info, remove
-        mascon_name.append(mascon_base.replace(f'_L{LMAX:d}', ''))
+        # stem is the mascon file without directory or suffix
+        # if lower case: will capitalize
+        # if mascon name contains degree and order info: scrub from string
+        stem = re.sub(r'_L(\d+)(M\d+)?', r'', Ylms.filename.stem.upper())
+        mascon_name.append(stem)
     # create single harmonics object from list
     mascon_Ylms = gravtk.harmonics().from_list(mascon_list, date=False)
     # clear mascon list variable
     del mascon_list
 
     # Calculating the number of cos and sin harmonics between LMIN and LMAX
     # taking into account MMAX (if MMAX == LMAX then LMAX-MMAX=0)
     n_harm=np.int64(LMAX**2 - LMIN**2 + 2*LMAX + 1 - (LMAX-MMAX)**2 - (LMAX-MMAX))
 
     # Initialing harmonics for least squares fitting
     # mascon kernel
-    M_lm = np.zeros((n_harm,n_mas))
+    M_lm = np.zeros((n_harm, n_mas))
     # mascon kernel converted to output unit
-    MA_lm = np.zeros((n_harm,n_mas))
+    MA_lm = np.zeros((n_harm, n_mas))
     # sensitivity kernel
-    A_lm = np.zeros((n_harm,n_mas))
+    A_lm = np.zeros((n_harm, n_mas))
     # Initializing conversion factors
     # factor for converting to smoothed coefficients of mass
     fact = np.zeros((n_harm))
     # factor for converting back into geoid coefficients
     fact_inv = np.zeros((n_harm))
     # smoothing factor
     wt_lm = np.zeros((n_harm))
 
     # ii is a counter variable for building the mascon column array
     ii = 0
     # Creating column array of clm/slm coefficients
     # Order is [C00...C6060,S11...S6060]
-    # Calculating factor to convert geoid spherical harmonic coefficients
-    # to coefficients of mass (Wahr, 1998)
-    coeff = rho_e*rad_e/3.0
-    coeff_inv = 0.75/(np.pi*rho_e*rad_e**3)
     # Switching between Cosine and Sine Stokes
     for cs,csharm in enumerate(['clm','slm']):
         # copy cosine and sin harmonics
         mascon_harm = getattr(mascon_Ylms, csharm)
         # for each spherical harmonic degree
         # +1 to include LMAX
         for l in range(LMIN,LMAX+1):
@@ -306,57 +314,64 @@
             # +1 to include l or MMAX (whichever is smaller)
             for m in range(cs,mm+1):
                 # Mascon Spherical Harmonics
                 M_lm[ii,:] = np.copy(mascon_harm[l,m,:])
                 # degree dependent factor to convert to mass
                 fact[ii] = (2.0*l + 1.0)/(1.0 + LOVE.kl[l])
                 # degree dependent factor to convert from mass
+                coeff_inv = 0.75/(np.pi*rho_e*rad_e**3)
                 fact_inv[ii] = coeff_inv*(1.0 + LOVE.kl[l])/(2.0*l + 1.0)
                 # degree dependent smoothing
                 wt_lm[ii] = np.copy(wt[l])
                 # add 1 to counter
                 ii += 1
 
     # Converting mascon coefficients to fit method
     if (FIT_METHOD == 1):
         # Fitting Sensitivity Kernel as mass coefficients
         # converting M_lm to mass coefficients of the kernel
         for i in range(n_harm):
             MA_lm[i,:] = M_lm[i,:]*wt_lm[i]*fact[i]
         fit_factor = wt_lm*fact
         inv_fit_factor = np.copy(fact_inv)
-    else:
+    elif (FIT_METHOD == 2):
         # Fitting Sensitivity Kernel as geoid coefficients
         for i in range(n_harm):
             MA_lm[:,:] = M_lm[i,:]*wt_lm[i]
         fit_factor = wt_lm*np.ones((n_harm))
         inv_fit_factor = np.ones((n_harm))
 
     # Fitting the sensitivity kernel from the input kernel
     for i in range(n_harm):
         # setting kern_i equal to 1 for d/o
         kern_i = np.zeros((n_harm))
         # converting to mass coefficients if specified
         kern_i[i] = 1.0*fit_factor[i]
         # spherical harmonics solution for the
         # mascon sensitivity kernels
-        # Least Squares Solutions: Inv(X'.X).(X'.Y)
-        kern_lm = np.linalg.lstsq(MA_lm, kern_i, rcond=-1)[0]
+        if (SOLVER == 'inv'):
+            kern_lm = np.dot(np.linalg.inv(MA_lm), kern_i)
+        elif (SOLVER == 'lstsq'):
+            kern_lm = np.linalg.lstsq(MA_lm, kern_i, rcond=-1)[0]
+        elif SOLVER in ('gelsd', 'gelsy', 'gelss'):
+            kern_lm, res, rnk, s = scipy.linalg.lstsq(MA_lm, kern_i,
+                lapack_driver=SOLVER)
+        # calculate the sensitivity kernel for each mascon
         for k in range(n_mas):
             A_lm[i,k] = kern_lm[k]*total_area[k]
     # free up larger variables
     del M_lm, MA_lm, wt_lm, fact, fact_inv, fit_factor
 
     # reshaping harmonics of sensitivity kernel to LMAX+1,MMAX+1
     # calculating the spatial sensitivity kernel of each mascon
     # kernel calculated as outlined in Tiwari (2009) and Jacobs (2012)
     # Initializing output sensitivity kernel (both spatial and Ylms)
     kern_Ylms = gravtk.harmonics(lmax=LMAX, mmax=MMAX)
-    kern_Ylms.clm = np.zeros((LMAX+1,MMAX+1,n_mas))
-    kern_Ylms.slm = np.zeros((LMAX+1,MMAX+1,n_mas))
+    kern_Ylms.clm = np.zeros((LMAX+1, MMAX+1, n_mas))
+    kern_Ylms.slm = np.zeros((LMAX+1, MMAX+1, n_mas))
     kern_Ylms.time = np.copy(total_area)
     # counter variable for deconstructing the mascon column arrays
     ii = 0
     # Switching between Cosine and Sine Stokes
     for cs,csharm in enumerate(['clm','slm']):
         # for each spherical harmonic degree
         # +1 to include LMAX
@@ -372,34 +387,35 @@
                 # add 1 to counter
                 ii += 1
     # free up larger variables
     del A_lm, inv_fit_factor
 
     # attributes for output files
     attributes = {}
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
     # for each mascon
     for k in range(n_mas):
         # get harmonics for mascon
         Ylms = kern_Ylms.index(k, date=False)
         # output sensitivity kernel to file
         args = (mascon_name[k],ocean_str,LMAX,order_str,gw_str,suffix)
         FILE1 = '{0}_SKERNEL_CLM{1}_L{2:d}{3}{4}.{5}'.format(*args)
-        Ylms.to_file(os.path.join(OUTPUT_DIRECTORY, FILE1),
-            format=DATAFORM, date=False, **attributes)
+        output_file = OUTPUT_DIRECTORY.joinpath(FILE1)
+        Ylms.to_file(output_file, format=DATAFORM,
+            date=False, **attributes)
         # change the permissions mode
-        os.chmod(os.path.join(OUTPUT_DIRECTORY,FILE1),MODE)
+        output_file.chmod(mode=MODE)
         # add output files to list object
-        output_files.append(os.path.join(OUTPUT_DIRECTORY,FILE1))
+        output_files.append(output_file)
 
     # attributes for output files
     attributes = {}
     attributes['units'] = 'unitless'
     attributes['longname'] = 'Sensitivity_Kernel'
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
     # if outputting spatial grids
     if SPATIAL:
         # Output spatial data object
         grid = gravtk.spatial()
         # Output Degree Spacing
         dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
         # Output Degree Interval
@@ -414,18 +430,18 @@
             grid.lon = np.arange(-180+dlon/2.0,180+dlon/2.0,dlon)
             grid.lat = np.arange(90.0-dlat/2.0,-90.0-dlat/2.0,-dlat)
             n_lon = len(grid.lon)
             n_lat = len(grid.lat)
         elif (INTERVAL == 3):
             # non-global grid set with BOUNDS parameter
             minlon,maxlon,minlat,maxlat = BOUNDS.copy()
-            grid.lon = np.arange(minlon+dlon/2.0,maxlon+dlon/2.0,dlon)
-            grid.lat = np.arange(maxlat-dlat/2.0,minlat-dlat/2.0,-dlat)
-            nlon = len(grid.lon)
-            nlat = len(grid.lat)
+            grid.lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+            grid.lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
+            n_lon = len(grid.lon)
+            n_lat = len(grid.lat)
 
         # Computing plms for converting to spatial domain
         theta = (90.0-grid.lat)*np.pi/180.0
         PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(theta))
 
         # for each mascon
         for k in range(n_mas):
@@ -434,32 +450,33 @@
             # convert spherical harmonics to output spatial grid
             grid.data = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
                 grid.lon, grid.lat, LMAX=LMAX, MMAX=MMAX, PLM=PLM).T
             grid.mask = np.zeros_like(grid.data, dtype=bool)
             # output sensitivity kernel to file
             args = (mascon_name[k],ocean_str,LMAX,order_str,gw_str,suffix)
             FILE2 = '{0}_SKERNEL{1}_L{2:d}{3}{4}.{5}'.format(*args)
-            grid.to_file(os.path.join(OUTPUT_DIRECTORY,FILE2),
-                format=DATAFORM, date=False, **attributes)
+            output_file = OUTPUT_DIRECTORY.joinpath(FILE2)
+            grid.to_file(output_file, format=DATAFORM,
+                date=False, **attributes)
             # change the permissions mode
-            os.chmod(os.path.join(OUTPUT_DIRECTORY,FILE2),MODE)
+            output_file.chmod(mode=MODE)
             # add output files to list object
-            output_files.append(os.path.join(OUTPUT_DIRECTORY,FILE2))
+            output_files.append(output_file)
 
     # return the list of output files
     return output_files
 
 # PURPOSE: print a file log for the mascon sensitivity kernel analysis
 def output_log_file(input_arguments, output_files):
     # format: calc_skernel_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_skernel_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
@@ -470,16 +487,16 @@
 
 # PURPOSE: print a error file log for the mascon sensitivity kernel analysis
 def output_error_log_file(input_arguments):
     # format: calc_skernel_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'calc_skernel_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -494,16 +511,15 @@
             least-squares mascon procedure
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for mascon files')
     # minimum spherical harmonic degree
     parser.add_argument('--lmin',
         type=int, default=1,
         help='Minimum spherical harmonic degree')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
@@ -532,31 +548,36 @@
         help='Gaussian smoothing radius (km)')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input data format for auxiliary files')
     # mascon index file and parameters
     parser.add_argument('--mascon-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Index file of mascons spherical harmonics')
     parser.add_argument('--redistribute-mascons',
         default=False, action='store_true',
         help='Redistribute mascon mass over the ocean')
     # 1: mass coefficients
     # 2: geoid coefficients
     parser.add_argument('--fit-method',
         type=int, default=1, choices=(1,2),
         help='Method for fitting sensitivity kernel to harmonics')
+    # least squares solver
+    choices = ('inv','lstsq','gelsd', 'gelsy', 'gelss')
+    parser.add_argument('--solver','-s',
+        type=str, default='lstsq', choices=choices,
+        help='Least squares solver for sensitivity kernel solutions')
     # land-sea mask for redistributing mascon mass
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing mascon mass')
     # output spatial grid
-    parser.add_argument('--spatial','-s',
+    parser.add_argument('--spatial','-o',
         default=False, action='store_true',
         help='Output spatial grid file for each mascon')
     # output grid parameters
     parser.add_argument('--spacing','-S',
         type=float, nargs='+', default=[0.5,0.5], metavar=('dlon','dlat'),
         help='Spatial resolution of output data')
     parser.add_argument('--interval','-I',
@@ -604,14 +625,15 @@
             MMAX=args.mmax,
             LOVE_NUMBERS=args.love,
             REFERENCE=args.reference,
             DATAFORM=args.format,
             MASCON_FILE=args.mascon_file,
             REDISTRIBUTE_MASCONS=args.redistribute_mascons,
             FIT_METHOD=args.fit_method,
+            SOLVER=args.solver,
             LANDMASK=args.mask,
             SPATIAL=args.spatial,
             DDEG=args.spacing,
             INTERVAL=args.interval,
             BOUNDS=args.bounds,
             OUTPUT_DIRECTORY=args.output_directory,
             MODE=args.mode)
```

### Comparing `gravity-toolkit-1.2.0/scripts/cnes_grace_sync.py` & `gravity-toolkit-1.2.1/scripts/cnes_grace_sync.py`

 * *Files 4% similar despite different names*

```diff
@@ -98,26 +98,29 @@
 import re
 import copy
 import time
 import gzip
 import struct
 import shutil
 import logging
+import pathlib
 import tarfile
 import argparse
 import posixpath
 import gravity_toolkit as gravtk
 
 # PURPOSE: sync local GRACE/GRACE-FO files with CNES server
 def cnes_grace_sync(DIRECTORY, DREL=[], TIMEOUT=None, LOG=False,
     CLOBBER=False, MODE=None):
     # remote CNES/GRGS host directory
     HOST = ['http://gravitegrace.get.obs-mip.fr','grgs.obs-mip.fr','data']
+
     # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # create dictionaries for dataset and host directory
     DSET = {}
     DSET['RL01'] = ['GSM', 'GAC']
     DSET['RL02'] = ['GSM', 'GAA', 'GAB']
     DSET['RL03'] = ['GSM', 'GAA', 'GAB']
     DSET['RL04'] = ['GSM']
@@ -166,39 +169,39 @@
     TAR['RL05']['GAB'] = ['CNES-GRGS.RL05.monthly.dealiasing.tar.gz']
 
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: CNES_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'CNES_sync_{today}.log'
-        fid1 = open(os.path.join(DIRECTORY,LOGFILE), mode='w', encoding='utf8')
-        logging.basicConfig(stream=fid1,level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'CNES_sync_{today}.log')
+        fid1 = LOGFILE.open(mode='w', encoding='utf8')
+        logging.basicConfig(stream=fid1, level=logging.INFO)
         logging.info(f'CNES Sync Log ({today})')
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
     # DATA RELEASES (RL01, RL02, RL03, RL04)
     # RL01 and RL02 are no longer updated as default
     for rl in DREL:
         # datasets (GSM, GAA, GAB)
         for ds in DSET[rl]:
             logging.info(f'CNES/{rl}/{ds}')
-            # specific GRACE directory
-            local_dir = os.path.join(DIRECTORY, 'CNES', rl, ds)
-            # check if GRACE directory exists and recursively create if not
-            os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+            # local directory for exact data product
+            local_dir = DIRECTORY.joinpath('CNES', rl, ds)
+            # check if directory exists and recursively create if not
+            local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
             # retrieve each tar file from CNES
             for t in TAR[rl][ds]:
                 remote_tar_path = copy.copy(HOST)
                 remote_tar_path.extend(REMOTE[rl][ds])
                 remote_tar_path.append(t)
                 # local copy of CNES data tar file
-                local_file = os.path.join(DIRECTORY, 'CNES', rl, t)
+                local_file = DIRECTORY.joinpath('CNES', rl, t)
                 MD5 = gravtk.utilities.get_hash(local_file)
                 # copy remote tar file to local if new or updated
                 gravtk.utilities.from_http(remote_tar_path,
                     local=local_file, timeout=TIMEOUT, hash=MD5, chunk=16384,
                     verbose=True, fid=fid1, mode=MODE)
                 # Create and submit request to get modification time of file
                 remote_file = posixpath.join(*remote_tar_path)
@@ -206,55 +209,57 @@
                 response = gravtk.utilities.urllib2.urlopen(request,
                     timeout=TIMEOUT)
                 # change modification time to remote
                 time_string = response.headers['last-modified']
                 remote_mtime = gravtk.utilities.get_unix_time(time_string,
                     format='%a, %d %b %Y %H:%M:%S %Z')
                 # keep remote modification time of file and local access time
-                os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
+                os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
 
                 # open file with tarfile (read)
                 tar = tarfile.open(name=local_file, mode='r:gz')
 
                 # copy files from the tar file into the data directory
                 member_list=[m for m in tar.getmembers() if re.search(ds,m.name)]
                 # for each member of the dataset within the tar file
                 for member in member_list:
                     # local gzipped version of the file
-                    fi = os.path.basename(member.name)
-                    local_file = os.path.join(local_dir, f'{fi}.gz')
+                    granule = pathlib.Path(member.name).stem
+                    local_file = local_dir.joinpath(f'{granule}.gz')
                     gzip_copy_file(tar, member, local_file, CLOBBER, MODE)
                 # close the tar file
                 tar.close()
 
             # find GRACE files and sort by date
-            grace_files = [fi for fi in os.listdir(local_dir) if re.search(ds,fi)]
+            grace_files = [f.name for f in local_dir.iterdir()
+                if re.search(ds, f.name)]
             # outputting GRACE filenames to index
-            index_file = os.path.join(local_dir, 'index.txt')
-            with open(index_file, mode='w', encoding='utf8') as fid:
+            index_file = local_dir.joinpath('index.txt')
+            with index_file.open(mode='w', encoding='utf8') as fid:
                 for fi in sorted(grace_files):
                     print(fi, file=fid)
             # change permissions of index file
-            os.chmod(index_file, MODE)
+            index_file.chmod(mode=MODE)
 
     # close log file and set permissions level to MODE
     if LOG:
         fid1.close()
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: copy file from tar file checking if file exists locally
 # and if the original file is newer than the local file
 def gzip_copy_file(tar, member, local_file, CLOBBER, MODE):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # last modification time of file within tar file
     file1_mtime = member.mtime
     # check if output compressed file exists in local directory
-    if os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if local_file.exists():
         # check last modification time of output gzipped file
         with gzip.open(local_file, 'rb') as fileID:
             fileobj = fileID.fileobj
             fileobj.seek(4)
             # extract little endian 4 bit unsigned integer
             file2_mtime, = struct.unpack("<I", fileobj.read(4))
         # if remote file is newer: overwrite the local file
@@ -264,36 +269,35 @@
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist, is to be overwritten, or CLOBBERed
     if TEST or CLOBBER:
         # Printing files copied from tar file to new compressed file
         logging.info(f'{tar.name}/{member.name} --> ')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # extract file contents to new compressed file
         f_in = tar.extractfile(member)
         with gzip.GzipFile(local_file, 'wb', 9, None, file1_mtime) as f_out:
             shutil.copyfileobj(f_in, f_out)
         f_in.close()
         # keep remote modification time of file and local access time
-        os.utime(local_file, (os.stat(local_file).st_atime, file1_mtime))
-        os.chmod(local_file, MODE)
+        os.utime(local_file, (local_file.stat().st_atime, file1_mtime))
+        local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""CNES/GRGS GRACE data download program for
             gravity field products
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str, nargs='+',
         default=['RL05'], choices=['RL01','RL02','RL03','RL04','RL05'],
         help='GRACE/GRACE-FO data release')
     # connection timeout
```

### Comparing `gravity-toolkit-1.2.0/scripts/combine_harmonics.py` & `gravity-toolkit-1.2.1/scripts/combine_harmonics.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 combine_harmonics.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 Converts a file from the spherical harmonic domain into the spatial domain
 
 CALLING SEQUENCE:
     python combine_harmonics.py -F 2 --lmax 60 -U 1 infile outfile
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
@@ -20,19 +20,20 @@
     --reference X: Reference frame for load love numbers
         CF: Center of Surface Figure (default)
         CM: Center of Mass of Earth System
         CE: Center of Mass of Solid Earth
     -R X, --radius X: Gaussian smoothing radius (km)
     -d, --destripe: use a decorrelation filter (destriping filter)
     -U X, --units X: output units
-        1: cm of water thickness
-        2: mm of geoid height
-        3: mm of elastic crustal deformation [Davis 2004]
-        4: microGal gravitational perturbation
-        5: millibars equivalent surface pressure
+        0: norm, no unit conversion
+        1: cmwe, centimeters water equivalent
+        2: mmGH, millimeters geoid height
+        3: mmCU, millimeters elastic crustal deformation
+        4: micGal, microGal gravity perturbations
+        5: mbar, millibars equivalent surface pressure
     -S X, --spacing X: spatial resolution of output data (dlon,dlat)
     -I X, --interval X: output grid interval
         1: (0:360, 90:-90)
         2: (degree spacing/2)
         3: non-global grid (set with defined bounds)
     -B X, --bounds X: non-global grid bounding box (minlon,maxlon,minlat,maxlat)
     --redistribute-mass: redistribute total mass over the ocean
@@ -67,14 +68,19 @@
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     spatial.py: spatial data class for reading, writing and processing data
     units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: allow units argument to be set to 0 for no unit conversion
+    Updated 03/2023: add index ascii/netCDF4/HDF5 datatypes as possible inputs
+        add descriptive file-level attributes to output netCDF4/HDF5 files
+        use attributes from units class for writing to netCDF4/HDF5 files
     Updated 02/2023: use get function to retrieve specific units
         use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
         iterate over harmonics objects versus indexing
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 07/2022: create mask for output gridded variables
@@ -103,22 +109,23 @@
 from __future__ import print_function
 
 import sys
 import os
 import re
 import copy
 import logging
+import pathlib
 import argparse
 import traceback
 import numpy as np
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: converts from the spherical harmonic domain into the spatial domain
@@ -135,38 +142,60 @@
     BOUNDS=None,
     REDISTRIBUTE=False,
     LANDMASK=None,
     MEAN_FILE=None,
     DATAFORM=None,
     MODE=0o775):
 
+    # verify inputs
+    INPUT_FILE = pathlib.Path(INPUT_FILE).expanduser().absolute()
+    OUTPUT_FILE = pathlib.Path(OUTPUT_FILE).expanduser().absolute()
     # verify that output directory exists
-    DIRECTORY = os.path.abspath(os.path.dirname(OUTPUT_FILE))
-    if not os.access(DIRECTORY, os.F_OK):
-        os.makedirs(DIRECTORY,MODE,exist_ok=True)
+    OUTPUT_FILE.parent.mkdir(mode=MODE, parents=True, exist_ok=True)
+    # attributes for output files
+    attributes = dict(ROOT={})
+    attributes['ROOT']['product_type'] = 'gravity_field'
+    attributes['ROOT']['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
 
     # upper bound of spherical harmonic orders (default = LMAX)
     if MMAX is None:
         MMAX = np.copy(LMAX)
 
     # read input spherical harmonic coefficients from file
-    input_Ylms = gravtk.harmonics().from_file(INPUT_FILE, format=DATAFORM)
+    if DATAFORM in ('ascii', 'netCDF4', 'HDF5'):
+        dataform = copy.copy(DATAFORM)
+        input_Ylms = gravtk.harmonics().from_file(INPUT_FILE,
+            format=DATAFORM)
+        attributes['ROOT']['lineage'] = input_Ylms.filename.name
+    elif DATAFORM in ('index-ascii', 'index-netCDF4', 'index-HDF5'):
+        # read from index file
+        _,dataform = DATAFORM.split('-')
+        input_Ylms = gravtk.harmonics().from_index(INPUT_FILE,
+            format=dataform)
+        attributes['ROOT']['lineage'] = [f.name for f in input_Ylms.filename]
     # reform harmonic dimensions to be l,m,t
     # truncate to degree and order LMAX, MMAX
     input_Ylms = input_Ylms.truncate(lmax=LMAX, mmax=MMAX).expand_dims()
 
     # remove mean file from input Ylms
     if MEAN_FILE:
         mean_Ylms = gravtk.harmonics().from_file(MEAN_FILE,
             format=DATAFORM, date=False)
         input_Ylms.subtract(mean_Ylms)
 
     # read arrays of kl, hl, and ll Love Numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
         REFERENCE=REFERENCE, FORMAT='class')
+    # add attributes for earth parameters
+    attributes['ROOT']['earth_model'] = LOVE.model
+    attributes['ROOT']['earth_love_numbers'] = LOVE.citation
+    attributes['ROOT']['reference_frame'] = LOVE.reference
+    # add attributes for maximum degree and order
+    attributes['ROOT']['max_degree'] = LMAX
+    attributes['ROOT']['max_order'] = MMAX
 
     # distribute total mass uniformly over the ocean
     if REDISTRIBUTE:
         # read Land-Sea Mask and convert to spherical harmonics
         ocean_Ylms = gravtk.ocean_stokes(LANDMASK, LMAX,
             MMAX=MMAX, LOVE=LOVE)
         # calculate ratio between total mass and a uniformly distributed
@@ -210,104 +239,76 @@
         grid.lon = np.arange(dlon/2.0,360+dlon/2.0,dlon)
         grid.lat = np.arange(90.0-dlat/2.0,-90.0-dlat/2.0,-dlat)
         nlon = len(grid.lon)
         nlat = len(grid.lat)
     elif (INTERVAL == 3):
         # non-global grid set with BOUNDS parameter
         minlon,maxlon,minlat,maxlat = BOUNDS.copy()
-        grid.lon = np.arange(minlon+dlon/2.0,maxlon+dlon/2.0,dlon)
-        grid.lat = np.arange(maxlat-dlat/2.0,minlat-dlat/2.0,-dlat)
+        grid.lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+        grid.lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
         nlon = len(grid.lon)
         nlat = len(grid.lat)
     # output spatial grid
-    grid.data = np.zeros((nlat,nlon,nt))
-    grid.mask = np.zeros((nlat,nlon,nt), dtype=bool)
-    # update attributes
-    grid.update_spacing()
-    grid.update_extents()
-    grid.update_dimensions()
+    grid.data = np.zeros((nlat, nlon, nt))
+    grid.mask = np.zeros((nlat, nlon, nt), dtype=bool)
 
     # Setting units factor for output
     # dfactor computes the degree dependent coefficients
     factors = gravtk.units(lmax=LMAX).harmonic(*LOVE)
-    if (UNITS == 1):
-        # 1: cmwe, centimeters water equivalent
-        dfactor = factors.get('cmwe')
-    elif (UNITS == 2):
-        # 2: mmGH, millimeters geoid height
-        dfactor = factors.get('mmGH')
-    elif (UNITS == 3):
-        # 3: mmCU, millimeters elastic crustal deformation
-        dfactor = factors.get('mmCU')
-    elif (UNITS == 4):
-        # 4: micGal, microGal gravity perturbations
-        dfactor = factors.get('microGal')
-    elif (UNITS == 5):
-        # 5: mbar, millibars equivalent surface pressure
-        dfactor = factors.get('mbar')
-    else:
-        raise ValueError(f'Invalid units code {UNITS:d}')
+    # output units and units longname
+    # 0: norm, no unit conversion
+    # 1: cmwe, centimeters water equivalent
+    # 2: mmGH, millimeters geoid height
+    # 3: mmCU, millimeters elastic crustal deformation
+    # 4: micGal, microGal gravity perturbations
+    # 5: mbar, millibars equivalent surface pressure
+    units = gravtk.units.bycode(UNITS)
+    units_name, units_longname = gravtk.units.get_attributes(units)
+    dfactor = factors.get(units)
+    # add attributes for earth parameters
+    attributes['ROOT']['earth_radius'] = f'{factors.rad_e:0.3f} cm'
+    attributes['ROOT']['earth_density'] = f'{factors.rho_e:0.3f} g/cm'
+    attributes['ROOT']['earth_gravity_constant'] = f'{factors.GM:0.3f} cm^3/s^2'
 
     # Computing plms for converting to spatial domain
     theta = (90.0 - grid.lat)*np.pi/180.0
     PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(theta))
 
     # converting harmonics to truncated, smoothed coefficients in output units
     for t,Ylms in enumerate(input_Ylms):
         # convolve spherical harmonics with degree dependent factors
         Ylms.convolve(dfactor*wt)
         # convert spherical harmonics to output spatial grid
         grid.data[:,:,t] = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
             grid.lon, grid.lat, LMAX=LMAX, PLM=PLM).T
 
     # outputting data to file
-    output_data(grid.squeeze(), FILENAME=OUTPUT_FILE,
-        DATAFORM=DATAFORM, UNITS=UNITS, MODE=MODE)
+    grid.squeeze().to_file(filename=OUTPUT_FILE, format=dataform,
+        units=units_name, longname=units_longname,
+        attributes=attributes)
 
-# PURPOSE: wrapper function for outputting data to file
-def output_data(data, FILENAME=None, DATAFORM=None, UNITS=None, MODE=None):
-    # output units and units longname
-    unit_short = ['cmwe', 'mmGH', 'mmCU', 'microGal', 'mbar']
-    unit_name = ['Equivalent_Water_Thickness', 'Geoid_Height',
-        'Elastic_Crustal_Uplift', 'Gravitational_Undulation',
-        'Equivalent_Surface_Pressure']
-    # attributes for output files
-    attributes = {}
-    attributes['units'] = copy.copy(unit_short[UNITS-1])
-    attributes['longname'] = copy.copy(unit_name[UNITS-1])
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
-    # output to file
-    if (DATAFORM == 'ascii'):
-        # ascii (.txt)
-        data.to_ascii(FILENAME)
-    elif (DATAFORM == 'netCDF4'):
-        # netcdf (.nc)
-        data.to_netCDF4(FILENAME, **attributes)
-    elif (DATAFORM == 'HDF5'):
-        # HDF5 (.H5)
-        data.to_HDF5(FILENAME, **attributes)
     # change output permissions level to MODE
-    os.chmod(FILENAME, MODE)
+    OUTPUT_FILE.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Converts a file from the spherical harmonic
             domain into the spatial domain
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # input and output file
     parser.add_argument('infile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Input harmonic file')
     parser.add_argument('outfile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Output spatial file')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
         type=int, default=60,
         help='Maximum spherical harmonic degree')
     parser.add_argument('--mmax','-m',
         type=int, default=None,
@@ -332,15 +333,15 @@
         help='Gaussian smoothing radius (km)')
     # Use a decorrelation (destriping) filter
     parser.add_argument('--destripe','-d',
         default=False, action='store_true',
         help='Verbose output of run')
     # output units
     parser.add_argument('--units','-U',
-        type=int, default=1, choices=[1,2,3,4,5],
+        type=int, default=1, choices=[0,1,2,3,4,5],
         help='Output units')
     # output grid parameters
     parser.add_argument('--spacing','-S',
         type=float, nargs='+', default=[0.5,0.5], metavar=('dlon','dlat'),
         help='Spatial resolution of output data')
     parser.add_argument('--interval','-I',
         type=int, default=2, choices=[1,2,3],
@@ -352,23 +353,26 @@
     # redistribute total mass over the ocean
     parser.add_argument('--redistribute-mass',
         default=False, action='store_true',
         help='Redistribute total mass over the ocean')
     # land-sea mask for redistributing over the ocean
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing over the ocean')
     # mean file to remove
     parser.add_argument('--mean',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Mean file to remove from the harmonic data')
     # input and output data format (ascii, netCDF4, HDF5)
+    choices = []
+    choices.extend(['ascii','netCDF4','HDF5'])
+    choices.extend(['index-ascii','index-netCDF4','index-HDF5'])
     parser.add_argument('--format','-F',
-        type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
+        type=str, default='netCDF4', choices=choices,
         help='Input and output data format')
     # print information about each input and output file
     parser.add_argument('--verbose','-V',
         action='count', default=0,
         help='Verbose output of run')
     # permissions mode of the output files (octal)
     parser.add_argument('--mode','-M',
```

### Comparing `gravity-toolkit-1.2.0/scripts/convert_harmonics.py` & `gravity-toolkit-1.2.1/scripts/convert_harmonics.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 convert_harmonics.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (05/2023)
 Converts a file from the spatial domain into the spherical harmonic domain
 
 CALLING SEQUENCE:
     python convert_harmonics.py -F 2 --lmax 60 -U 1 infile outfile
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
@@ -58,14 +58,17 @@
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     spatial.py: spatial data class for reading, writing and processing data
     units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: updated inputs to spatial from_ascii function
+        add descriptive file-level attributes to output netCDF4/HDF5 files
     Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
         iterate over spatial objects versus indexing
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use wrapper function for reading load Love numbers
         use argparse descriptions within sphinx documentation
     Updated 12/2021: can use variable loglevels for verbose output
@@ -83,23 +86,25 @@
     Written 10/2019
 """
 from __future__ import print_function
 
 import sys
 import os
 import re
+import copy
 import logging
+import pathlib
 import argparse
 import traceback
 import numpy as np
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: converts from the spatial domain into the spherical harmonic domain
@@ -112,18 +117,28 @@
     DDEG=None,
     INTERVAL=None,
     FILL_VALUE=None,
     HEADER=None,
     DATAFORM=None,
     MODE=0o775):
 
+    # verify inputs
+    INPUT_FILE = pathlib.Path(INPUT_FILE).expanduser().absolute()
+    OUTPUT_FILE = pathlib.Path(OUTPUT_FILE).expanduser().absolute()
     # verify that output directory exists
-    DIRECTORY = os.path.abspath(os.path.dirname(OUTPUT_FILE))
-    if not os.access(DIRECTORY, os.F_OK):
-        os.makedirs(DIRECTORY,MODE,exist_ok=True)
+    OUTPUT_FILE.parent.mkdir(mode=MODE, parents=True, exist_ok=True)
+    # attributes for output files
+    attributes = {}
+    attributes['product_type'] = 'gravity_field'
+    attributes['lineage'] = INPUT_FILE.name
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
+
+    # upper bound of spherical harmonic orders (default = LMAX)
+    if MMAX is None:
+        MMAX = np.copy(LMAX)
 
     # Grid spacing
     dlon,dlat = (DDEG,DDEG) if (np.ndim(DDEG) == 0) else (DDEG[0],DDEG[1])
     # Grid dimensions
     if (INTERVAL == 1):# (0:360, 90:-90)
         nlon = np.int64((360.0/dlon)+1.0)
         nlat = np.int64((180.0/dlat)+1.0)
@@ -131,35 +146,44 @@
         nlon = np.int64((360.0/dlon))
         nlat = np.int64((180.0/dlat))
 
     # read spatial file in data format
     # expand dimensions
     if (DATAFORM == 'ascii'):
         # ascii (.txt)
-        input_spatial = gravtk.spatial(spacing=[dlon,dlat],nlat=nlat,
-            nlon=nlon,fill_value=FILL_VALUE).from_ascii(INPUT_FILE,
-            header=HEADER).expand_dims()
+        input_spatial = gravtk.spatial(fill_value=FILL_VALUE).from_ascii(
+            INPUT_FILE, header=HEADER, spacing=[dlon,dlat], nlat=nlat,
+            nlon=nlon).expand_dims()
     elif (DATAFORM == 'netCDF4'):
         # netcdf (.nc)
         input_spatial = gravtk.spatial().from_netCDF4(INPUT_FILE).expand_dims()
     elif (DATAFORM == 'HDF5'):
         # HDF5 (.H5)
         input_spatial = gravtk.spatial().from_HDF5(INPUT_FILE).expand_dims()
     # convert missing values to zero
     input_spatial.replace_invalid(0.0)
     # input data shape
-    nlat,nlon,nt = input_spatial.shape
+    nlat, nlon, nt = input_spatial.shape
 
     # read arrays of kl, hl, and ll Love Numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
-        REFERENCE=REFERENCE)
-
-    # upper bound of spherical harmonic orders (default = LMAX)
-    if MMAX is None:
-        MMAX = np.copy(LMAX)
+        REFERENCE=REFERENCE, FORMAT='class')
+    # add attributes for earth parameters
+    attributes['earth_model'] = LOVE.model
+    attributes['earth_love_numbers'] = LOVE.citation
+    attributes['reference_frame'] = LOVE.reference
+    # add attributes for maximum degree and order
+    attributes['max_degree'] = LMAX
+    attributes['max_order'] = MMAX
+
+    # add attributes for earth parameters
+    factors = gravtk.units(lmax=LMAX)
+    attributes['earth_radius'] = f'{factors.rad_e:0.3f} cm'
+    attributes['earth_density'] = f'{factors.rho_e:0.3f} g/cm'
+    attributes['earth_gravity_constant'] = f'{factors.GM:0.3f} cm^3/s^2'
 
     # calculate associated Legendre polynomials
     th = (90.0 - input_spatial.lat)*np.pi/180.0
     PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(th))
 
     # create list of harmonics objects
     Ylms_list = []
@@ -170,39 +194,38 @@
             LMIN=0, LMAX=LMAX, MMAX=MMAX, PLM=PLM, LOVE=LOVE)
         output_Ylms.time = np.copy(spatial_data.time)
         output_Ylms.month = gravtk.time.calendar_to_grace(spatial_data.time)
         # append to list
         Ylms_list.append(output_Ylms)
     # convert Ylms list for output spherical harmonics
     Ylms = gravtk.harmonics().from_list(Ylms_list, clear=True)
+    # copy attribute to output harmonics
+    Ylms.attributes['ROOT'] = copy.copy(attributes)
 
-    # attributes for output files
-    attributes = {}
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
     # outputting data to file
-    Ylms.to_file(OUTPUT_FILE, format=DATAFORM, **attributes)
+    Ylms.to_file(OUTPUT_FILE, format=DATAFORM)
     # change output permissions level to MODE
-    os.chmod(OUTPUT_FILE, MODE)
+    OUTPUT_FILE.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Converts a file from the spatial domain into the
             spherical harmonic domain
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # input and output file
     parser.add_argument('infile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Input spatial file')
     parser.add_argument('outfile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Output harmonic file')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
         type=int, default=60,
         help='Maximum spherical harmonic degree')
     parser.add_argument('--mmax','-m',
         type=int, default=None,
```

### Comparing `gravity-toolkit-1.2.0/scripts/dealiasing_monthly_mean.py` & `gravity-toolkit-1.2.1/scripts/dealiasing_monthly_mean.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 dealiasing_monthly_mean.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 
 Reads GRACE/GRACE-FO AOD1B datafiles for a specific product and outputs monthly
     the mean for a specific GRACE/GRACE-FO processing center and data release
     GAA: atmospheric loading from ECMWF
     GAB: oceanic loading from OMCT/MPIOM
     GAC: global atmospheric and oceanic loading
     GAD: ocean bottom pressure from OMCT/MPIOM
@@ -44,14 +44,19 @@
 PROGRAM DEPENDENCIES:
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     time.py: utilities for calculating time operations
 
 UPDATE HISTORY:
+    Updated 05/2023: use formatting for reading from date file
+        use pathlib to define and operate on paths
+    Updated 03/2023: read data into flattened harmonics objects
+        debug-level logging of member names and header lines
+        convert shape and ndim to harmonic class properties
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
     Updated 10/2021: using python logging for handling verbose output
     Updated 07/2021: can use default argument files to define options
         added option to output in spherical harmonic model (SHM) format
         remove choices for argparse processing centers
@@ -73,14 +78,15 @@
 
 import sys
 import os
 import re
 import gzip
 import time
 import logging
+import pathlib
 import tarfile
 import argparse
 import numpy as np
 import gravity_toolkit as gravtk
 
 # PURPOSE: calculate the Julian day from the year and the day of the year
 # http://scienceworld.wolfram.com/astronomy/JulianDate.html
@@ -96,32 +102,32 @@
 
     # output data suffix
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
     # aod1b data products
     aod1b_products = dict(GAA='atm',GAB='ocn',GAC='glo',GAD='oba')
     # compile regular expressions operator for the clm/slm headers
     # for the specific AOD1b product
-    hx = re.compile(r'^DATA.*SET.*{0}'.format(aod1b_products[DSET]),re.VERBOSE)
+    hx = re.compile(fr'^DATA.*SET.*{aod1b_products[DSET]}',re.VERBOSE)
     # compile regular expression operator to find numerical instances
     # will extract the data from the file
     regex_pattern = r'[-+]?(?:(?:\d*\.\d+)|(?:\d+\.?))(?:[Ee][+-]?\d+)?'
     rx = re.compile(regex_pattern, re.VERBOSE)
 
     # set number of hours in a file
     # set the ocean model for a given release
     if DREL in ('RL01','RL02','RL03','RL04','RL05'):
         # for 00, 06, 12 and 18
-        n_time = 4
+        nt = 4
         ATMOSPHERE = 'ECMWF'
         OCEAN_MODEL = 'OMCT'
         default_center = 'EIGEN'
         default_lmax = 100
     elif DREL in ('RL06',):
         # for 00, 03, 06, 09, 12, 15, 18 and 21
-        n_time = 8
+        nt = 8
         ATMOSPHERE = 'ECMWF'
         OCEAN_MODEL = 'MPIOM'
         default_center = 'GFZOP'
         default_lmax = 180
     else:
         raise ValueError('Invalid data release')
     # Maximum spherical harmonic degree (LMAX)
@@ -132,49 +138,54 @@
     # AOD1B data products
     product = {}
     product['atm'] = f'Atmospheric loading from {ATMOSPHERE}'
     product['ocn'] = f'Oceanic loading from {OCEAN_MODEL}'
     product['glo'] = 'Global atmospheric and oceanic loading'
     product['oba'] = f'Ocean bottom pressure from {OCEAN_MODEL}'
 
-    # GRACE AOD1B directory for data release
-    aod1b_dir = os.path.join(base_dir,'AOD1B',DREL)
-    # GRACE data directory for data release and processing center
-    grace_dir = os.path.join(base_dir,PROC,DREL)
+    # input directory setup
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    aod1b_dir = base_dir.joinpath('AOD1B', DREL)
+    grace_dir = base_dir.joinpath(PROC, DREL)
     # recursively create output directory if not currently existing
-    if not os.access(os.path.join(grace_dir,DSET),os.F_OK):
-        os.makedirs(os.path.join(grace_dir,DSET), MODE)
+    grace_dir.joinpath(DSET).mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # attributes for output files
     attributes = {}
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
     # file formatting string if outputting to SHM format
     shm = '{0}-2_{1:4.0f}{2:03.0f}-{3:4.0f}{4:03.0f}_{5}_{6}_{7}_{8}00.gz'
     # center name if outputting to SHM format
     if (PROC == 'CSR'):
         CENTER = 'UTCSR'
     elif (PROC == 'GFZ'):
         CENTER = default_center
     elif (PROC == 'JPL'):
         CENTER = 'JPLEM'
     else:
         CENTER = default_center
 
     # read input DATE file from GSM data product
-    grace_datefile = '{0}_{1}_DATES.txt'.format(PROC, DREL)
-    date_input = np.loadtxt(os.path.join(grace_dir,'GSM',grace_datefile),
-        skiprows=1)
-    grace_month = date_input[:,1].astype(np.int64)
-    start_yr = date_input[:,2]
-    start_day = date_input[:,3].astype(np.int64)
-    end_yr = date_input[:,4]
-    end_day = date_input[:,5].astype(np.int64)
+    grace_date_file = f'{PROC}_{DREL}_DATES.txt'
+    # names and formats of GRACE/GRACE-FO date ascii file
+    names = ('t','mon','styr','stday','endyr','endday','total')
+    formats = ('f','i','i','i','i','i','i')
+    dtype = np.dtype({'names':names, 'formats':formats})
+    input_date_file = grace_dir.joinpath('GSM', grace_date_file)
+    date_input = np.loadtxt(input_date_file, skiprows=1, dtype=dtype)
+    tdec = date_input['t']
+    grace_month = date_input['mon']
+    start_yr = date_input['styr']
+    start_day = date_input['stday']
+    end_yr = date_input['endyr']
+    end_day = date_input['endday']
+    total_days = date_input['total']
     # output date file reduced to months with complete AOD
-    f_out = open(os.path.join(grace_dir,DSET,grace_datefile),
-        mode='w', encoding='utf8')
+    output_date_file = grace_dir.joinpath(DSET, grace_date_file)
+    f_out = output_date_file.open(mode='w', encoding='utf8')
     # date file header information
     args = ('Mid-date','Month','Start_Day','End_Day','Total_Days')
     print('{0} {1:>10} {2:>11} {3:>10} {4:>13}'.format(*args), file=f_out)
 
     # for each GRACE/GRACE-FO month
     for t,gm in enumerate(grace_month):
         # check if GRACE/GRACE-FO month crosses years
@@ -197,184 +208,199 @@
         if (DATAFORM == 'SHM'):
             MISSION = 'GRAC' if (gm <= 186) else 'GRFO'
             FILE = shm.format(DSET.upper(),start_yr[t],start_day[t],
                 end_yr[t],end_day[t],MISSION,CENTER,'BC01',DREL[2:])
         else:
             args = (PROC,DREL,DSET.upper(),LMAX,gm,suffix[DATAFORM])
             FILE = '{0}_{1}_{2}_CLM_L{3:d}_{4:03d}.{5}'.format(*args)
+        # complete path to output filename
+        OUTPUT_FILE = grace_dir.joinpath(DSET, FILE)
 
         # calendar dates to read
         JD = np.array(julian_days_to_read)
         Y,M,D,h,m,s = gravtk.time.convert_julian(JD,
             astype='i', format='tuple')
         # find unique year and month pairs to read
         rx1='|'.join(['{0:d}-{1:02d}'.format(*p) for p in set(zip(Y,M))])
         rx2='|'.join(['{0:0d}-{1:02d}-{2:02d}'.format(*p) for p in set(zip(Y,M,D))])
         # compile regular expressions operators for finding tar files
-        tx = re.compile(r'AOD1B_({0})_\d+.(tar.gz|tgz)$'.format(rx1),re.VERBOSE)
+        tx = re.compile(rf'AOD1B_({rx1})_\d+.(tar.gz|tgz)$', re.VERBOSE)
         # finding all of the tar files in the AOD1b directory
-        input_tar_files = [tf for tf in os.listdir(aod1b_dir) if tx.match(tf)]
+        input_tar_files = [tf for tf in aod1b_dir.iterdir() if tx.match(tf.name)]
         # compile regular expressions operators for file dates
         # will extract year and month and calendar day from the ascii file
-        fx = re.compile(r'AOD1B_({0})_X_\d+.asc(.gz)?$'.format(rx2),re.VERBOSE)
+        fx = re.compile(rf'AOD1B_({rx2})_X_\d+.asc(.gz)?$', re.VERBOSE)
 
         # check the last modified times of the tar file members
-        input_mtime = np.zeros_like(julian_days_to_read,dtype=np.int64)
-        input_file_check = np.zeros_like(julian_days_to_read,dtype=bool)
+        input_mtime = np.zeros_like(julian_days_to_read, dtype=np.int64)
+        input_file_check = np.zeros_like(julian_days_to_read, dtype=bool)
         c = 0
         # for each tar file
-        for fi in sorted(input_tar_files):
+        for input_file in sorted(input_tar_files):
             # open the AOD1B monthly tar file
-            tar = tarfile.open(name=os.path.join(aod1b_dir,fi), mode='r:gz')
+            tar = tarfile.open(name=str(input_file), mode='r:gz')
             # for each ascii file within the tar file that matches fx
             monthly_members = [m for m in tar.getmembers() if fx.match(m.name)]
             for member in monthly_members:
                 # check last modification time of input tar file members
                 input_mtime[c] = member.mtime
                 input_file_check[c] = True
                 c += 1
 
         # check if all files exist
         COMPLETE = input_file_check.all()
         # if output file exists: check if input tar file is newer
         TEST = False
         OVERWRITE = 'clobber'
-        if os.access(os.path.join(grace_dir,DSET,FILE), os.F_OK):
+        if OUTPUT_FILE.exists():
             # check last modification time of input and output files
-            output_mtime = os.stat(os.path.join(grace_dir,DSET,FILE)).st_mtime
+            output_mtime = OUTPUT_FILE.stat().st_mtime
             # if input tar file is newer: overwrite the output file
             if (input_mtime > output_mtime).any():
                 TEST = True
                 OVERWRITE = 'overwrite'
         else:
             TEST = True
             OVERWRITE = 'new'
 
         # print GRACE/GRACE-FO dates if there is a complete month of AOD
         if COMPLETE:
             # print GRACE/GRACE-FO dates to file
-            print(('{0:13.8f} {1:03d} {2:8.0f} {3:03d} {4:8.0f} {5:03d} '
-                '{6:8.0f}').format(date_input[t,0],gm,start_yr[t],start_day[t],
-                end_yr[t],end_day[t],date_input[t,6]), file=f_out)
+            print((f'{tdec[t]:13.8f} {gm:03d} '
+                f'{start_yr[t]:8.0f} {start_day[t]:03d} '
+                f'{end_yr[t]:8.0f} {end_day[t]:03d} '
+                f'{total_days[t]:8.0f}'), file=f_out)
 
         # if there are new files, files to be rewritten or clobbered
         if COMPLETE and (TEST or CLOBBER):
             # if verbose: output information about the output file
             logging.info(f'{FILE} ({OVERWRITE})')
             # allocate for the mean output harmonics
             Ylms = gravtk.harmonics(lmax=LMAX, mmax=LMAX)
-            nt = len(julian_days_to_read)*n_time
-            Ylms.clm = np.zeros((LMAX+1,LMAX+1,nt))
-            Ylms.slm = np.zeros((LMAX+1,LMAX+1,nt))
-            Ylms.time = np.zeros((nt))
+            # number of time points
+            n_time = len(julian_days_to_read)*nt
+            # flattened harmonics object
+            YLMS = gravtk.harmonics(lmax=default_lmax, mmax=default_lmax,
+                flattened=True)
+            YLMS.l = np.zeros((n_harm), dtype=int)
+            YLMS.m = np.zeros((n_harm), dtype=int)
+            YLMS.clm = np.zeros((n_harm, n_time))
+            YLMS.slm = np.zeros((n_harm, n_time))
+            # calendar dates
+            years = np.zeros((n_time), dtype=int)
+            months = np.zeros((n_time), dtype=int)
+            days = np.zeros((n_time), dtype=int)
+            hours = np.zeros((n_time), dtype=int)
             count = 0
             # for each tar file
-            for fi in sorted(input_tar_files):
+            for input_file in sorted(input_tar_files):
                 # open the AOD1B monthly tar file
-                tar = tarfile.open(name=os.path.join(aod1b_dir,fi), mode='r:gz')
+                tar = tarfile.open(name=str(input_file), mode='r:gz')
                 # for each ascii file within the tar file that matches fx
                 monthly_members=[m for m in tar.getmembers() if fx.match(m.name)]
                 for member in monthly_members:
+                    # track tar file members
+                    logging.debug(member.name)
                     # extract member name
-                    YMD,SFX = fx.findall(member.name).pop()
+                    YMD, SFX = fx.findall(member.name).pop()
+                    YY, MM, DD = re.findall(r'\d+', YMD)
                     # open datafile for day
                     if (SFX == '.gz'):
                         fid = gzip.GzipFile(fileobj=tar.extractfile(member))
                     else:
                         fid = tar.extractfile(member)
                     # create counters for hour in dataset
-                    hours = np.zeros((n_time))
                     c = 0
                     # while loop ends when dataset is read
-                    while (c < n_time):
+                    while (c < nt):
                         # read line
                         file_contents=fid.readline().decode('ISO-8859-1')
                         # find file header for data product
                         if bool(hx.search(file_contents)):
+                            # track file header lines
+                            logging.debug(file_contents)
                             # extract hour from header and convert to float
                             HH, = re.findall(r'(\d+):\d+:\d+',file_contents)
-                            hours[c] = np.int64(HH)
+                            # convert dates to int and save to arrays
+                            years[count] = np.int64(YY)
+                            months[count] = np.int64(MM)
+                            days[count] = np.int64(DD)
+                            hours[count] = np.int64(HH)
                             # read each line of spherical harmonics
                             for k in range(0,n_harm):
                                 file_contents=fid.readline().decode('ISO-8859-1')
                                 # find numerical instances in the data line
                                 line_contents = rx.findall(file_contents)
                                 # spherical harmonic degree and order
-                                l1 = np.int64(line_contents[0])
-                                m1 = np.int64(line_contents[1])
-                                # spherical harmonic data saved to output Ylms
-                                if (l1 <= LMAX) & (m1 <= LMAX):
-                                    Ylms.clm[l1,m1,c]+=np.float64(line_contents[2])
-                                    Ylms.slm[l1,m1,c]+=np.float64(line_contents[3])
+                                YLMS.l[k] = np.int64(line_contents[0])
+                                YLMS.m[k] = np.int64(line_contents[1])
+                                # extract spherical harmonics
+                                YLMS.clm[k,count] = np.float64(line_contents[2])
+                                YLMS.slm[k,count] = np.float64(line_contents[3])
                             # add 1 to hour counter
                             c += 1
+                            count += 1
                     # close the input file for day
                     fid.close()
-                    # year fraction of the particular date and times
-                    YEAR = np.repeat(Y[count//n_time], n_time).astype('f')
-                    MONTH = np.repeat(M[count//n_time], n_time).astype('f')
-                    DAY = np.repeat(D[count//n_time], n_time).astype('f')
-                    tdec = gravtk.time.convert_calendar_decimal(YEAR,
-                        MONTH, day=DAY, hour=hours)
-                    Ylms.time[count:count+n_time] = np.copy(tdec)
-                    # add to day counter
-                    count += n_time
 
+            # calculate times for flattened harmonics
+            YLMS.time = gravtk.time.convert_calendar_decimal(
+                years, months, day=days, hour=hours)
+            YLMS.month = gravtk.time.calendar_to_grace(YLMS.time)
+            # convert to expanded form and truncate to LMAX
+            Ylms = YLMS.expand(date=True).truncate(LMAX)
             # calculate mean harmonics for GRACE/GRACE-FO month
             # convert from harmonics object to dealiasing object
             mean_Ylms = dealiasing().from_harmonics(Ylms.mean())
             mean_Ylms.time = np.mean(Ylms.time)
             mean_Ylms.month = np.int64(gm)
             # product information
             mean_Ylms.center = PROC
             mean_Ylms.release = DREL
             mean_Ylms.product = DSET
             # start and end time for month
             start_time = gravtk.time.convert_julian(np.min(JD))
-            mean_Ylms.start_time = ['{0:4.0f}'.format(start_time['year']),
-                '{0:02.0f}'.format(start_time['month']),
-                '{0:02.0f}'.format(start_time['day'])]
+            mean_Ylms.start_time = [f"{start_time['year']:4.0f}",
+                f"{start_time['month']:02.0f}",
+                f"{start_time['day']:02.0f}"]
             end_time = gravtk.time.convert_julian(np.max(JD))
-            mean_Ylms.end_time = ['{0:4.0f}'.format(end_time['year']),
-                '{0:02.0f}'.format(end_time['month']),
-                '{0:02.0f}'.format(end_time['day'])]
+            mean_Ylms.end_time = [f"{end_time['year']:4.0f}",
+                f"{end_time['month']:02.0f}",
+                f"{end_time['day']:02.0f}"]
 
             # output mean Ylms to file
             if (DATAFORM == 'ascii'):
                 # ascii (.txt)
-                mean_Ylms.to_ascii(os.path.join(grace_dir,DSET,FILE))
+                mean_Ylms.to_ascii(OUTPUT_FILE)
             elif (DATAFORM == 'netCDF4'):
                 # netcdf (.nc)
-                mean_Ylms.to_netCDF4(os.path.join(grace_dir,DSET,FILE),
-                    **attributes)
+                mean_Ylms.to_netCDF4(OUTPUT_FILE, **attributes)
             elif (DATAFORM == 'HDF5'):
                 # HDF5 (.H5)
-                mean_Ylms.to_HDF5(os.path.join(grace_dir,DSET,FILE),
-                    **attributes)
+                mean_Ylms.to_HDF5(OUTPUT_FILE, **attributes)
             elif (DATAFORM == 'SHM'):
-                mean_Ylms.to_SHM(os.path.join(grace_dir,DSET,FILE),
-                    gzip=True)
+                mean_Ylms.to_SHM(OUTPUT_FILE, gzip=True)
             # set the permissions mode of the output file
-            os.chmod(os.path.join(grace_dir,DSET,FILE), MODE)
+            OUTPUT_FILE.chmod(mode=MODE)
         # log if dataset is incomplete
         elif not COMPLETE:
             logging.info(f'File {FILE} not output (incomplete)')
 
     # if outputting as spherical harmonic model files
     if (DATAFORM == 'SHM'):
-        # Create an index file for each GRACE product
-        grace_files = [fi for fi in os.listdir(os.path.join(grace_dir,DSET)) if
-            re.match(r'{0}-2(.*?)\.gz'.format(DSET),fi)]
+        # Create an index file for the output GRACE product
+        grace_files = [f.name for f in grace_dir.joinpath(DSET).iterdir() if
+            re.match(rf'{DSET}-2(.*?)\.gz', f.name)]
         # outputting GRACE filenames to index
-        index_file = os.path.join(grace_dir,DSET,'index.txt')
-        with open(index_file, mode='w', encoding='utf8') as fid:
+        grace_index_file = grace_dir.joinpath(DSET, 'index.txt')
+        with grace_index_file.open(mode='w', encoding='utf8') as fid:
             for fi in sorted(grace_files):
                 print(fi, file=fid)
         # change permissions of index file
-        os.chmod(index_file, MODE)
+        grace_index_file.chmod(mode=MODE)
 
     # print completion flag
     logging.info(f'Complete: {PROC}/{DREL}/{DSET}')
     # close the output date file
     f_out.close()
 
 # PURPOSE: additional routines for the harmonics module
@@ -390,42 +416,42 @@
 
     def from_harmonics(self, temp):
         """
         Convert a harmonics object to a new dealiasing object
         """
         self = dealiasing(lmax=temp.lmax, mmax=temp.mmax)
         # try to assign variables to self
-        for key in ['clm','slm','time','month','shape','ndim','filename',
+        for key in ['clm','slm','time','month','filename',
             'center','release','product','start_time','end_time']:
             try:
                 val = getattr(temp, key)
                 setattr(self, key, np.copy(val))
             except AttributeError:
                 pass
-        # assign ndim and shape attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def to_SHM(self, filename, **kwargs):
         """
         Write a harmonics object to SHM file
         Inputs: full path of output SHM file
         Options:
             harmonics objects contain date information
             keyword arguments for SHM output
         """
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         # set default verbosity
         kwargs.setdefault('verbose',False)
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         # open the output file
         if self.gzip:
             fid = gzip.open(self.filename, 'wt')
         else:
-            fid = open(self.filename, mode='w', encoding='utf8')
+            fid = self.filename.open(mode='w', encoding='utf8')
         # print the header informat
         self.print_header(fid)
         self.print_harmonic(fid)
         self.print_global(fid)
         self.print_variables(fid,'double precision')
         # output file format
         file_format = ('{0:6} {1:4d} {2:4d} {3:+18.12E} {4:+18.12E} '
@@ -485,15 +511,15 @@
         units = 'meters'
         fid.write('      {0:20}: {1}\n'.format('units',units))
         value = '6.3781366000E+06'
         fid.write('      {0:20}: {1}\n'.format('value',value))
         fid.write('\n')
 
     # PURPOSE: print global attributes to YAML header
-    def print_global(self,fid):
+    def print_global(self, fid):
         fid.write('  {0}:\n'.format('global_attributes'))
         # product title
         if (self.month <= 186):
             MISSION = 'GRACE'
             PROJECT = 'NASA Gravity Recovery And Climate Experiment (GRACE)'
             ACKNOWLEDGEMENT = ('GRACE is a joint mission of NASA (USA) and '
                 'DLR (Germany).')
@@ -585,15 +611,15 @@
         end_date = '{0}-{1}-{2}'.format(*self.end_time)
         fid.write('    {0:22}: {1}\n'.format('time_coverage_end',end_date))
         today = time.strftime('%Y-%m-%d',time.localtime())
         fid.write('    {0:22}: {1}\n'.format('date_created', today))
         fid.write('\n')
 
     # PURPOSE: print variable descriptions to YAML header
-    def print_variables(self,fid,data_precision):
+    def print_variables(self, fid, data_precision):
         # variables
         fid.write('  {0}:\n'.format('variables'))
         # record_key
         fid.write('    {0:22}:\n'.format('record_key'))
         long_name = 'Earth Gravity Spherical Harmonic Model Format Type 2'
         fid.write('      {0:20}: {1}\n'.format('long_name', long_name))
         fid.write('      {0:20}: {1}\n'.format('data_type', 'string'))
@@ -677,16 +703,15 @@
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
```

### Comparing `gravity-toolkit-1.2.0/scripts/esa_costg_swarm_sync.py` & `gravity-toolkit-1.2.1/scripts/esa_costg_swarm_sync.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 esa_costg_swarm_sync.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Syncs Swarm gravity field products from the ESA Swarm Science Server
     https://earth.esa.int/eogateway/missions/swarm/data
     https://www.esa.int/Applications/Observing_the_Earth/Swarm
 
 CALLING SEQUENCE:
     python esa_costg_swarm_sync.py
 
@@ -25,14 +25,15 @@
         https://lxml.de/
         https://github.com/lxml/lxml
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 10/2021: using python logging for handling verbose output
     Written 09/2021
 """
 from __future__ import print_function
@@ -41,36 +42,37 @@
 import re
 import os
 import io
 import json
 import time
 import shutil
 import logging
+import pathlib
 import argparse
 import posixpath
 import lxml.etree
 import gravity_toolkit as gravtk
 
 # PURPOSE: sync local Swarm files with ESA server
 def esa_costg_swarm_sync(DIRECTORY, RELEASE=None, TIMEOUT=None, LOG=False,
     LIST=False, CLOBBER=False, CHECKSUM=False, MODE=0o775):
 
-    # local directory for exact data product
-    local_dir = os.path.join(DIRECTORY,'Swarm',RELEASE,'GSM')
     # check if directory exists and recursively create if not
-    os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    # local directory for exact data product
+    local_dir = DIRECTORY.joinpath('Swarm',RELEASE,'GSM')
+    local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: ESA_Swarm_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'ESA_Swarm_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'ESA_Swarm_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'ESA Swarm Sync Log ({today})')
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
     # Swarm Science Server url
     # using the JSON api protocols to retrieve files
@@ -115,46 +117,47 @@
         maxfiles = len(table['results'])
         # update position
         pos += maxfiles
 
     # find lines of valid files
     valid_lines = [i for i,f in enumerate(colnames) if R1.match(f)]
     # write each file to an index
-    index_file = os.path.join(local_dir,'index.txt')
-    fid = open(index_file, mode='w', encoding='utf8')
+    index_file = local_dir.joinpath(local_dir,'index.txt')
+    fid = index_file.open(mode='w', encoding='utf8')
     # for each data and header file
     for i in valid_lines:
         # remote and local versions of the file
         parameters = gravtk.utilities.urlencode({'file':
             posixpath.join('swarm','Level2longterm','EGF',colnames[i])})
         remote_file = posixpath.join(HOST,
             f'?do=download&{parameters}')
-        local_file = os.path.join(local_dir,colnames[i])
+        local_file = local_dir.joinpath(colnames[i])
         # check that file is not in file system unless overwriting
         http_pull_file(remote_file, collastmod[i], local_file,
             TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
             CHECKSUM=CHECKSUM, MODE=MODE)
         # output Swarm filenames to index
         print(colnames[i], file=fid)
     # change permissions of index file
-    os.chmod(index_file, MODE)
+    index_file.chmod(mode=MODE)
 
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def http_pull_file(remote_file, remote_mtime, local_file, TIMEOUT=120,
     LIST=False, CLOBBER=False, CHECKSUM=False, MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if CHECKSUM and os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if CHECKSUM and local_file.exists():
         # generate checksum hash for local file
         # open the local_file in binary read mode
         local_hash = gravtk.utilities.get_hash(local_file)
         # Create and submit request.
         # There are a wide range of exceptions that can be thrown here
         # including HTTPError and URLError.
         req = gravtk.utilities.urllib2.Request(remote_file)
@@ -164,66 +167,65 @@
         remote_buffer.seek(0)
         # generate checksum hash for remote file
         remote_hash = gravtk.utilities.get_hash(remote_buffer)
         # compare checksums
         if (local_hash != remote_hash):
             TEST = True
             OVERWRITE = f' (checksums: {local_hash} {remote_hash})'
-    elif os.access(local_file, os.F_OK):
+    elif local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         logging.info(f'{remote_file} --> ')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # if executing copy command (not only printing the files)
         if not LIST:
             # chunked transfer encoding size
             CHUNK = 16 * 1024
             # copy bytes or transfer file
-            if CHECKSUM and os.access(local_file, os.F_OK):
+            if CHECKSUM and local_file.exists():
                 # store bytes to file using chunked transfer encoding
                 remote_buffer.seek(0)
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     shutil.copyfileobj(remote_buffer, f, CHUNK)
             else:
                 # Create and submit request.
                 # There are a range of exceptions that can be thrown here
                 # including HTTPError and URLError.
                 request = gravtk.utilities.urllib2.Request(remote_file)
                 response = gravtk.utilities.urllib2.urlopen(request,
                     timeout=TIMEOUT)
                 # copy remote file contents to local file
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     shutil.copyfileobj(response, f, CHUNK)
             # keep remote modification time of file and local access time
-            os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-            os.chmod(local_file, MODE)
+            os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+            local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs Swarm gravity field products from the
             ESA Swarm Science Server
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # data release
     parser.add_argument('--release','-r',
         type=str, default='RL01', choices=['RL01'],
         help='Data release to sync')
     # connection timeout
     parser.add_argument('--timeout','-t',
```

### Comparing `gravity-toolkit-1.2.0/scripts/geocenter_compare_tellus.py` & `gravity-toolkit-1.2.1/scripts/geocenter_compare_tellus.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 geocenter_compare_tellus.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 Plots the GRACE/GRACE-FO geocenter time series for different
     GRACE/GRACE-FO processing centers comparing with the
     JPL GRACE Tellus product
 
 CALLING SEQUENCE:
     python geocenter_processing_centers.py --start 4 --end 216
 
@@ -13,25 +13,26 @@
     -D X, --directory X: working data directory with geocenter files
     -r X, --release X: GRACE/GRACE-FO data release
     -S X, --start X: starting GRACE month for time series
     -E X, --end X: ending GRACE month for time series
     -M X, --missing X: Missing GRACE months in time series
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 05/2022: use argparse descriptions within documentation
     Updated 12/2021: adjust minimum x limit based on starting GRACE month
     Updated 11/2021: use gravity_toolkit geocenter class for operations
     Written 05/2021
 """
 from __future__ import print_function
 
-import os
+import pathlib
 import argparse
 import warnings
 import numpy as np
 import gravity_toolkit as gravtk
 # attempt imports
 try:
     import matplotlib
@@ -78,15 +79,15 @@
         # additionally plot GFZ with SLR replaced pole tide
         if (pr == 'GFZwPT'):
             fargs = ('GFZ',DREL,model_str,input_flags[3])
         else:
             fargs = (pr,DREL,model_str,input_flags[2])
         # read geocenter file for processing center and model
         grace_file = '{0}_{1}_{2}_{3}.txt'.format(*fargs)
-        DEG1 = gravtk.geocenter().from_UCI(os.path.join(grace_dir,grace_file))
+        DEG1 = gravtk.geocenter().from_UCI(grace_dir.joinpath(grace_file))
         # indices for mean months
         kk, = np.nonzero((DEG1.month >= START_MON) & (DEG1.month <= 176))
         DEG1.mean(apply=True, indices=kk)
         # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
         DEG1.to_cartesian(kl=0.021)
         # plot each coefficient
         for j,key in enumerate(fig_labels):
@@ -103,15 +104,15 @@
                     data[i] = val[mm]
             # plot all dates
             ax[j].plot(tdec, data, color=plot_colors['Iterated SLF'],
                 label='Iterated SLF')
 
         if (pr == 'GFZwPT'):
             grace_file = 'GRAVIS-2B_GFZOP_GEOCENTER_0002.dat'
-            DEG1 = gravtk.geocenter().from_gravis(os.path.join(grace_dir,grace_file))
+            DEG1 = gravtk.geocenter().from_gravis(grace_dir.joinpath(grace_file))
             # indices for mean months
             kk, = np.nonzero((DEG1.month >= START_MON) & (DEG1.month <= 176))
             DEG1.mean(apply=True, indices=kk)
             # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
             DEG1.to_cartesian(kl=0.021)
             # plot each coefficient
             for j,key in enumerate(fig_labels):
@@ -129,15 +130,15 @@
                         data[i] = val[mm]
                 # plot all dates
                 ax[j].plot(tdec, data, color=plot_colors['GFZ GravIS'],
                     label='GFZ GravIS')
 
         # Running function read_tellus_geocenter.py
         grace_file = f'TN-13_GEOC_{pr}_{DREL}.txt'
-        DEG1 = gravtk.geocenter().from_tellus(os.path.join(grace_dir,grace_file),
+        DEG1 = gravtk.geocenter().from_tellus(grace_dir.joinpath(grace_file),
             JPL=True)
         # indices for mean months
         kk, = np.nonzero((DEG1.month >= START_MON) & (DEG1.month <= 176))
         DEG1.mean(apply=True, indices=kk)
         # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
         DEG1.to_cartesian(kl=0.021)
         # plot each coefficient
@@ -201,29 +202,28 @@
             text.set_color(plot_colors[text.get_text()])
         # labels and set limits
         ax[0].set_ylabel('Geocenter Variation [mm]', fontsize=14)
         # adjust locations of subplots
         fig.subplots_adjust(left=0.06,right=0.98,bottom=0.12,top=0.94,wspace=0.05)
         # save figure to file
         OUTPUT_FIGURE = f'TN13_SV19_{pr}_{DREL}.pdf'
-        plt.savefig(os.path.join(grace_dir,OUTPUT_FIGURE), format='pdf', dpi=300)
+        plt.savefig(grace_dir.joinpath(OUTPUT_FIGURE), format='pdf', dpi=300)
         plt.clf()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Plots the GRACE/GRACE-FO geocenter time series for
             different GRACE/GRACE-FO processing centers comparing with the
             JPL GRACE Tellus product
             """
     )
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str,
         default='RL06', choices=['RL04','RL05','RL06'],
         help='GRACE/GRACE-FO data release')
     # start and end GRACE/GRACE-FO months
```

### Comparing `gravity-toolkit-1.2.0/scripts/geocenter_monte_carlo.py` & `gravity-toolkit-1.2.1/scripts/geocenter_monte_carlo.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,34 +1,35 @@
 #!/usr/bin/env python
 u"""
 geocenter_monte_carlo.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 CALLING SEQUENCE:
     python geocenter_monte_carlo.py --start 4 --end 237
 
 COMMAND LINE OPTIONS:
     -D X, --directory X: working data directory with geocenter files
     -c X, --center X: GRACE/GRACE-FO data processing center
     -r X, --release X: GRACE/GRACE-FO data release
     -S X, --start X: starting GRACE month for time series
     -E X, --end X: ending GRACE month for time series
     -M X, --missing X: Missing GRACE months in time series
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 05/2022: use argparse descriptions within documentation
     Updated 12/2021: adjust minimum x limit based on starting GRACE month
     Written 11/2021
 """
 from __future__ import print_function
 
-import os
+import pathlib
 import argparse
 import warnings
 import numpy as np
 import gravity_toolkit as gravtk
 
 # attempt imports
 try:
@@ -70,15 +71,15 @@
     # 3 row plot (C10, C11 and S11)
     ax = {}
     fig,(ax[0],ax[1],ax[2])=plt.subplots(num=1,ncols=3,sharey=True,figsize=(9,4))
 
     # read geocenter file for processing center and model
     fargs = (PROC,DREL,model_str,input_flag,gia_str,delta_str,ds_str)
     grace_file = '{0}_{1}_{2}_{3}{4}{5}{6}.nc'.format(*fargs)
-    DEG1 = gravtk.geocenter().from_netCDF4(os.path.join(grace_dir,grace_file))
+    DEG1 = gravtk.geocenter().from_netCDF4(grace_dir.joinpath(grace_file))
     # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
     DEG1.to_cartesian(kl=0.021)
     # number of monte carlo runs
     _,nruns = np.shape(DEG1.C10)
 
     # plot each coefficient
     for j,key in enumerate(fig_labels):
@@ -153,28 +154,27 @@
 
     # labels and set limits
     ax[0].set_ylabel(f'{PROC} Geocenter Variation [mm]', fontsize=14)
     # adjust locations of subplots
     fig.subplots_adjust(left=0.06,right=0.98,bottom=0.12,top=0.94,wspace=0.05)
     # save figure to file
     OUTPUT_FIGURE = f'SV19_{PROC}_{DREL}_monte_carlo.pdf'
-    plt.savefig(os.path.join(grace_dir,OUTPUT_FIGURE), format='pdf', dpi=300)
+    plt.savefig(grace_dir.joinpath(OUTPUT_FIGURE), format='pdf', dpi=300)
     plt.clf()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Plots the GRACE/GRACE-FO geocenter time series for
             each iteration of a monte carlo solution
             """
     )
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
```

### Comparing `gravity-toolkit-1.2.0/scripts/geocenter_ocean_models.py` & `gravity-toolkit-1.2.1/scripts/geocenter_ocean_models.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 geocenter_ocean_models.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 Plots the GRACE/GRACE-FO geocenter time series comparing results
     using different ocean bottom pressure estimates
 
 CALLING SEQUENCE:
     python geocenter_ocean_models.py --start 4 --end 216 \
         --ocean MPIOM ECCO_kf080i ECCO_dr080i
 
@@ -15,14 +15,15 @@
     -r X, --release X: GRACE/GRACE-FO data release
     -S X, --start X: starting GRACE month for time series
     -E X, --end X: ending GRACE month for time series
     -M X, --missing X: Missing GRACE months in time series
     -O X, --ocean X: ocean bottom pressure products to use
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 05/2022: use argparse descriptions within documentation
     Updated 12/2021: adjust minimum x limit based on starting GRACE month
     Updated 11/2021: use gravity_toolkit geocenter class for operations
     Updated 07/2021: iterate over each processing center
@@ -30,15 +31,15 @@
     Updated 04/2020: use units class for setting earth parameters
     Updated 02/2020: add minor ticks and adjust x axes
     Updated 11/2019: adjust axes and set directory to full path
     Updated 09/2019: for public release of time series to references page
 """
 from __future__ import print_function
 
-import os
+import pathlib
 import argparse
 import warnings
 import numpy as np
 import gravity_toolkit as gravtk
 
 # attempt imports
 try:
@@ -77,15 +78,15 @@
     # 3 row plot (C10, C11 and S11)
     ax = {}
     fig,(ax[0],ax[1],ax[2])=plt.subplots(num=1,ncols=3,sharey=True,figsize=(9,4))
     # plot geocenter estimates for each processing center
     for k,mdl in enumerate(MODEL):
         # read geocenter file for processing center and model
         grace_file = '{0}_{1}_{2}_{3}.txt'.format(PROC,DREL,mdl,input_flags[2])
-        DEG1 = gravtk.geocenter().from_UCI(os.path.join(grace_dir,grace_file))
+        DEG1 = gravtk.geocenter().from_UCI(grace_dir.joinpath(grace_file))
         # indices for mean months
         kk, = np.nonzero((DEG1.month >= START_MON) & (DEG1.month <= 176))
         DEG1.mean(apply=True, indices=kk)
         # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
         DEG1.to_cartesian(kl=0.021)
         # plot each coefficient
         for j,key in enumerate(fig_labels):
@@ -102,15 +103,15 @@
             # plot all dates
             label = mdl.replace('_','-')
             ax[j].plot(tdec, data, color=plot_colors[k], label=label)
 
     # read geocenter file for processing center and model
     model_str = 'OMCT' if DREL in ('RL04','RL05') else 'MPIOM'
     grace_file = '{0}_{1}_{2}_{3}.txt'.format(PROC,DREL,model_str,input_flags[2])
-    DEG1 = gravtk.geocenter().from_UCI(os.path.join(grace_dir,grace_file))
+    DEG1 = gravtk.geocenter().from_UCI(grace_dir.joinpath(grace_file))
     # add axis labels and adjust font sizes for axis ticks
     for j,key in enumerate(fig_labels):
         # vertical lines for end of the GRACE mission and start of GRACE-FO
         jj, = np.flatnonzero(DEG1.month == 186)
         kk, = np.flatnonzero(DEG1.month == 198)
         ax[j].axvspan(DEG1.time[jj],DEG1.time[kk],
             color='0.5',ls='dashed',alpha=0.15)
@@ -147,28 +148,27 @@
         text.set_color(plot_colors[i])
     # labels and set limits
     ax[0].set_ylabel('Geocenter Variation [mm]', fontsize=14)
     # adjust locations of subplots
     fig.subplots_adjust(left=0.06,right=0.98,bottom=0.12,top=0.94,wspace=0.05)
     # save figure to file
     OUTPUT_FIGURE = f'SV19_{PROC}_{DREL}_ocean_models.pdf'
-    plt.savefig(os.path.join(grace_dir,OUTPUT_FIGURE), format='pdf', dpi=300)
+    plt.savefig(grace_dir.joinpath(OUTPUT_FIGURE), format='pdf', dpi=300)
     plt.clf()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Plots the GRACE/GRACE-FO geocenter time series
             comparing results using different ocean bottom pressure estimates
             """
     )
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'], choices=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO processing center')
     # GRACE/GRACE-FO data release
```

### Comparing `gravity-toolkit-1.2.0/scripts/geocenter_processing_centers.py` & `gravity-toolkit-1.2.1/scripts/geocenter_processing_centers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 geocenter_processing_centers.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 Plots the GRACE/GRACE-FO geocenter time series for different
     GRACE/GRACE-FO processing centers
 
 CALLING SEQUENCE:
     python geocenter_processing_centers.py --start 4 --end 216
 
 COMMAND LINE OPTIONS:
@@ -13,14 +13,15 @@
     -c X, --center X: GRACE/GRACE-FO processing center
     -r X, --release X: GRACE/GRACE-FO data release
     -S X, --start X: starting GRACE month for time series
     -E X, --end X: ending GRACE month for time series
     -M X, --missing X: Missing GRACE months in time series
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 05/2022: use argparse descriptions within documentation
     Updated 12/2021: adjust minimum x limit based on starting GRACE month
         make the list of processing centers an option
     Updated 11/2021: use gravity_toolkit geocenter class for operations
@@ -31,15 +32,15 @@
     Updated 04/2020: use units class for setting earth parameters
     Updated 02/2020: add minor ticks and adjust x axes
     Updated 11/2019: adjust axes and set directory to full path
     Updated 09/2019: for public release of time series to references page
 """
 from __future__ import print_function
 
-import os
+import pathlib
 import argparse
 import warnings
 import numpy as np
 import gravity_toolkit as gravtk
 
 # attempt imports
 try:
@@ -89,15 +90,15 @@
             fargs = ('GFZ',DREL,model_str,input_flags[3])
         elif (pr == 'GFZ+CS21+CS22'):
             fargs = ('GFZ',DREL,model_str,input_flags[4])
         else:
             fargs = (pr,DREL,model_str,input_flags[2])
         # read geocenter file for processing center and model
         grace_file = '{0}_{1}_{2}_{3}.txt'.format(*fargs)
-        DEG1 = gravtk.geocenter().from_UCI(os.path.join(grace_dir,grace_file))
+        DEG1 = gravtk.geocenter().from_UCI(grace_dir.joinpath(grace_file))
         # indices for mean months
         kk, = np.nonzero((DEG1.month >= START_MON) & (DEG1.month <= 176))
         DEG1.mean(apply=True, indices=kk)
         # setting Load Love Number (kl) to 0.021 to match Swenson et al. (2008)
         DEG1.to_cartesian(kl=0.021)
         # plot each coefficient
         for j,key in enumerate(fig_labels):
@@ -158,28 +159,27 @@
         text.set_color(plot_colors[text.get_text()])
     # labels and set limits
     ax[0].set_ylabel('Geocenter Variation [mm]', fontsize=14)
     # adjust locations of subplots
     fig.subplots_adjust(left=0.06,right=0.98,bottom=0.12,top=0.94,wspace=0.05)
     # save figure to file
     OUTPUT_FIGURE = f'SV19_{DREL}_centers.pdf'
-    plt.savefig(os.path.join(grace_dir,OUTPUT_FIGURE), format='pdf', dpi=300)
+    plt.savefig(grace_dir.joinpath(OUTPUT_FIGURE), format='pdf', dpi=300)
     plt.clf()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Plots the GRACE/GRACE-FO geocenter time series for
             different GRACE/GRACE-FO processing centers
             """
     )
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # Data processing center or satellite mission
     PROC = ['CSR','GFZ','GFZwPT','JPL']
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+', default=PROC,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
```

### Comparing `gravity-toolkit-1.2.0/scripts/gfz_icgem_costg_ftp.py` & `gravity-toolkit-1.2.1/scripts/gfz_icgem_costg_ftp.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gfz_icgem_costg_ftp.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Syncs GRACE/GRACE-FO/Swarm COST-G data from the GFZ International
     Centre for Global Earth Models (ICGEM)
 
 CALLING SEQUENCE:
     python gfz_icgem_costg_ftp.py --mission Grace Grace-FO Swarm
 
 OUTPUTS:
@@ -34,14 +34,15 @@
     lxml: processing XML and HTML in Python
         https://pypi.python.org/pypi/lxml
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for syncing files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 10/2021: using python logging for handling verbose output
     Written 09/2021
 """
 from __future__ import print_function
@@ -50,14 +51,15 @@
 import os
 import re
 import time
 import ftplib
 import shutil
 import hashlib
 import logging
+import pathlib
 import argparse
 import posixpath
 import gravity_toolkit as gravtk
 
 # PURPOSE: create and compile regular expression operator to find files
 def compile_regex_pattern(MISSION, DSET):
     if ((DSET == 'GSM') and (MISSION == 'Swarm')):
@@ -70,20 +72,18 @@
     # return the compiled regular expression operator used to find files
     return re.compile(regex, re.VERBOSE)
 
 # PURPOSE: sync local GRACE/GRACE-FO/Swarm files with GFZ ICGEM server
 def gfz_icgem_costg_ftp(DIRECTORY, MISSION=[], RELEASE=None, TIMEOUT=None,
     LOG=False, LIST=False, CLOBBER=False, CHECKSUM=False, MODE=None):
 
-    # connect and login to GFZ ICGEM ftp server
-    ftp = ftplib.FTP('icgem.gfz-potsdam.de', timeout=TIMEOUT)
-    ftp.login()
-
     # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # dealiasing datasets for each mission
     DSET = {}
     DSET['Grace'] = ['GAC','GSM']
     DSET['Grace-FO'] = ['GSM']
     DSET['Swarm'] = ['GAA','GAB','GAC','GAD','GSM']
     # local subdirectory for data
     LOCAL = {}
@@ -92,34 +92,36 @@
     LOCAL['Swarm'] = 'Swarm'
 
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: GFZ_ICGEM_COST-G_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'GFZ_ICGEM_COST-G_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'GFZ_ICGEM_COST-G_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'GFZ ICGEM COST-G Sync Log ({today})')
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
+    # connect and login to GFZ ICGEM ftp server
+    ftp = ftplib.FTP('icgem.gfz-potsdam.de', timeout=TIMEOUT)
+    ftp.login()
+
     # find files for a particular mission
     logging.info(f'{MISSION} Spherical Harmonics:')
 
     # Sync gravity field dealiasing products
     for ds in DSET[MISSION]:
         # print string of exact data product
         logging.info(f'{MISSION}/{RELEASE}/{ds}')
         # local directory for exact data product
-        local_dir = os.path.join(DIRECTORY,LOCAL[MISSION],RELEASE,ds)
+        local_dir = DIRECTORY.joinpath(LOCAL[MISSION], RELEASE, ds)
         # check if directory exists and recursively create if not
-        if not os.path.exists(local_dir):
-            os.makedirs(local_dir,MODE)
+        local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
         # compile the regular expression operator to find files
         R1 = compile_regex_pattern(MISSION, ds)
         # set the remote path to download files
         if ds in ('GAA','GAB','GAC','GAD') and (MISSION == 'Swarm'):
             remote_path = [ftp.host,'02_COST-G',MISSION,'GAX_products',ds]
         elif ds in ('GAA','GAB','GAC','GAD') and (MISSION != 'Swarm'):
             remote_path = [ftp.host,'02_COST-G',MISSION,'GAX_products']
@@ -133,106 +135,106 @@
         remote_files,remote_mtimes = gravtk.utilities.ftp_list(
             remote_path, timeout=TIMEOUT, basename=True, pattern=R1,
             sort=True)
         # download the file from the ftp server
         for fi,remote_mtime in zip(remote_files,remote_mtimes):
             # remote and local versions of the file
             remote_path.append(fi)
-            local_file = os.path.join(local_dir,fi)
+            local_file = local_dir.joinpath(fi)
             ftp_mirror_file(ftp, remote_path, remote_mtime,
                 local_file, TIMEOUT=TIMEOUT, LIST=LIST,
                 CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
             # remove the file from the remote path list
             remote_path.remove(fi)
         # find local GRACE/GRACE-FO/Swarm files to create index
-        grace_files=[fi for fi in os.listdir(local_dir) if R1.match(fi)]
+        grace_files = sorted([f.name for f in local_dir.iterdir()
+            if R1.match(f.name)])
         # write each file to an index
-        index_file = os.path.join(local_dir,'index.txt')
-        with open(index_file, mode='w', encoding='utf8') as fid:
+        index_file = local_dir.joinpath('index.txt')
+        with index_file.open(mode='w', encoding='utf8') as fid:
             # output GRACE/GRACE-FO/Swarm filenames to index
             for fi in sorted(grace_files):
                 print(fi, file=fid)
         # change permissions of index file
-        os.chmod(index_file, MODE)
+        index_file.chmod(mode=MODE)
 
     # close the ftp connection
     ftp.quit()
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def ftp_mirror_file(ftp,remote_path,remote_mtime,local_file,
     TIMEOUT=None,LIST=False,CLOBBER=False,CHECKSUM=False,MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if CHECKSUM and os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if CHECKSUM and local_file.exists():
         # generate checksum hash for local file
         # open the local_file in binary read mode
-        with open(local_file, 'rb') as local_buffer:
-            local_hash = hashlib.md5(local_buffer.read()).hexdigest()
+        local_hash = gravtk.utilities.get_hash(local_file)
         # copy remote file contents to bytesIO object
         remote_buffer = gravtk.utilities.from_ftp(remote_path,
             timeout=TIMEOUT)
         # generate checksum hash for remote file
         remote_hash = hashlib.md5(remote_buffer.getvalue()).hexdigest()
         # compare checksums
         if (local_hash != remote_hash):
             TEST = True
             OVERWRITE = f' (checksums: {local_hash} {remote_hash})'
-    elif os.access(local_file, os.F_OK):
+    elif local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         remote_ftp_url = posixpath.join('ftp://',*remote_path)
         logging.info(f'{remote_ftp_url} -->')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # if executing copy command (not only printing the files)
         if not LIST:
             # copy file from ftp server or from bytesIO object
-            if CHECKSUM and os.access(local_file, os.F_OK):
+            if CHECKSUM and local_file.exists():
                 # store bytes to file using chunked transfer encoding
                 remote_buffer.seek(0)
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     shutil.copyfileobj(remote_buffer, f, 16 * 1024)
             else:
                 # path to remote file
                 remote_file = posixpath.join(*remote_path[1:])
                 # copy remote file contents to local file
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     ftp.retrbinary(f'RETR {remote_file}', f.write)
             # keep remote modification time of file and local access time
-            os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-            os.chmod(local_file, MODE)
+            os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+            local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs GRACE/GRACE-FO/Swarm COST-G data from the
             GFZ International Centre for Global Earth Models (ICGEM)
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # mission (GRACE, GRACE Follow-On or Swarm)
     choices = ['Grace','Grace-FO','Swarm']
     parser.add_argument('--mission','-m',
         type=str, nargs='+',
         default=['Grace','Grace-FO','Swarm'], choices=choices,
         help='Mission to sync between GRACE, GRACE-FO and Swarm')
```

### Comparing `gravity-toolkit-1.2.0/scripts/gfz_isdc_dealiasing_ftp.py` & `gravity-toolkit-1.2.1/scripts/gfz_isdc_dealiasing_ftp.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gfz_isdc_dealiasing_ftp.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Syncs GRACE Level-1b dealiasing products from the GFZ Information
     System and Data Center (ISDC)
 Optionally outputs as monthly tar files
 
 CALLING SEQUENCE:
     python gfz_isdc_dealiasing_ftp.py --year=2015 --release=RL06 --tar
 
@@ -26,14 +26,16 @@
     lxml: processing XML and HTML in Python
         https://pypi.python.org/pypi/lxml
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for syncing files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: increase default year range to sync
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 10/2021: using python logging for handling verbose output
     Updated 07/2021: added option to sync only specific months
     Updated 05/2021: added option for connection timeout (in seconds)
     Updated 01/2021: using utilities module to list files from ftp
@@ -48,68 +50,70 @@
 
 import sys
 import os
 import re
 import time
 import ftplib
 import logging
+import pathlib
 import tarfile
 import argparse
 import posixpath
 import gravity_toolkit as gravtk
 
 # PURPOSE: syncs GRACE Level-1b dealiasing products from the GFZ data server
 # and optionally outputs as monthly tar files
 def gfz_isdc_dealiasing_ftp(base_dir, DREL, YEAR=None, MONTHS=None, TAR=False,
     TIMEOUT=None, LOG=False, CLOBBER=False, MODE=None):
-    # output data directory
-    grace_dir = os.path.join(base_dir,'AOD1B',DREL)
-    os.makedirs(grace_dir) if not os.access(grace_dir,os.F_OK) else None
+    # check if directory exists and recursively create if not
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+    grace_dir = base_dir.joinpath('AOD1B',DREL)
+    grace_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: GFZ_AOD1B_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'GFZ_AOD1B_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(base_dir,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = base_dir.joinpath(f'GFZ_AOD1B_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'GFZ AOD1b Sync Log ({today})')
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
     # remote HOST for DREL on GFZ data server
     # connect and login to GFZ ftp server
-    ftp = ftplib.FTP('isdcftp.gfz-potsdam.de',timeout=TIMEOUT)
+    ftp = ftplib.FTP('isdcftp.gfz-potsdam.de', timeout=TIMEOUT)
     ftp.login()
 
     # compile regular expression operator for years to sync
     if YEAR is None:
         regex_years = r'\d{4}'
     else:
         regex_years = r'|'.join(rf'{y:d}' for y in YEAR)
     # compile regular expression operator for years to sync
     R1 = re.compile(rf'({regex_years})', re.VERBOSE)
     # suffix for each data release
-    SUFFIX = dict(RL04='tar.gz',RL05='tar.gz',RL06='tgz')
+    SUFFIX = dict(RL04='tar.gz', RL05='tar.gz', RL06='tgz')
 
     # find remote yearly directories for DREL
     YRS,_ = gravtk.utilities.ftp_list([ftp.host,'grace',
         'Level-1B', 'GFZ','AOD',DREL], timeout=TIMEOUT, basename=True,
         pattern=R1, sort=True)
     # for each year
     for Y in YRS:
         # for each month of interest
         for M in MONTHS:
             # output tar file for year and month
             args = (Y, M, DREL.replace('RL',''), SUFFIX[DREL])
             FILE = 'AOD1B_{0}-{1:02d}_{2}.{3}'.format(*args)
             # check if output tar file exists (if TAR)
-            local_tar_file = os.path.join(grace_dir,FILE)
-            TEST = not os.access(local_tar_file, os.F_OK)
+            local_tar_file = grace_dir.joinpath(FILE)
+            TEST = not local_tar_file.exists()
             # compile regular expressions operators for file dates
             # will extract year and month and calendar day from the ascii file
             regex_pattern = r'AOD1B_({0})-({1:02d})-(\d+)_X_\d+.asc.gz$'
             R2 = re.compile(regex_pattern.format(Y,M), re.VERBOSE)
             remote_files,remote_mtimes = gravtk.utilities.ftp_list(
                 [ftp.host,'grace','Level-1B','GFZ','AOD',DREL,Y],
                 timeout=TIMEOUT, basename=True, pattern=R2, sort=True)
@@ -129,43 +133,44 @@
                     tar_info = tarfile.TarInfo(name=fi)
                     tar_info.mtime = remote_mtime
                     tar_info.size = remote_buffer.getbuffer().nbytes
                     tar.addfile(tarinfo=tar_info, fileobj=remote_buffer)
                 # close tar file and set permissions level to MODE
                 tar.close()
                 logging.info(f' --> {local_tar_file}\n')
-                os.chmod(local_tar_file, MODE)
+                local_tar_file.chmod(mode=MODE)
             elif (file_count > 0) and not TAR:
                 # copy each gzip file and keep as individual daily files
                 for fi,remote_mtime in zip(remote_files,remote_mtimes):
                     # remote and local version of each input file
                     remote = [ftp.host,'grace','Level-1B','GFZ','AOD',DREL,Y,fi]
-                    local = os.path.join(grace_dir,fi)
-                    ftp_mirror_file(ftp,remote,remote_mtime,local,
-                        CLOBBER=CLOBBER,MODE=MODE)
+                    local_file = grace_dir.joinpath(fi)
+                    ftp_mirror_file(ftp,remote,remote_mtime,local_file,
+                        CLOBBER=CLOBBER, MODE=MODE)
 
     # close the ftp connection
     ftp.quit()
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(base_dir,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def ftp_mirror_file(ftp,remote_path,remote_mtime,local_file,
     CLOBBER=False,MODE=0o775):
     # path to remote file
     remote_file = posixpath.join(*remote_path[1:])
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
@@ -173,41 +178,40 @@
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         remote_ftp_url = posixpath.join('ftp://',*remote_path)
         logging.info(f'{remote_ftp_url} -->')
         logging.info(f'\t{local_file}{OVERWRITE}\n')
         # copy remote file contents to local file
-        with open(local_file, 'wb') as f:
+        with local_file.open(mode='wb') as f:
             ftp.retrbinary(f'RETR {remote_file}', f.write)
         # keep remote modification time of file and local access time
-        os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-        os.chmod(local_file, MODE)
+        os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+        local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs GRACE Level-1b dealiasing products from
             the GFZ Information System and Data Center (ISDC)
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
         metavar='DREL', type=str, nargs='+',
         default=['RL06'], choices=['RL04','RL05','RL06'],
         help='GRACE/GRACE-FO data release')
     # years to download
     parser.add_argument('--year','-Y',
-        type=int, nargs='+', default=range(2000,2021),
+        type=int, nargs='+', default=range(2000,2023),
         help='Years of data to sync')
     # months to download
     parser.add_argument('--month','-m',
         type=int, nargs='+', default=range(1,13),
         help='Months of data to sync')
     # output dealiasing files as monthly tar files
     parser.add_argument('--tar','-T',
```

### Comparing `gravity-toolkit-1.2.0/scripts/gfz_isdc_grace_ftp.py` & `gravity-toolkit-1.2.1/scripts/gfz_isdc_grace_ftp.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 gfz_isdc_grace_ftp.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Syncs GRACE/GRACE-FO data from the GFZ Information System and Data Center (ISDC)
 Syncs CSR/GFZ/JPL files for RL06 GAA/GAB/GAC/GAD/GSM
     GAA and GAB are GFZ/JPL only
 Gets the latest technical note (TN) files
 Gets the monthly GRACE/GRACE-FO newsletters
 
 CALLING SEQUENCE:
@@ -36,14 +36,15 @@
     lxml: processing XML and HTML in Python
         https://pypi.python.org/pypi/lxml
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for syncing files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 10/2022: fix version check for mission
     Updated 08/2022: moved regular expression function to utilities
         Dynamically select newest version of granules for index
     Updated 04/2022: added option for GRACE/GRACE-FO Level-2 data version
         sync GRACE/GRACE-FO technical notes and newsletters
@@ -68,137 +69,136 @@
 import re
 import copy
 import time
 import ftplib
 import shutil
 import hashlib
 import logging
+import pathlib
 import argparse
 import posixpath
 import gravity_toolkit as gravtk
 
 # PURPOSE: sync local GRACE/GRACE-FO files with GFZ ISDC server
 def gfz_isdc_grace_ftp(DIRECTORY, PROC=[], DREL=[], VERSION=[],
     NEWSLETTERS=False, TIMEOUT=None, LOG=False, LIST=False,
     CLOBBER=False, CHECKSUM=False, MODE=None):
 
-    # connect and login to GFZ ISDC ftp server
-    ftp = ftplib.FTP('isdcftp.gfz-potsdam.de', timeout=TIMEOUT)
-    ftp.login()
-
     # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # mission shortnames
     shortname = {'grace':'GRAC', 'grace-fo':'GRFO'}
     # datasets for each processing center
     DSET = {}
     DSET['CSR'] = ['GAC', 'GAD', 'GSM']
     DSET['GFZ'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
     DSET['JPL'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
 
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: GFZ_ISDC_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'GFZ_ISDC_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'GFZ_ISDC_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'GFZ ISDC Sync Log ({today})')
         logging.info('CENTERS={0}'.format(','.join(PROC)))
         logging.info('RELEASES={0}'.format(','.join(DREL)))
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
+    # connect and login to GFZ ISDC ftp server
+    ftp = ftplib.FTP('isdcftp.gfz-potsdam.de', timeout=TIMEOUT)
+    ftp.login()
+
     # Degree 1 (geocenter) coefficients
     logging.info('Degree 1 Coefficients:')
-    local_dir = os.path.join(DIRECTORY,'geocenter')
+    local_dir = DIRECTORY.joinpath('geocenter')
     # check if geocenter directory exists and recursively create if not
-    os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+    local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
     # TN-13 JPL degree 1 files
     # compile regular expression operator for remote files
     R1 = re.compile(r'TN-13_GEOC_(CSR|GFZ|JPL)_(.*?).txt$', re.VERBOSE)
     # get filenames from remote directory
     remote_files,remote_mtimes = gravtk.utilities.ftp_list(
         [ftp.host,'grace-fo','DOCUMENTS','TECHNICAL_NOTES'],
         timeout=TIMEOUT, basename=True, pattern=R1, sort=True)
     # for each file on the remote server
     for fi,remote_mtime in zip(remote_files,remote_mtimes):
         # extract filename from regex object
         remote_path = [ftp.host,'grace-fo','DOCUMENTS','TECHNICAL_NOTES',fi]
-        local_file = os.path.join(local_dir,fi)
+        local_file = local_dir.joinpath(fi)
         ftp_mirror_file(ftp, remote_path, remote_mtime,
             local_file, TIMEOUT=TIMEOUT, LIST=LIST,
             CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
 
     # SLR C2,0 coefficients
     logging.info('C2,0 Coefficients:')
-    local_dir = os.path.expanduser(DIRECTORY)
     # compile regular expression operator for remote files
     R1 = re.compile(r'TN-(05|07|11)_C20_SLR_RL(.*?).txt$', re.VERBOSE)
     # get filenames from remote directory
     remote_files,remote_mtimes = gravtk.utilities.ftp_list(
         [ftp.host,'grace','DOCUMENTS','TECHNICAL_NOTES'],
         timeout=TIMEOUT, basename=True, pattern=R1, sort=True)
     # for each file on the remote server
     for fi,remote_mtime in zip(remote_files,remote_mtimes):
         # extract filename from regex object
         remote_path = [ftp.host,'grace','DOCUMENTS','TECHNICAL_NOTES',fi]
-        local_file = os.path.join(local_dir,re.sub(r'(_RL.*?).txt','.txt',fi))
+        local_file = DIRECTORY.joinpath(re.sub(r'(_RL.*?).txt','.txt',fi))
         ftp_mirror_file(ftp, remote_path, remote_mtime,
             local_file, TIMEOUT=TIMEOUT, LIST=LIST,
             CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
 
     # SLR C3,0 coefficients
     logging.info('C3,0 Coefficients:')
-    local_dir = os.path.expanduser(DIRECTORY)
     # compile regular expression operator for remote files
     R1 = re.compile(r'TN-(14)_C30_C20_SLR_GSFC.txt$', re.VERBOSE)
     # get filenames from remote directory
     remote_files,remote_mtimes = gravtk.utilities.ftp_list(
         [ftp.host,'grace-fo','DOCUMENTS','TECHNICAL_NOTES'],
         timeout=TIMEOUT, basename=True, pattern=R1, sort=True)
     # for each file on the remote server
     for fi,remote_mtime in zip(remote_files,remote_mtimes):
         # extract filename from regex object
         remote_path = [ftp.host,'grace-fo','DOCUMENTS','TECHNICAL_NOTES',fi]
-        local_file = os.path.join(local_dir,re.sub(r'(SLR_GSFC)','GSFC_SLR',fi))
+        local_file = DIRECTORY.joinpath(re.sub(r'(SLR_GSFC)','GSFC_SLR',fi))
         ftp_mirror_file(ftp, remote_path, remote_mtime,
             local_file, TIMEOUT=TIMEOUT, LIST=LIST,
             CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
 
     # TN-08 GAE, TN-09 GAF and TN-10 GAG ECMWF atmosphere correction products
     logging.info('TN-08 GAE, TN-09 GAF and TN-10 GAG products:')
-    local_dir = os.path.expanduser(DIRECTORY)
     ECMWF_files = []
     ECMWF_files.append('TN-08_GAE-2_2006032-2010031_0000_EIGEN_G---_0005.gz')
     ECMWF_files.append('TN-09_GAF-2_2010032-2015131_0000_EIGEN_G---_0005.gz')
     ECMWF_files.append('TN-10_GAG-2_2015132-2099001_0000_EIGEN_G---_0005.gz')
     # compile regular expression operator for remote files
     R1 = re.compile(r'({0}|{1}|{2})'.format(*ECMWF_files), re.VERBOSE)
     # get filenames from remote directory
     remote_files,remote_mtimes = gravtk.utilities.ftp_list(
         [ftp.host,'grace','DOCUMENTS','TECHNICAL_NOTES'],
         timeout=TIMEOUT, basename=True, pattern=R1, sort=True)
     # for each file on the remote server
     for fi,remote_mtime in zip(remote_files,remote_mtimes):
         # extract filename from regex object
         remote_path = [ftp.host,'grace','DOCUMENTS','TECHNICAL_NOTES',fi]
-        local_file = os.path.join(local_dir,fi)
+        local_file = DIRECTORY.joinpath(fi)
         ftp_mirror_file(ftp, remote_path, remote_mtime,
             local_file, TIMEOUT=TIMEOUT, LIST=LIST,
             CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
 
     # GRACE and GRACE-FO newsletters
     if NEWSLETTERS:
         # local newsletter directory (place GRACE and GRACE-FO together)
-        local_dir = os.path.join(DIRECTORY,'newsletters')
+        local_dir = DIRECTORY.joinpath('newsletters')
         # check if newsletters directory exists and recursively create if not
-        os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+        local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
         # for each satellite mission (grace, grace-fo)
         for i,mi in enumerate(['grace','grace-fo']):
             logging.info(f'{mi} Newsletters:')
             # compile regular expression operator for remote files
             NAME = mi.upper().replace('-','_')
             R1 = re.compile(rf'{NAME}_SDS_NL_(\d+).pdf', re.VERBOSE)
             # find years for GRACE/GRACE-FO newsletters
@@ -213,32 +213,31 @@
                     [ftp.host,mi,'DOCUMENTS','NEWSLETTER',Y],
                     timeout=TIMEOUT, basename=True, pattern=R1,
                     sort=True)
                 # for each file on the remote server
                 for fi,remote_mtime in zip(remote_files,remote_mtimes):
                     # extract filename from regex object
                     remote_path = [ftp.host,mi,'DOCUMENTS','NEWSLETTER',Y,fi]
-                    local_file = os.path.join(local_dir,fi)
+                    local_file = local_dir.joinpath(fi)
                     ftp_mirror_file(ftp, remote_path, remote_mtime,
                         local_file, TIMEOUT=TIMEOUT, LIST=LIST,
                         CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
 
     # GRACE/GRACE-FO level-2 spherical harmonic products
     logging.info('GRACE/GRACE-FO L2 Global Spherical Harmonics:')
     # for each processing center (CSR, GFZ, JPL)
     for pr in PROC:
         # for each data release (RL04, RL05, RL06)
         for rl in DREL:
             # for each level-2 product (GAC, GAD, GSM, GAA, GAB)
             for ds in DSET[pr]:
                 # local directory for exact data product
-                local_dir = os.path.join(DIRECTORY, pr, rl, ds)
+                local_dir = DIRECTORY.joinpath(pr, rl, ds)
                 # check if directory exists and recursively create if not
-                if not os.path.exists(local_dir):
-                    os.makedirs(local_dir,MODE)
+                local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
                 # list of GRACE/GRACE-FO files for index
                 grace_files = []
                 # for each satellite mission (grace, grace-fo)
                 for i,mi in enumerate(['grace','grace-fo']):
                     # modifiers for intermediate data releases
                     if (int(VERSION[i]) > 0):
                         drel_str = f'{rl}.{VERSION[i]}'
@@ -251,111 +250,111 @@
                     # get filenames from remote directory
                     remote_files,remote_mtimes = gravtk.utilities.ftp_list(
                         [ftp.host,mi,'Level-2',pr,drel_str], timeout=TIMEOUT,
                         basename=True, pattern=R1, sort=True)
                     for fi,remote_mtime in zip(remote_files,remote_mtimes):
                         # extract filename from regex object
                         remote_path = [ftp.host,mi,'Level-2',pr,drel_str,fi]
-                        local_file = os.path.join(local_dir,fi)
+                        local_file = local_dir.joinpath(fi)
                         ftp_mirror_file(ftp, remote_path, remote_mtime,
                             local_file, TIMEOUT=TIMEOUT, LIST=LIST,
                             CLOBBER=CLOBBER, CHECKSUM=CHECKSUM, MODE=MODE)
                     # regular expression operator for data product
                     rx = gravtk.utilities.compile_regex_pattern(
                         pr, rl, ds, mission=shortname[mi])
                     # find local GRACE/GRACE-FO files to create index
-                    granules = [f for f in os.listdir(local_dir) if rx.match(f)]
+                    granules = sorted([f.name for f in local_dir.iterdir()
+                        if rx.match(f.name)])
                     # reduce list of GRACE/GRACE-FO files to unique dates
                     granules = gravtk.time.reduce_by_date(granules)
                     # extend list of GRACE/GRACE-FO files with granules
                     grace_files.extend(granules)
 
                 # outputting GRACE/GRACE-FO filenames to index
-                index_file = os.path.join(local_dir,'index.txt')
-                with open(index_file, mode='w', encoding='utf8') as fid:
+                index_file = local_dir.joinpath('index.txt')
+                with index_file.open(mode='w', encoding='utf8') as fid:
                     for fi in sorted(grace_files):
                         print(fi, file=fid)
                 # change permissions of index file
-                os.chmod(index_file, MODE)
+                index_file.chmod(mode=MODE)
 
     # close the ftp connection
     ftp.quit()
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def ftp_mirror_file(ftp,remote_path,remote_mtime,local_file,
     TIMEOUT=None,LIST=False,CLOBBER=False,CHECKSUM=False,MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if CHECKSUM and os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if CHECKSUM and local_file.exists():
         # generate checksum hash for local file
         # open the local_file in binary read mode
-        with open(local_file, 'rb') as local_buffer:
-            local_hash = hashlib.md5(local_buffer.read()).hexdigest()
+        local_hash = gravtk.utilities.get_hash(local_file)
         # copy remote file contents to bytesIO object
         remote_buffer = gravtk.utilities.from_ftp(remote_path,
             timeout=TIMEOUT)
         # generate checksum hash for remote file
         remote_hash = hashlib.md5(remote_buffer.getvalue()).hexdigest()
         # compare checksums
         if (local_hash != remote_hash):
             TEST = True
             OVERWRITE = f' (checksums: {local_hash} {remote_hash})'
-    elif os.access(local_file, os.F_OK):
+    elif local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         remote_ftp_url = posixpath.join('ftp://',*remote_path)
         logging.info(f'{remote_ftp_url} -->')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # if executing copy command (not only printing the files)
         if not LIST:
             # copy file from ftp server or from bytesIO object
-            if CHECKSUM and os.access(local_file, os.F_OK):
+            if CHECKSUM and local_file.exists():
                 # store bytes to file using chunked transfer encoding
                 remote_buffer.seek(0)
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     shutil.copyfileobj(remote_buffer, f, 16 * 1024)
             else:
                 # path to remote file
                 remote_file = posixpath.join(*remote_path[1:])
                 # copy remote file contents to local file
-                with open(local_file, 'wb') as f:
+                with local_file.open(mode='wb') as f:
                     ftp.retrbinary(f'RETR {remote_file}', f.write)
             # keep remote modification time of file and local access time
-            os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-            os.chmod(local_file, MODE)
+            os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+            local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs GRACE/GRACE-FO data from the GFZ
             Information System and Data Center (ISDC)
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'], choices=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO processing center')
     # GRACE/GRACE-FO data release
```

### Comparing `gravity-toolkit-1.2.0/scripts/grace_mean_harmonics.py` & `gravity-toolkit-1.2.1/scripts/grace_mean_harmonics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 grace_mean_harmonics.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates the temporal mean of the GRACE/GRACE-FO spherical harmonics
     for a given date range from a set of parameters
 
 COMMAND LINE OPTIONS:
     -D X, --directory X: GRACE/GRACE-FO working data directory
     -c X, --center X: GRACE/GRACE-FO processing center
@@ -68,15 +68,17 @@
         Replaces low-degree harmonics with SLR values (if specified)
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: add root attributes to output netCDF4 and HDF5 files
+        convert shape and ndim to harmonic class properties
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
         option to specify a specific geocenter correction file
     Updated 11/2021: add GSFC low-degree harmonics
@@ -110,23 +112,24 @@
 """
 from __future__ import print_function
 
 import sys
 import os
 import time
 import logging
+import pathlib
 import argparse
 import numpy as np
 import traceback
 import collections
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: import GRACE/GRACE-FO files for a given months range
@@ -148,14 +151,17 @@
     SLR_C40=None,
     SLR_C50=None,
     MEAN_FILE=None,
     MEANFORM=None,
     VERBOSE=0,
     MODE=0o775):
 
+    # input directory setup
+    base_dir = pathlib.Path(base_dir).expanduser().absolute()
+
     # attributes for output files
     attributes = collections.OrderedDict()
     attributes['generating_institute'] = PROC
     attributes['product_release'] = DREL
     attributes['product_name'] = DSET
     attributes['product_type'] = 'gravity_field'
 
@@ -189,39 +195,39 @@
     nt = grace_Ylms.shape[-1]
     # calculate RMS of harmonic errors
     mean_Ylms.eclm = np.sqrt(np.sum(input_Ylms['eclm']**2,axis=2)/nt)
     mean_Ylms.eslm = np.sqrt(np.sum(input_Ylms['eslm']**2,axis=2)/nt)
 
     # default output filename if not entering via parameter file
     if not MEAN_FILE:
-        DIRECTORY = os.path.expanduser(input_Ylms['directory'])
+        DIRECTORY = pathlib.Path(input_Ylms['directory']).expanduser().absolute()
         args = (PROC,DREL,DSET,grace_str,LMAX,order_str,START,END,suffix)
         file_format = '{0}_{1}_{2}_MEAN_CLM{3}_L{4:d}{5}_{6:03d}-{7:03d}.{8}'
-        MEAN_FILE = os.path.join(DIRECTORY,file_format.format(*args))
+        MEAN_FILE = DIRECTORY.joinpath(file_format.format(*args))
     else:
-        DIRECTORY = os.path.dirname(MEAN_FILE)
+        MEAN_FILE = pathlib.Path(MEAN_FILE).expanduser().absolute()
+        DIRECTORY = MEAN_FILE.parent
     # recursively create output directory if non-existent
-    if not os.access(DIRECTORY, os.F_OK):
-        os.makedirs(DIRECTORY, MODE)
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # output spherical harmonics for the static field
     if (MEANFORM == 'gfc'):
         # output mean field to gfc format
         mean_Ylms.attributes['ROOT'] = attributes
         mean_Ylms.to_gfc(MEAN_FILE, verbose=VERBOSE)
     else:
         # add attributes from input GRACE fields
         attributes.update(input_Ylms.get('attributes'))
-        attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+        attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
         # output mean field to specified file format
         mean_Ylms.attributes['ROOT'] = attributes
         mean_Ylms.to_file(MEAN_FILE, format=MEANFORM,
             verbose=VERBOSE)
     # change the permissions mode
-    os.chmod(MEAN_FILE, MODE)
+    MEAN_FILE.chmod(mode=MODE)
 
     # return the output file
     return MEAN_FILE
 
 # PURPOSE: additional routines for the harmonics module
 class mean(gravtk.harmonics):
     def __init__(self, **kwargs):
@@ -234,39 +240,39 @@
 
     def from_harmonics(self, temp):
         """
         Convert a harmonics object to a new mean object
         """
         self = mean(lmax=temp.lmax, mmax=temp.mmax)
         # try to assign variables to self
-        for key in ['clm','slm','eclm','eslm','shape','ndim','filename',
+        for key in ['clm','slm','eclm','eslm','filename',
             'center','release','product']:
             try:
                 val = getattr(temp, key)
                 setattr(self, key, np.copy(val))
             except AttributeError:
                 pass
-        # assign ndim and shape attributes
+        # assign degree and order fields
         self.update_dimensions()
         return self
 
     def to_gfc(self, filename, **kwargs):
         """
         Write a harmonics object to gfc file
         Inputs: full path of output gfc file
         Options:
             harmonics objects contain date information
             keyword arguments for gfc output
         """
-        self.filename = os.path.expanduser(filename)
+        self.filename = pathlib.Path(filename).expanduser().absolute()
         # set default verbosity
         kwargs.setdefault('verbose',False)
-        logging.info(self.filename)
+        logging.info(str(self.filename))
         # open the output file
-        fid = open(self.filename, mode='w', encoding='utf8')
+        fid = self.filename.open(mode='w', encoding='utf8')
         # print the header informat
         self.print_header(fid)
         # output file format
         file_format = ('{0:3} {1:4d} {2:4d} {3:+18.12E} {4:+18.12E}  '
             '{5:11.5E}  {6:11.5E}')
         # write to file for each spherical harmonic degree and order
         for m in range(0, self.mmax+1):
@@ -294,16 +300,16 @@
 
 # PURPOSE: print a file log for the GRACE/GRACE-FO mean program
 def output_log_file(input_arguments, output_file):
     # format: GRACE_mean_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_mean_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILE:')
@@ -313,16 +319,16 @@
 
 # PURPOSE: print a error file log for the GRACE/GRACE-FO mean program
 def output_error_log_file(input_arguments):
     # format: GRACE_mean_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_mean_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -338,16 +344,15 @@
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
@@ -396,15 +401,15 @@
     # GFZ: GRACE/GRACE-FO coefficients from GFZ GravIS
     #     http://gravis.gfz-potsdam.de/corrections
     parser.add_argument('--geocenter',
         metavar='DEG1', type=str,
         choices=['Tellus','SLR','SLF','UCI','Swenson','GFZ'],
         help='Update Degree 1 coefficients with SLR or derived values')
     parser.add_argument('--geocenter-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Specific geocenter file if not default')
     parser.add_argument('--interpolate-geocenter',
         default=False, action='store_true',
         help='Least-squares model missing Degree 1 coefficients')
     # replace low degree harmonics with values from Satellite Laser Ranging
     parser.add_argument('--slr-c20',
         type=str, default=None, choices=['CSR','GFZ','GSFC'],
@@ -422,15 +427,15 @@
         type=str, default=None, choices=['CSR','GSFC','LARES'],
         help='Replace C40 coefficients with SLR values')
     parser.add_argument('--slr-c50',
         type=str, default=None, choices=['CSR','GSFC','LARES'],
         help='Replace C50 coefficients with SLR values')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Output GRACE/GRACE-FO mean file')
     # input data format (ascii, netCDF4, HDF5, gfc)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Output data format for GRACE/GRACE-FO mean file')
     # Output log file for each job in forms
     # GRACE_mean_run_2002-04-01_PID-00000.log
```

### Comparing `gravity-toolkit-1.2.0/scripts/grace_spatial_error.py` & `gravity-toolkit-1.2.1/scripts/grace_spatial_error.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 grace_spatial_error.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates the GRACE/GRACE-FO errors following Wahr et al. (2006)
 
 Spatial output units: cm w.e., mm geoid height, mm elastic uplift,
     microgal gravity perturbation or surface pressure (mbar)
 
 COMMAND LINE OPTIONS:
@@ -115,15 +115,17 @@
 
 REFERENCES:
     J Wahr, S C Swenson, I Velicogna, "Accuracy of GRACE mass estimates",
         Geophysical Research Letters, 33(6), L06401, (2006)
         http://dx.doi.org/10.1029/2005GL025305
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: add root attributes to output netCDF4 and HDF5 files
+        use attributes from units class for writing to netCDF4/HDF5 files
     Updated 02/2023: use get function to retrieve specific units
         use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
         refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
@@ -163,23 +165,24 @@
 
 import sys
 import os
 import re
 import copy
 import time
 import logging
+import pathlib
 import numpy as np
 import argparse
 import traceback
 import collections
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: import GRACE files for a given months range
@@ -213,16 +216,16 @@
     MEANFORM=None,
     OUTPUT_DIRECTORY=None,
     FILE_PREFIX=None,
     VERBOSE=0,
     MODE=0o775):
 
     # recursively create output directory if not currently existing
-    if not os.access(OUTPUT_DIRECTORY, os.F_OK):
-        os.makedirs(OUTPUT_DIRECTORY, mode=MODE, exist_ok=True)
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # output attributes for spatial files
     attributes = collections.OrderedDict()
     attributes['generating_institute'] = PROC
     attributes['product_release'] = DREL
     attributes['product_name'] = DSET
     attributes['product_type'] = 'gravity_field'
@@ -274,55 +277,56 @@
     # add attributes for input GRACE/GRACE-FO spherical harmonics
     for att_name, att_val in Ylms['attributes'].items():
         attributes[att_name] = att_val
 
     # use a mean file for the static field to remove
     if MEAN_FILE:
         # read data form for input mean file (ascii, netCDF4, HDF5, gfc)
+        MEAN_FILE = pathlib.Path(MEAN_FILE).expanduser().absolute()
         mean_Ylms = gravtk.harmonics().from_file(MEAN_FILE,
             format=MEANFORM, date=False)
         # remove the input mean
         GRACE_Ylms.subtract(mean_Ylms)
-        attributes['lineage'].append(os.path.basename(MEAN_FILE))
+        attributes['lineage'].append(MEAN_FILE.name)
     else:
         GRACE_Ylms.mean(apply=True)
 
     # filter GRACE/GRACE-FO coefficients
     if DESTRIPE:
         # destriping GRACE/GRACE-FO coefficients
         ds_str = '_FL'
         GRACE_Ylms = GRACE_Ylms.destripe()
         attributes['filtering'] = 'Destriped'
     else:
         # using standard GRACE/GRACE-FO harmonics
         ds_str = ''
 
     # full path to directory for specific GRACE/GRACE-FO product
-    GRACE_Ylms.directory = Ylms['directory']
+    GRACE_Ylms.directory = pathlib.Path(Ylms['directory']).expanduser().absolute()
     # default file prefix
     if not FILE_PREFIX:
         FILE_PREFIX = '{0}_{1}_{2}{3}_'.format(PROC,DREL,DSET,Ylms['title'])
 
     # calculating GRACE error (Wahr et al 2006)
     # output GRACE error file (for both LMAX==MMAX and LMAX != MMAX cases)
     fargs = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GRACE_Ylms.month[0],
         GRACE_Ylms.month[-1],suffix[DATAFORM])
     delta_format = '{0}_{1}_{2}_DELTA_CLM_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
-    DELTA_FILE = os.path.join(GRACE_Ylms.directory,delta_format.format(*fargs))
-    # full path of the GRACE directory
-    # if file was previously calculated, will read file
-    # else will calculate the GRACE error
-    if not os.access(DELTA_FILE, os.F_OK):
+    DELTA_FILE = GRACE_Ylms.directory.joinpath(delta_format.format(*fargs))
+    # check full path of the GRACE directory for delta file
+    # if file was previously calculated: will read file
+    # else: will calculate the GRACE/GRACE-FO error
+    if not DELTA_FILE.exists():
         # add output delta file to list object
         output_files.append(DELTA_FILE)
 
         # Delta coefficients of GRACE time series (Error components)
-        delta_Ylms = gravtk.harmonics(lmax=LMAX,mmax=MMAX)
-        delta_Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-        delta_Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+        delta_Ylms = gravtk.harmonics(lmax=LMAX, mmax=MMAX)
+        delta_Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+        delta_Ylms.slm = np.zeros((LMAX+1, MMAX+1))
         # Smoothing Half-Width (CNES is a 10-day solution)
         # 365/10/2 = 18.25 (next highest is 19)
         # All other solutions are monthly solutions (HFWTH for annual = 6)
         if ((PROC == 'CNES') and (DREL in ('RL01','RL02'))):
             HFWTH = 19
         else:
             HFWTH = 6
@@ -345,29 +349,31 @@
                     # variance of data-(smoothed+annual+semi)
                     val2 = getattr(delta_Ylms, csharm)
                     val2[l,m] = np.sqrt(np.sum(smth['noise']**2)/nsmth)
 
         # attributes for output files
         kwargs = {}
         kwargs['title'] = 'GRACE/GRACE-FO Spherical Harmonic Errors'
-        kwargs['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+        kwargs['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
         # save GRACE/GRACE-FO delta harmonics to file
         delta_Ylms.time = np.copy(tsmth)
         delta_Ylms.month = np.int64(nsmth)
         delta_Ylms.to_file(DELTA_FILE, format=DATAFORM, **kwargs)
         # set the permissions mode of the output harmonics file
-        os.chmod(DELTA_FILE, MODE)
+        DELTA_FILE.chmod(mode=MODE)
         # append delta harmonics file to output files list
         output_files.append(DELTA_FILE)
     else:
-        # read GRACE DELTA spherical harmonics datafile
-        delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE, format=DATAFORM)
-        # truncate grace delta clm and slm to d/o LMAX/MMAX
+        # read GRACE/GRACE-FO delta harmonics from file
+        delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE,
+            format=DATAFORM)
+        # truncate GRACE/GRACE-FO delta clm and slm to d/o LMAX/MMAX
         delta_Ylms = delta_Ylms.truncate(lmax=LMAX, mmax=MMAX)
-        nsmth = np.int64(delta_Ylms.time)
+        tsmth = np.squeeze(delta_Ylms.time)
+        nsmth = np.int64(delta_Ylms.month)
 
     # Output spatial data object
     delta = gravtk.spatial()
     # Output Degree Spacing
     dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
     # Output Degree Interval
     if (INTERVAL == 1):
@@ -381,51 +387,38 @@
         delta.lon = np.arange(-180+dlon/2.0,180+dlon/2.0,dlon)
         delta.lat = np.arange(90.0-dlat/2.0,-90.0-dlat/2.0,-dlat)
         nlon = len(delta.lon)
         nlat = len(delta.lat)
     elif (INTERVAL == 3):
         # non-global grid set with BOUNDS parameter
         minlon,maxlon,minlat,maxlat = BOUNDS.copy()
-        delta.lon = np.arange(minlon+dlon/2.0,maxlon+dlon/2.0,dlon)
-        delta.lat = np.arange(maxlat-dlat/2.0,minlat-dlat/2.0,-dlat)
+        delta.lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+        delta.lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
         nlon = len(delta.lon)
         nlat = len(delta.lat)
 
-    # Earth Parameters
     # output spatial units
-    unit_list = ['cmwe', 'mmGH', 'mmCU', u'\u03BCGal', 'mbar']
-    unit_name = ['Equivalent_Water_Thickness', 'Geoid_Height',
-        'Elastic_Crustal_Uplift', 'Gravitational_Undulation',
-        'Equivalent_Surface_Pressure']
     # dfactor is the degree dependent coefficients
     # for specific spherical harmonic output units
     factors = gravtk.units(lmax=LMAX).harmonic(*LOVE)
-    if (UNITS == 1):
-        # 1: cmwe, centimeters water equivalent
-        dfactor = factors.get('cmwe')
-    elif (UNITS == 2):
-        # 2: mmGH, millimeters geoid height
-        dfactor = factors.get('mmGH')
-    elif (UNITS == 3):
-        # 3: mmCU, millimeters elastic crustal deformation
-        dfactor = factors.get('mmCU')
-    elif (UNITS == 4):
-        # 4: micGal, microGal gravity perturbations
-        dfactor = factors.get('microGal')
-    elif (UNITS == 5):
-        # 5: mbar, millibars equivalent surface pressure
-        dfactor = factors.get('mbar')
-    else:
-        raise ValueError(f'Invalid units code {UNITS:d}')
+    # 1: cmwe, centimeters water equivalent
+    # 2: mmGH, millimeters geoid height
+    # 3: mmCU, millimeters elastic crustal deformation
+    # 4: micGal, microGal gravity perturbations
+    # 5: mbar, millibars equivalent surface pressure
+    units = gravtk.units.bycode(UNITS)
+    dfactor = factors.get(units)
+    # output spatial units and descriptive units longname
+    units_name, units_longname = gravtk.units.get_attributes(units)
     # add attributes for earth parameters
     attributes['earth_radius'] = f'{factors.rad_e:0.3f} cm'
     attributes['earth_density'] = f'{factors.rho_e:0.3f} g/cm'
     attributes['earth_gravity_constant'] = f'{factors.GM:0.3f} cm^3/s^2'
     # add attributes to output spatial object
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
     delta.attributes['ROOT'] = attributes
 
     # Computing plms for converting to spatial domain
     phi = delta.lon[np.newaxis,:]*np.pi/180.0
     theta = (90.0 - delta.lat)*np.pi/180.0
     PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(theta))
     # square of legendre polynomials truncated to order MMAX
@@ -434,57 +427,54 @@
 
     # Calculating cos(m*phi)^2 and sin(m*phi)^2
     m = delta_Ylms.m[:,np.newaxis]
     ccos = np.cos(np.dot(m,phi))**2
     ssin = np.sin(np.dot(m,phi))**2
 
     # truncate delta harmonics to spherical harmonic range
-    Ylms = delta_Ylms.truncate(LMAX,lmin=LMIN,mmax=MMAX)
+    Ylms = delta_Ylms.truncate(LMAX, lmin=LMIN, mmax=MMAX)
     # convolve delta harmonics with degree dependent factors
     # smooth harmonics and convert to output units
     Ylms = Ylms.convolve(dfactor*wt).power(2.0).scale(1.0/nsmth)
     # Calculate fourier coefficients
-    d_cos = np.zeros((MMAX+1,nlat))# [m,th]
-    d_sin = np.zeros((MMAX+1,nlat))# [m,th]
+    d_cos = np.zeros((MMAX+1, nlat))# [m,th]
+    d_sin = np.zeros((MMAX+1, nlat))# [m,th]
     # Calculating delta spatial values
-    for k in range(0,nlat):
+    for k in range(0, nlat):
         # summation over all spherical harmonic degrees
         d_cos[:,k] = np.sum(PLM2[:,:,k]*Ylms.clm, axis=0)
         d_sin[:,k] = np.sum(PLM2[:,:,k]*Ylms.slm, axis=0)
 
     # Multiplying by c/s(phi#m) to get spatial maps (lon,lat)
     delta.data = np.sqrt(np.dot(ccos.T,d_cos) + np.dot(ssin.T,d_sin)).T
 
     # output file format
     file_format = '{0}{1}_L{2:d}{3}{4}{5}_ERR_{6:03d}-{7:03d}.{8}'
-    # attributes for output files
-    kwargs = {}
-    kwargs['units'] = copy.copy(unit_list[UNITS-1])
-    kwargs['longname'] = copy.copy(unit_name[UNITS-1])
     # output error file to ascii, netCDF4 or HDF5
-    fargs = (FILE_PREFIX,unit_list[UNITS-1],LMAX,order_str,gw_str,ds_str,
+    fargs = (FILE_PREFIX,units,LMAX,order_str,gw_str,ds_str,
         GRACE_Ylms.month[0],GRACE_Ylms.month[-1],suffix[DATAFORM])
-    FILE = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
-    delta.to_file(FILE, format=DATAFORM, date=False, verbose=VERBOSE, **kwargs)
+    OUTPUT_FILE = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
+    delta.to_file(OUTPUT_FILE, format=DATAFORM, date=False,
+        verbose=VERBOSE, units=units_name, longname=units_longname)
     # set the permissions mode of the output files
-    os.chmod(FILE, MODE)
+    OUTPUT_FILE.chmod(mode=MODE)
     # add file to list
-    output_files.append(FILE)
+    output_files.append(OUTPUT_FILE)
 
     # return the list of output files
     return output_files
 
 # PURPOSE: print a file log for the GRACE analysis
 def output_log_file(input_arguments, output_files):
     # format: GRACE_error_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_error_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
@@ -495,16 +485,16 @@
 
 # PURPOSE: print a error file log for the GRACE analysis
 def output_error_log_file(input_arguments):
     # format: GRACE_error_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_error_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -520,20 +510,18 @@
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for spatial files')
     parser.add_argument('--file-prefix','-P',
         type=str,
         help='Prefix string for input and output files')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
@@ -626,15 +614,15 @@
     # GFZ: GRACE/GRACE-FO coefficients from GFZ GravIS
     #     http://gravis.gfz-potsdam.de/corrections
     parser.add_argument('--geocenter',
         metavar='DEG1', type=str,
         choices=['Tellus','SLR','SLF','UCI','Swenson','GFZ'],
         help='Update Degree 1 coefficients with SLR or derived values')
     parser.add_argument('--geocenter-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Specific geocenter file if not default')
     parser.add_argument('--interpolate-geocenter',
         default=False, action='store_true',
         help='Least-squares model missing Degree 1 coefficients')
     # replace low degree harmonics with values from Satellite Laser Ranging
     parser.add_argument('--slr-c20',
         type=str, default=None, choices=['CSR','GFZ','GSFC'],
@@ -656,15 +644,15 @@
         help='Replace C50 coefficients with SLR values')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input/output data format')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # Output log file for each job in forms
     # GRACE_error_run_2002-04-01_PID-00000.log
```

### Comparing `gravity-toolkit-1.2.0/scripts/grace_spatial_maps.py` & `gravity-toolkit-1.2.1/scripts/scale_grace_maps.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,20 +1,18 @@
 #!/usr/bin/env python
 u"""
-grace_spatial_maps.py
-Written by Tyler Sutterley (03/2023)
+scale_grace_maps.py
+Written by Tyler Sutterley (05/2023)
 
 Reads in GRACE/GRACE-FO spherical harmonic coefficients and exports
-    monthly spatial fields
+    monthly scaled spatial fields, estimated scaling errors,
+    and estimated scaled delta errors
 
 Will correct with the specified GIA model group, destripe/smooth/process,
-    and export the data in specified units
-
-Spatial output units: cm w.e., mm geoid height, mm elastic uplift,
-    microgal gravity perturbation or surface pressure (mbar)
+    and export the data in centimeters water equivalent (cm w.e.)
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -D X, --directory X: Working data directory
     -O X, --output-directory X: output directory for spatial files
     -P X, --file-prefix X: prefix string for input and output files
     -c X, --center X: GRACE/GRACE-FO processing center
@@ -81,26 +79,18 @@
         GSFC: use values from GSFC (TN-14)
     --slr-c40 X: Replace C40 coefficients with SLR values
         CSR: use values from CSR (5x5 with 6,1)
         GSFC: use values from GSFC
     --slr-c50 X: Replace C50 coefficients with SLR values
         CSR: use values from CSR (5x5 with 6,1)
         GSFC: use values from GSFC
-    -U X, --units X: output units
-        1: cm of water thickness
-        2: mm of geoid height
-        3: mm of elastic crustal deformation [Davis 2004]
-        4: microGal gravitational perturbation
-        5: mbar equivalent surface pressure
     --spacing X: spatial resolution of output data (dlon,dlat)
     --interval X: output grid interval
         1: (0:360, 90:-90)
         2: (degree spacing/2)
-        3: non-global grid (set with defined bounds)
-    --bounds X: non-global grid bounding box (minlon,maxlon,minlat,maxlat)
     --mean-file X: GRACE/GRACE-FO mean file to remove from the harmonic data
     --mean-format X: Input data format for GRACE/GRACE-FO mean file
         ascii
         netCDF4
         HDF5
         gfc
     --mask X: Land-sea mask for redistributing land water flux
@@ -109,119 +99,129 @@
         ascii
         netCDF4
         HDF5
         index-ascii
         index-netCDF4
         index-HDF5
     --redistribute-removed: redistribute removed mass fields over the ocean
+    --scale-file X: scaling factor file
     --log: Output log of files created for each job
     -V, --verbose: verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
+    future: Compatibility layer between Python 2 and Python 3
+        https://python-future.org/
 
 PROGRAM DEPENDENCIES:
-    grace_input_months.py: Reads GRACE/GRACE-FO files for a specified spherical
-            harmonic degree and order and for a specified date range
-        Includes degree 1 with with Swenson values (if specified)
+    grace_input_months.py: Reads GRACE/GRACE-FO files for a specified date range
+        Includes degree 1 values (if specified)
         Replaces low-degree harmonics with SLR values (if specified)
-    read_GIA_model.py: reads harmonics for a glacial isostatic adjustment model
+    read_GIA_model.py: reads spherical harmonics for glacial isostatic adjustment
     read_love_numbers.py: reads Load Love Numbers from Han and Wahr (1995)
-    associated_legendre.py: Computes fully normalized associated
-        Legendre polynomials
     gauss_weights.py: Computes the Gaussian weights as a function of degree
-    ocean_stokes.py: converts a land-sea mask to a series of spherical harmonics
-    gen_stokes.py: converts a spatial field into a series of spherical harmonics
+    ocean_stokes.py: reads a land-sea mask and converts to spherical harmonics
+    gen_stokes.py: converts a spatial field into spherical harmonic coefficients
     geocenter.py: converts between spherical harmonics and geocenter variations
     harmonic_summation.py: calculates a spatial field from spherical harmonics
-    units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
+    time_series.smooth.py: smoothes a time-series using a Loess-type algorithm
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     spatial.py: spatial data class for reading, writing and processing data
+    time.py: utilities for calculating time operations
+    units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     utilities.py: download and management utilities for files
 
+REFERENCES:
+    C-W Hsu and I Velicogna, "Detection of Sea Level Fingerprints derived from
+        GRACE gravity data", Geophysical Research Letters, 44(17), (2017).
+        https://doi.org/10.1002/2017GL074070
+
+    F W Landerer and S C Swenson, "Accuracy of scaled GRACE terrestrial water
+        storage estimates", Water Resources Research, 48(4), W04531, (2012).
+        https://doi.org/10.1029/2011WR011453
+
+    J Wahr, S C Swenson, and I Velicogna, "Accuracy of GRACE mass estimates",
+        Geophysical Research Letters, 33(6), L06401, (2006).
+        https://doi.org/10.1029/2005GL025305
+
 UPDATE HISTORY:
-    Updated 03/2023: add root attributes to output netCDF4 and HDF5 files
-    Updated 02/2023: use get function to retrieve specific units
-        use love numbers class with additional attributes
-    Updated 01/2023: refactored associated legendre polynomials
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: use new scaling_factors inheritance of spatial class
+        single input file with scaling factor variables
+        updated inputs to spatial from_ascii function
+        use attributes from units class for writing to netCDF4/HDF5 files
+    Updated 02/2023: use love numbers class with additional attributes
+    Updated 01/2023: refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
-    Updated 07/2022: create mask for output gridded variables
     Updated 04/2022: use wrapper function for reading load Love numbers
         use argparse descriptions within sphinx documentation
     Updated 12/2021: can use variable loglevels for verbose output
         option to specify a specific geocenter correction file
         fix default file prefix to include center and release information
     Updated 11/2021: add GSFC low-degree harmonics
     Updated 10/2021: using python logging for handling verbose output
         add more choices for setting input format of the removed files
-    Updated 07/2021: simplified file imports using wrappers in harmonics
+    Updated 07/2021: switch from parameter files to argparse arguments
+        simplified file imports using wrappers in harmonics
         added path to default land-sea mask for mass redistribution
         remove choices for argparse processing centers
-    Updated 06/2021: switch from parameter files to argparse arguments
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 04/2021: include parameters for replacing C21/S21 and C22/S22
     Updated 02/2021: changed remove index to files with specified formats
-    Updated 01/2021: harmonics object output from gen_stokes.py/ocean_stokes.py
-    Updated 12/2020: added more love number options and from gfc for mean files
-    Updated 10/2020: use argparse to set command line parameters
-    Updated 08/2020: use utilities to define path to load love numbers file
-    Updated 06/2020: using spatial data class for output operations
-    Updated 05/2020: for public release
+    Updated 02/2021: for public release
 """
-from __future__ import print_function
+from __future__ import print_function, division
 
 import sys
 import os
-import re
 import copy
 import time
 import logging
-import numpy as np
+import pathlib
 import argparse
+import numpy as np
 import traceback
-import collections
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: import GRACE/GRACE-FO files for a given months range
-# Converts the GRACE/GRACE-FO harmonics applying the specified procedures
-def grace_spatial_maps(base_dir, PROC, DREL, DSET, LMAX, RAD,
+# Calculates monthly scaled spatial maps from GRACE/GRACE-FO
+# spherical harmonic coefficients
+def scale_grace_maps(base_dir, PROC, DREL, DSET, LMAX, RAD,
     START=None,
     END=None,
     MISSING=None,
     LMIN=None,
     MMAX=None,
     LOVE_NUMBERS=0,
     REFERENCE=None,
     DESTRIPE=False,
-    UNITS=None,
     DDEG=None,
     INTERVAL=None,
-    BOUNDS=None,
     GIA=None,
     GIA_FILE=None,
     ATM=False,
     POLE_TIDE=False,
     DEG1=None,
     DEG1_FILE=None,
     MODEL_DEG1=False,
@@ -233,124 +233,134 @@
     SLR_C50=None,
     DATAFORM=None,
     MEAN_FILE=None,
     MEANFORM=None,
     REMOVE_FILES=None,
     REMOVE_FORMAT=None,
     REDISTRIBUTE_REMOVED=False,
+    SCALE_FILE=None,
     LANDMASK=None,
     OUTPUT_DIRECTORY=None,
     FILE_PREFIX=None,
     VERBOSE=0,
     MODE=0o775):
 
-    # recursively create output directory if not currently existing
-    if not os.access(OUTPUT_DIRECTORY, os.F_OK):
-        os.makedirs(OUTPUT_DIRECTORY, mode=MODE, exist_ok=True)
-
-    # output attributes for spatial files
-    attributes = collections.OrderedDict()
-    attributes['generating_institute'] = PROC
-    attributes['product_release'] = DREL
-    attributes['product_name'] = DSET
-    attributes['product_type'] = 'gravity_field'
-    attributes['title'] = 'GRACE/GRACE-FO Spatial Data'
+    # recursively create output Directory if not currently existing
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # list object of output files for file logs (full path)
     output_files = []
 
     # file information
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
+    # output file format
+    file_format = '{0}{1}{2}_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
 
     # read arrays of kl, hl, and ll Love Numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
         REFERENCE=REFERENCE, FORMAT='class')
-    # add attributes for earth model and love numbers
-    attributes['earth_model'] = LOVE.model
-    attributes['earth_love_numbers'] = LOVE.citation
-    attributes['reference_frame'] = LOVE.reference
+
+    # atmospheric ECMWF "jump" flag (if ATM)
+    atm_str = '_wATM' if ATM else ''
+    # output string for both LMAX==MMAX and LMAX != MMAX cases
+    MMAX = np.copy(LMAX) if not MMAX else MMAX
+    order_str = f'M{MMAX:d}' if (MMAX != LMAX) else ''
+    # output spatial units
+    units = 'cmwe'
+    units_name, units_longname = gravtk.units.get_attributes(units)
+    # invalid value
+    fill_value = -9999.0
 
     # Calculating the Gaussian smoothing for radius RAD
     if (RAD != 0):
         wt = 2.0*np.pi*gravtk.gauss_weights(RAD,LMAX)
         gw_str = f'_r{RAD:0.0f}km'
-        attributes['smoothing_radius'] = f'{RAD:0.0f} km'
     else:
         # else = 1
         wt = np.ones((LMAX+1))
         gw_str = ''
 
-    # flag for spherical harmonic order
-    MMAX = np.copy(LMAX) if not MMAX else MMAX
-    order_str = f'M{MMAX:d}' if (MMAX != LMAX) else ''
-    # add attributes for LMAX and MMAX
-    attributes['max_degree'] = LMAX
-    attributes['max_order'] = MMAX
+    # Read Ocean function and convert to Ylms for redistribution
+    if REDISTRIBUTE_REMOVED:
+        # read Land-Sea Mask and convert to spherical harmonics
+        ocean_Ylms = gravtk.ocean_stokes(LANDMASK, LMAX, MMAX=MMAX,
+            LOVE=LOVE)
+
+    # Grid spacing
+    dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
+    # Grid dimensions
+    if (INTERVAL == 1):# (0:360, 90:-90)
+        nlon = np.int64((360.0/dlon)+1.0)
+        nlat = np.int64((180.0/dlat)+1.0)
+    elif (INTERVAL == 2):# degree spacing/2
+        nlon = np.int64((360.0/dlon))
+        nlat = np.int64((180.0/dlat))
+
+    # field mapping for input spatial variables
+    field_mapping = dict(lon='lon', lat='lat', data='kfactor',
+        error='error', magnitude='power')
+    # read data for input scale files (ascii, netCDF4, HDF5)
+    if (DATAFORM == 'ascii'):
+        kfactor = gravtk.scaling_factors().from_ascii(SCALE_FILE,
+            spacing=[dlon,dlat], nlat=nlat, nlon=nlon)
+    elif (DATAFORM == 'netCDF4'):
+        kfactor = gravtk.scaling_factors().from_netCDF4(SCALE_FILE,
+            date=False, field_mapping=field_mapping)
+    elif (DATAFORM == 'HDF5'):
+        kfactor = gravtk.scaling_factors().from_HDF5(SCALE_FILE,
+            date=False, field_mapping=field_mapping)
+    # input data shape
+    nlat, nlon = kfactor.shape
 
-    # reading GRACE months for input date range
+    # input GRACE/GRACE-FO spherical harmonic datafiles for date range
     # replacing low-degree harmonics with SLR values if specified
     # include degree 1 (geocenter) harmonics if specified
     # correcting for Pole-Tide and Atmospheric Jumps if specified
     Ylms = gravtk.grace_input_months(base_dir, PROC, DREL, DSET, LMAX,
         START, END, MISSING, SLR_C20, DEG1, MMAX=MMAX, SLR_21=SLR_21,
         SLR_22=SLR_22, SLR_C30=SLR_C30, SLR_C40=SLR_C40, SLR_C50=SLR_C50,
         DEG1_FILE=DEG1_FILE, MODEL_DEG1=MODEL_DEG1, ATM=ATM,
         POLE_TIDE=POLE_TIDE)
-    # convert to harmonics object and remove mean if specified
+    # create harmonics object from GRACE/GRACE-FO data
     GRACE_Ylms = gravtk.harmonics().from_dict(Ylms)
-    # add attributes for input GRACE/GRACE-FO spherical harmonics
-    for att_name, att_val in Ylms['attributes'].items():
-        attributes[att_name] = att_val
     # use a mean file for the static field to remove
     if MEAN_FILE:
         # read data form for input mean file (ascii, netCDF4, HDF5, gfc)
         mean_Ylms = gravtk.harmonics().from_file(MEAN_FILE,
             format=MEANFORM, date=False)
         # remove the input mean
         GRACE_Ylms.subtract(mean_Ylms)
-        attributes['lineage'].append(os.path.basename(MEAN_FILE))
     else:
         GRACE_Ylms.mean(apply=True)
+    # date information of GRACE/GRACE-FO coefficients
+    nfiles = len(GRACE_Ylms.time)
 
     # filter GRACE/GRACE-FO coefficients
     if DESTRIPE:
         # destriping GRACE/GRACE-FO coefficients
         ds_str = '_FL'
         GRACE_Ylms = GRACE_Ylms.destripe()
-        attributes['filtering'] = 'Destriped'
     else:
         # using standard GRACE/GRACE-FO harmonics
         ds_str = ''
 
     # input GIA spherical harmonic datafiles
     GIA_Ylms_rate = gravtk.gia(lmax=LMAX).from_GIA(GIA_FILE, GIA=GIA, mmax=MMAX)
-    # output GIA string for filename
-    if GIA:
-        gia_str = f'_{GIA_Ylms_rate.title}'
-        attributes['GIA'] = (GIA_Ylms_rate.citation, os.path.basename(GIA_FILE))
-    else:
-        gia_str = ''
+    gia_str = f'_{GIA_Ylms_rate.title}' if GIA else ''
     # monthly GIA calculated by gia_rate*time elapsed
     # finding change in GIA each month
     GIA_Ylms = GIA_Ylms_rate.drift(GRACE_Ylms.time, epoch=2003.3)
     GIA_Ylms.month[:] = np.copy(GRACE_Ylms.month)
 
     # default file prefix
     if not FILE_PREFIX:
         fargs = (PROC,DREL,DSET,Ylms['title'],gia_str)
         FILE_PREFIX = '{0}_{1}_{2}{3}{4}_'.format(*fargs)
 
-    # Read Ocean function and convert to Ylms for redistribution
-    if REDISTRIBUTE_REMOVED:
-        # read Land-Sea Mask and convert to spherical harmonics
-        ocean_Ylms = gravtk.ocean_stokes(LANDMASK, LMAX,
-            MMAX=MMAX, LOVE=LOVE)
-        ocean_str = '_OCN'
-    else:
-        ocean_str = ''
-
     # input spherical harmonic datafiles to be removed from the GRACE data
     # Remove sets of Ylms from the GRACE data before returning
     remove_Ylms = GRACE_Ylms.zeros_like()
     remove_Ylms.time[:] = np.copy(GRACE_Ylms.time)
     remove_Ylms.month[:] = np.copy(GRACE_Ylms.month)
     if REMOVE_FILES:
         # extend list if a single format was entered for all files
@@ -360,22 +370,20 @@
         for REMOVE_FILE,REMOVEFORM in zip(REMOVE_FILES,REMOVE_FORMAT):
             if REMOVEFORM in ('ascii','netCDF4','HDF5'):
                 # ascii (.txt)
                 # netCDF4 (.nc)
                 # HDF5 (.H5)
                 Ylms = gravtk.harmonics().from_file(REMOVE_FILE,
                     format=REMOVEFORM)
-                attributes['lineage'].append(os.path.basename(REMOVE_FILE))
             elif REMOVEFORM in ('index-ascii','index-netCDF4','index-HDF5'):
                 # read from index file
                 _,removeform = REMOVEFORM.split('-')
                 # index containing files in data format
                 Ylms = gravtk.harmonics().from_index(REMOVE_FILE,
                     format=removeform)
-                attributes['lineage'].extend(os.path.basename(Ylms.filename))
             # reduce to GRACE/GRACE-FO months and truncate to degree and order
             Ylms = Ylms.subset(GRACE_Ylms.month).truncate(lmax=LMAX,mmax=MMAX)
             # distribute removed Ylms uniformly over the ocean
             if REDISTRIBUTE_REMOVED:
                 # calculate ratio between total removed mass and
                 # a uniformly distributed cm of water over the ocean
                 ratio = Ylms.clm[0,0,:]/ocean_Ylms.clm[0,0]
@@ -390,174 +398,290 @@
             if DESTRIPE:
                 Ylms = Ylms.destripe()
             # add data for month t and INDEX_FILE to the total
             # remove_clm and remove_slm matrices
             # redistributing the mass over the ocean if specified
             remove_Ylms.add(Ylms)
 
+    # calculating GRACE/GRACE-FO error (Wahr et al. 2006)
+    # output GRACE error file (for both LMAX==MMAX and LMAX != MMAX cases)
+    fargs = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GRACE_Ylms.month[0],
+        GRACE_Ylms.month[-1], suffix[DATAFORM])
+    delta_format = '{0}_{1}_{2}_DELTA_CLM_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
+    GRACE_Ylms.directory = pathlib.Path(Ylms['directory']).expanduser().absolute()
+    DELTA_FILE = GRACE_Ylms.directory.joinpath(delta_format.format(*fargs))
+    # check full path of the GRACE directory for delta file
+    # if file was previously calculated: will read file
+    # else: will calculate the GRACE/GRACE-FO error
+    if not DELTA_FILE.exists():
+        # add output delta file to list object
+        output_files.append(DELTA_FILE)
+
+        # Delta coefficients of GRACE time series (Error components)
+        delta_Ylms = gravtk.harmonics(lmax=LMAX, mmax=MMAX)
+        delta_Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+        delta_Ylms.slm = np.zeros((LMAX+1, MMAX+1))
+        # Smoothing Half-Width (CNES is a 10-day solution)
+        # All other solutions are monthly solutions (HFWTH for annual = 6)
+        if ((PROC == 'CNES') and (DREL in ('RL01','RL02'))):
+            HFWTH = 19
+        else:
+            HFWTH = 6
+        # Equal to the noise of the smoothed time-series
+        # for each spherical harmonic order
+        for m in range(0,MMAX+1):# MMAX+1 to include MMAX
+            # for each spherical harmonic degree
+            for l in range(m,LMAX+1):# LMAX+1 to include LMAX
+                # Delta coefficients of GRACE time series
+                for cs,csharm in enumerate(['clm','slm']):
+                    # calculate GRACE Error (Noise of smoothed time-series)
+                    # With Annual and Semi-Annual Terms
+                    val1 = getattr(GRACE_Ylms, csharm)
+                    smth = gravtk.time_series.smooth(GRACE_Ylms.time,
+                        val1[l,m,:], HFWTH=HFWTH)
+                    # number of smoothed points
+                    nsmth = len(smth['data'])
+                    tsmth = np.mean(smth['time'])
+                    # GRACE/GRACE-FO delta Ylms
+                    # variance of data-(smoothed+annual+semi)
+                    val2 = getattr(delta_Ylms, csharm)
+                    val2[l,m] = np.sqrt(np.sum(smth['noise']**2)/nsmth)
+
+        # attributes for output files
+        attributes = {}
+        attributes['title'] = 'GRACE/GRACE-FO Spherical Harmonic Errors'
+        attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
+        # save GRACE/GRACE-FO delta harmonics to file
+        delta_Ylms.time = np.copy(tsmth)
+        delta_Ylms.month = np.int64(nsmth)
+        delta_Ylms.to_file(DELTA_FILE, format=DATAFORM, **attributes)
+        # set the permissions mode of the output harmonics file
+        DELTA_FILE.chmod(mode=MODE)
+        # append delta harmonics file to output files list
+        output_files.append(DELTA_FILE)
+    else:
+        # read GRACE/GRACE-FO delta harmonics from file
+        delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE,
+            format=DATAFORM)
+        # truncate GRACE/GRACE-FO delta clm and slm to d/o LMAX/MMAX
+        delta_Ylms = delta_Ylms.truncate(lmax=LMAX, mmax=MMAX)
+        tsmth = np.squeeze(delta_Ylms.time)
+        nsmth = np.int64(delta_Ylms.month)
+
     # Output spatial data object
     grid = gravtk.spatial()
-    # Output Degree Spacing
-    dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
-    # Output Degree Interval
-    if (INTERVAL == 1):
-        # (-180:180,90:-90)
-        nlon = np.int64((360.0/dlon)+1.0)
-        nlat = np.int64((180.0/dlat)+1.0)
-        grid.lon = -180 + dlon*np.arange(0,nlon)
-        grid.lat = 90.0 - dlat*np.arange(0,nlat)
-    elif (INTERVAL == 2):
-        # (Degree spacing)/2
-        grid.lon = np.arange(-180+dlon/2.0,180+dlon/2.0,dlon)
-        grid.lat = np.arange(90.0-dlat/2.0,-90.0-dlat/2.0,-dlat)
-        nlon = len(grid.lon)
-        nlat = len(grid.lat)
-    elif (INTERVAL == 3):
-        # non-global grid set with BOUNDS parameter
-        minlon,maxlon,minlat,maxlat = BOUNDS.copy()
-        grid.lon = np.arange(minlon+dlon/2.0,maxlon+dlon/2.0,dlon)
-        grid.lat = np.arange(maxlat-dlat/2.0,minlat-dlat/2.0,-dlat)
-        nlon = len(grid.lon)
-        nlat = len(grid.lat)
+    grid.lon = np.copy(kfactor.lon)
+    grid.lat = np.copy(kfactor.lat)
+    grid.time = np.zeros((nfiles))
+    grid.month = np.zeros((nfiles),dtype=np.int64)
+    grid.data = np.zeros((nlat, nlon, nfiles))
+    grid.mask = np.zeros((nlat, nlon, nfiles),dtype=bool)
 
     # Computing plms for converting to spatial domain
+    phi = grid.lon[np.newaxis,:]*np.pi/180.0
     theta = (90.0-grid.lat)*np.pi/180.0
     PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(theta))
+    # square of legendre polynomials truncated to order MMAX
+    mm = np.arange(0,MMAX+1)
+    PLM2 = PLM[:,mm,:]**2
+
+    # dfactor is the degree dependent coefficients
+    # for converting to centimeters water equivalent (cmwe)
+    dfactor = gravtk.units(lmax=LMAX).harmonic(*LOVE).cmwe
 
-    # Earth Parameters
-    # output spatial units
-    unit_list = ['cmwe', 'mmGH', 'mmCU', u'\u03BCGal', 'mbar']
-    unit_name = ['Equivalent_Water_Thickness', 'Geoid_Height',
-        'Elastic_Crustal_Uplift', 'Gravitational_Undulation',
-        'Equivalent_Surface_Pressure']
-    # Setting units factor for output
-    # dfactor computes the degree dependent coefficients
-    factors = gravtk.units(lmax=LMAX).harmonic(*LOVE)
-    if (UNITS == 1):
-        # 1: cmwe, centimeters water equivalent
-        dfactor = factors.get('cmwe')
-    elif (UNITS == 2):
-        # 2: mmGH, millimeters geoid height
-        dfactor = factors.get('mmGH')
-    elif (UNITS == 3):
-        # 3: mmCU, millimeters elastic crustal deformation
-        dfactor = factors.get('mmCU')
-    elif (UNITS == 4):
-        # 4: micGal, microGal gravity perturbations
-        dfactor = factors.get('microGal')
-    elif (UNITS == 5):
-        # 5: mbar, millibars equivalent surface pressure
-        dfactor = factors.get('mbar')
-    else:
-        raise ValueError(f'Invalid units code {UNITS:d}')
-    # add attributes for earth parameters
-    attributes['earth_radius'] = f'{factors.rad_e:0.3f} cm'
-    attributes['earth_density'] = f'{factors.rho_e:0.3f} g/cm'
-    attributes['earth_gravity_constant'] = f'{factors.GM:0.3f} cm^3/s^2'
-    # add attributes to output spatial object
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
-    grid.attributes['ROOT'] = attributes
-
-    # output file format
-    file_format = '{0}{1}_L{2:d}{3}{4}{5}_{6:03d}.{7}'
-    # keyword arguments for output files
-    kwargs = {}
-    kwargs['units'] = copy.copy(unit_list[UNITS-1])
-    kwargs['longname'] = copy.copy(unit_name[UNITS-1])
     # converting harmonics to truncated, smoothed coefficients in units
     # combining harmonics to calculate output spatial fields
-    for i,grace_month in enumerate(GRACE_Ylms.month):
+    for i,gm in enumerate(GRACE_Ylms.month):
         # GRACE/GRACE-FO harmonics for time t
         Ylms = GRACE_Ylms.index(i)
         # Remove GIA rate for time
         Ylms.subtract(GIA_Ylms.index(i))
         # Remove monthly files to be removed
         Ylms.subtract(remove_Ylms.index(i))
         # smooth harmonics and convert to output units
         Ylms.convolve(dfactor*wt)
         # convert spherical harmonics to output spatial grid
-        grid.data = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
-            grid.lon, grid.lat, LMIN=LMIN, LMAX=LMAX,
-            MMAX=MMAX, PLM=PLM).T
-        grid.mask = np.zeros_like(grid.data, dtype=bool)
+        grid.data[:,:,i] = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
+            grid.lon, grid.lat, LMAX=LMAX, MMAX=MMAX, PLM=PLM).T
         # copy time variables for month
-        grid.time = np.copy(Ylms.time)
-        grid.month = np.copy(Ylms.month)
+        grid.time[i] = np.copy(Ylms.time)
+        grid.month[i] = np.copy(Ylms.month)
 
-        # output monthly files to ascii, netCDF4 or HDF5
-        fargs = (FILE_PREFIX,unit_list[UNITS-1],LMAX,order_str,gw_str,
-            ds_str,grace_month,suffix[DATAFORM])
-        FILE = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
-        grid.to_file(FILE, format=DATAFORM, date=True, verbose=VERBOSE,
-            **kwargs)
-        # set the permissions mode of the output files
-        os.chmod(FILE, MODE)
-        # add file to list
-        output_files.append(FILE)
+    # scale output data with kfactor
+    grid = grid.scale(kfactor.data)
+    grid.replace_invalid(fill_value, mask=kfactor.mask)
+
+    # output monthly files to ascii, netCDF4 or HDF5
+    fargs = (FILE_PREFIX, '', units, LMAX, order_str, gw_str,
+        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
+    FILE = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
+    # attributes for output files
+    attributes = {}
+    attributes['units'] = copy.copy(units_name)
+    attributes['longname'] = copy.copy(units_longname)
+    attributes['title'] = 'GRACE/GRACE-FO Spatial Data'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
+    if (DATAFORM == 'ascii'):
+        # ascii (.txt)
+        grid.to_ascii(FILE, date=True, verbose=VERBOSE)
+    elif (DATAFORM == 'netCDF4'):
+        # netCDF4
+        grid.to_netCDF4(FILE, date=True, verbose=VERBOSE, **attributes)
+    elif (DATAFORM == 'HDF5'):
+        # HDF5
+        grid.to_HDF5(FILE, date=True, verbose=VERBOSE, **attributes)
+    # set the permissions mode of the output files
+    FILE.chmod(mode=MODE)
+    # add file to list
+    output_files.append(FILE)
+
+    # calculate power of scaled GRACE/GRACE-FO data
+    scaled_power = grid.sum(power=2.0).power(0.5)
+    # calculate residual leakage errors
+    # scaled by ratio of GRACE and synthetic power
+    ratio = scaled_power.scale(np.power(kfactor.magnitude,-1))
+    # replace invalid values with 0
+    ratio = np.nan_to_num(ratio.data, nan=0.0, posinf=0.0, neginf=0.0)
+    error = grid.copy()
+    error.data = kfactor.error*ratio
+    error.mask = np.copy(kfactor.mask)
+    error.update_mask()
+
+    # output monthly error files to ascii, netCDF4 or HDF5
+    fargs = (FILE_PREFIX, 'ERROR_', units, LMAX, order_str, gw_str,
+        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
+    FILE = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
+    # attributes for output files
+    attributes['title'] = 'GRACE/GRACE-FO Scaling Error'
+    if (DATAFORM == 'ascii'):
+        # ascii (.txt)
+        error.to_ascii(FILE, date=False, verbose=VERBOSE)
+    elif (DATAFORM == 'netCDF4'):
+        # netCDF4
+        error.to_netCDF4(FILE, date=False, verbose=VERBOSE, **attributes)
+    elif (DATAFORM == 'HDF5'):
+        # HDF5
+        error.to_HDF5(FILE, date=False, verbose=VERBOSE, **attributes)
+    # set the permissions mode of the output files
+    FILE.chmod(mode=MODE)
+    # add file to list
+    output_files.append(FILE)
+
+    # Output spatial data object
+    delta = gravtk.spatial()
+    delta.lon = np.copy(kfactor.lon)
+    delta.lat = np.copy(kfactor.lat)
+    delta.time = np.copy(tsmth)
+    delta.month = np.copy(nsmth)
+    delta.data = np.zeros((nlat, nlon))
+    delta.mask = np.zeros((nlat, nlon),dtype=bool)
+    # calculate scaled spatial error
+    # Calculating cos(m*phi)^2 and sin(m*phi)^2
+    m = delta_Ylms.m[:,np.newaxis]
+    ccos = np.cos(np.dot(m,phi))**2
+    ssin = np.sin(np.dot(m,phi))**2
+
+    # truncate delta harmonics to spherical harmonic range
+    Ylms = delta_Ylms.truncate(LMAX,lmin=LMIN,mmax=MMAX)
+    # convolve delta harmonics with degree dependent factors
+    # smooth harmonics and convert to output units
+    Ylms = Ylms.convolve(dfactor*wt).power(2.0).scale(1.0/nsmth)
+    # Calculate fourier coefficients
+    d_cos = np.zeros((MMAX+1,nlat))# [m,th]
+    d_sin = np.zeros((MMAX+1,nlat))# [m,th]
+    # Calculating delta spatial values
+    for k in range(0,nlat):
+        # summation over all spherical harmonic degrees
+        d_cos[:,k] = np.sum(PLM2[:,:,k]*Ylms.clm, axis=0)
+        d_sin[:,k] = np.sum(PLM2[:,:,k]*Ylms.slm, axis=0)
+    # Multiplying by c/s(phi#m) to get spatial error map
+    delta.data[:] = np.sqrt(np.dot(ccos.T,d_cos) + np.dot(ssin.T,d_sin)).T
+
+    # scale output harmonic errors with kfactor
+    delta = delta.scale(kfactor.data)
+    delta.replace_invalid(fill_value, mask=kfactor.mask)
+
+    # output monthly files to ascii, netCDF4 or HDF5
+    fargs = (FILE_PREFIX, 'DELTA_', units, LMAX, order_str, gw_str,
+        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
+    FILE = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
+    # attributes for output files
+    attributes['title'] = 'GRACE/GRACE-FO Spatial Error'
+    if (DATAFORM == 'ascii'):
+        # ascii (.txt)
+        delta.to_ascii(FILE, date=True, verbose=VERBOSE)
+    elif (DATAFORM == 'netCDF4'):
+        # netCDF4
+        delta.to_netCDF4(FILE, date=True, verbose=VERBOSE, **attributes)
+    elif (DATAFORM == 'HDF5'):
+        # HDF5
+        delta.to_HDF5(FILE, date=True, verbose=VERBOSE, **attributes)
+    # set the permissions mode of the output files
+    FILE.chmod(mode=MODE)
+    # add file to list
+    output_files.append(FILE)
 
     # return the list of output files
     return output_files
 
-# PURPOSE: print a file log for the GRACE analysis
+# PURPOSE: print a file log for the GRACE/GRACE-FO analysis
 def output_log_file(input_arguments, output_files):
-    # format: GRACE_processing_run_2002-04-01_PID-70335.log
+    # format: scale_GRACE_maps_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
-    LOGFILE = 'GRACE_processing_run_{0}_PID-{1:d}.log'.format(*args)
+    LOGFILE = 'scale_GRACE_maps_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
     for f in output_files:
         logging.info(f)
     # close the log file
     fid.close()
 
-# PURPOSE: print a error file log for the GRACE analysis
+# PURPOSE: print a error file log for the GRACE/GRACE-FO analysis
 def output_error_log_file(input_arguments):
-    # format: GRACE_processing_failed_run_2002-04-01_PID-70335.log
+    # format: scale_GRACE_maps_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
-    LOGFILE = 'GRACE_processing_failed_run_{0}_PID-{1:d}.log'.format(*args)
+    LOGFILE = 'scale_GRACE_maps_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
     traceback.print_exc(file=fid)
     # close the log file
     fid.close()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
-        description="""Calculates monthly spatial maps from GRACE/GRACE-FO
-            spherical harmonic coefficients
+        description="""Calculates scaled spatial maps from
+            GRACE/GRACE-FO spherical harmonic coefficients
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for spatial files')
     parser.add_argument('--file-prefix','-P',
         type=str,
         help='Prefix string for input and output files')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
@@ -611,29 +735,21 @@
     parser.add_argument('--radius','-R',
         type=float, default=0,
         help='Gaussian smoothing radius (km)')
     # Use a decorrelation (destriping) filter
     parser.add_argument('--destripe','-d',
         default=False, action='store_true',
         help='Use decorrelation (destriping) filter')
-    # output units
-    parser.add_argument('--units','-U',
-        type=int, default=1, choices=[1,2,3,4,5],
-        help='Output units')
     # output grid parameters
     parser.add_argument('--spacing',
         type=float, nargs='+', default=[0.5,0.5], metavar=('dlon','dlat'),
         help='Spatial resolution of output data')
     parser.add_argument('--interval',
-        type=int, default=2, choices=[1,2,3],
-        help=('Output grid interval '
-            '(1: global, 2: centered global, 3: non-global)'))
-    parser.add_argument('--bounds',
-        type=float, nargs=4, metavar=('lon_min','lon_max','lat_min','lat_max'),
-        help='Bounding box for non-global grid')
+        type=int, default=2, choices=[1,2],
+        help=('Output grid interval (1: global, 2: centered global)'))
     # GIA model type list
     models = {}
     models['IJ05-R2'] = 'Ivins R2 GIA Models'
     models['W12a'] = 'Whitehouse GIA Models'
     models['SM09'] = 'Simpson/Milne GIA Models'
     models['ICE6G'] = 'ICE-6G GIA Models'
     models['Wu10'] = 'Wu (2010) GIA Correction'
@@ -646,15 +762,15 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # correct for pole tide drift follow Wahr et al. (2015)
     parser.add_argument('--pole-tide',
@@ -672,15 +788,15 @@
     # GFZ: GRACE/GRACE-FO coefficients from GFZ GravIS
     #     http://gravis.gfz-potsdam.de/corrections
     parser.add_argument('--geocenter',
         metavar='DEG1', type=str,
         choices=['Tellus','SLR','SLF','UCI','Swenson','GFZ'],
         help='Update Degree 1 coefficients with SLR or derived values')
     parser.add_argument('--geocenter-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Specific geocenter file if not default')
     parser.add_argument('--interpolate-geocenter',
         default=False, action='store_true',
         help='Least-squares model missing Degree 1 coefficients')
     # replace low degree harmonics with values from Satellite Laser Ranging
     parser.add_argument('--slr-c20',
         type=str, default=None, choices=['CSR','GFZ','GSFC'],
@@ -702,48 +818,52 @@
         help='Replace C50 coefficients with SLR values')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input/output data format')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # monthly files to be removed from the GRACE/GRACE-FO data
     parser.add_argument('--remove-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='+',
+        type=pathlib.Path, nargs='+',
         help='Monthly files to be removed from the GRACE/GRACE-FO data')
     choices = []
     choices.extend(['ascii','netCDF4','HDF5'])
     choices.extend(['index-ascii','index-netCDF4','index-HDF5'])
     parser.add_argument('--remove-format',
         type=str, nargs='+', choices=choices,
         help='Input data format for files to be removed')
     parser.add_argument('--redistribute-removed',
         default=False, action='store_true',
         help='Redistribute removed mass fields over the ocean')
+    # scaling factor file
+    parser.add_argument('--scale-file',
+        type=pathlib.Path,
+        required=True, help='Scaling factor file')
     # land-sea mask for redistributing fluxes
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing land water flux')
     # Output log file for each job in forms
-    # GRACE_processing_run_2002-04-01_PID-00000.log
-    # GRACE_processing_failed_run_2002-04-01_PID-00000.log
+    # scale_GRACE_maps_run_2002-04-01_PID-00000.log
+    # scale_GRACE_maps_failed_run_2002-04-01_PID-00000.log
     parser.add_argument('--log',
         default=False, action='store_true',
         help='Output log file for each job')
-    # print information about each input and output file
+    # print information about processing run
     parser.add_argument('--verbose','-V',
         action='count', default=0,
-        help='Verbose output of run')
+        help='Verbose output of processing run')
     # permissions mode of the local directories and files (number in octal)
     parser.add_argument('--mode','-M',
         type=lambda x: int(x,base=8), default=0o775,
         help='Permissions mode of output files')
     # return the parser
     return parser
 
@@ -756,34 +876,32 @@
     # create logger
     loglevels = [logging.CRITICAL, logging.INFO, logging.DEBUG]
     logging.basicConfig(level=loglevels[args.verbose])
 
     # try to run the analysis with listed parameters
     try:
         info(args)
-        # run grace_spatial_maps algorithm with parameters
-        output_files = grace_spatial_maps(
+        # run scale_grace_maps algorithm with parameters
+        output_files = scale_grace_maps(
             args.directory,
             args.center,
             args.release,
             args.product,
             args.lmax,
             args.radius,
             START=args.start,
             END=args.end,
             MISSING=args.missing,
             LMIN=args.lmin,
             MMAX=args.mmax,
             LOVE_NUMBERS=args.love,
             REFERENCE=args.reference,
             DESTRIPE=args.destripe,
-            UNITS=args.units,
             DDEG=args.spacing,
             INTERVAL=args.interval,
-            BOUNDS=args.bounds,
             GIA=args.gia,
             GIA_FILE=args.gia_file,
             ATM=args.atm_correction,
             POLE_TIDE=args.pole_tide,
             DEG1=args.geocenter,
             DEG1_FILE=args.geocenter_file,
             MODEL_DEG1=args.interpolate_geocenter,
@@ -795,14 +913,15 @@
             SLR_C50=args.slr_c50,
             DATAFORM=args.format,
             MEAN_FILE=args.mean_file,
             MEANFORM=args.mean_format,
             REMOVE_FILES=args.remove_file,
             REMOVE_FORMAT=args.remove_format,
             REDISTRIBUTE_REMOVED=args.redistribute_removed,
+            SCALE_FILE=args.scale_file,
             LANDMASK=args.mask,
             OUTPUT_DIRECTORY=args.output_directory,
             FILE_PREFIX=args.file_prefix,
             VERBOSE=args.verbose,
             MODE=args.mode)
     except Exception as exc:
         # if there has been an error exception
```

### Comparing `gravity-toolkit-1.2.0/scripts/itsg_graz_grace_sync.py` & `gravity-toolkit-1.2.1/scripts/itsg_graz_grace_sync.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 itsg_graz_grace_sync.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Syncs GRACE/GRACE-FO and auxiliary data from the ITSG GRAZ server
 
 CALLING SEQUENCE:
     python itsg_graz_grace_sync.py --release Grace2018 --lmax 60
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
@@ -35,46 +35,49 @@
         https://lxml.de/
         https://github.com/lxml/lxml
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for syncing files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 10/2021: using python logging for handling verbose output
     Written 09/2021
 """
 from __future__ import print_function
 
 import sys
 import os
 import re
 import time
 import shutil
 import logging
+import pathlib
 import argparse
 import posixpath
 import gravity_toolkit as gravtk
 
 # PURPOSE: sync local GRACE/GRACE-FO files with ITSG GRAZ server
 def itsg_graz_grace_sync(DIRECTORY, RELEASE=None, LMAX=None, TIMEOUT=0,
     LOG=False, LIST=False, MODE=0o775, CLOBBER=False):
 
     # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # output to log file
         # format: ITSG_GRAZ_GRACE_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'ITSG_GRAZ_GRACE_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'ITSG_GRAZ_GRACE_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'ITSG GRAZ GRACE Sync Log ({today})')
         logging.info(f'Release: {RELEASE}')
         logging.info(f'LMAX: {LMAX:d}')
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
@@ -114,116 +117,117 @@
     files,mtimes = gravtk.utilities.http_list(REMOTE,
         timeout=TIMEOUT,pattern=R1,sort=True)
     # for each file on the remote directory
     for colname,remote_mtime in zip(files,mtimes):
         # extract parameters from input filename
         PFX,PRD,trunc,year,month,SFX = R1.findall(colname).pop()
         # local directory for output GRAZ data
-        local_dir=os.path.join(DIRECTORY,'GRAZ',DREL[RELEASE],DEALIASING[PRD])
+        local_dir = DIRECTORY.joinpath('GRAZ',DREL[RELEASE],DEALIASING[PRD])
         # check if local directory exists and recursively create if not
-        os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+        local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
         # local and remote versions of the file
-        local_file = os.path.join(local_dir,colname)
+        local_file = local_dir.joinpath(colname)
         remote_file = posixpath.join(*REMOTE,colname)
         # copy file from remote directory comparing modified dates
         http_pull_file(remote_file, remote_mtime, local_file,
             TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER, MODE=MODE)
 
     # sync ITSG GRAZ data for truncation
     subdir = f'monthly_n{LMAX:d}'
     REMOTE = [*HOST,release_directory,'monthly',subdir]
     files,mtimes = gravtk.utilities.http_list(REMOTE,
         timeout=TIMEOUT,pattern=R1,sort=True)
     # local directory for output GRAZ data
-    local_dir = os.path.join(DIRECTORY,'GRAZ',DREL[RELEASE],'GSM')
+    local_dir = DIRECTORY.joinpath('GRAZ',DREL[RELEASE],'GSM')
     # check if local directory exists and recursively create if not
-    os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+    local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
     # for each file on the remote directory
     for colname,remote_mtime in zip(files,mtimes):
         # local and remote versions of the file
-        local_file = os.path.join(local_dir,colname)
+        local_file = local_dir.joinpath(colname)
         remote_file = posixpath.join(*REMOTE,colname)
         # copy file from remote directory comparing modified dates
         http_pull_file(remote_file, remote_mtime, local_file,
             TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER, MODE=MODE)
 
     # create index file for GRACE/GRACE-FO L2 Spherical Harmonic Data
     # DATA PRODUCTS (GAC, GAD, GSM, GAA, GAB)
     for ds in ['GAA','GAB','GAC','GAD','GSM']:
         # local directory for exact data product
-        local_dir = os.path.join(DIRECTORY,'GRAZ',DREL[RELEASE],ds)
-        if not os.access(local_dir,os.F_OK):
+        local_dir = DIRECTORY.joinpath('GRAZ',DREL[RELEASE],ds)
+        if not local_dir.exists():
             continue
         # find local GRACE files to create index
-        grace_files=[fi for fi in os.listdir(local_dir) if R1.match(fi)]
+        grace_files = sorted([f.name for f in local_dir.iterdir()
+            if R1.match(f.name)])
         # outputting GRACE filenames to index
-        index_file = os.path.join(local_dir,'index.txt')
-        with open(index_file, mode='w', encoding='utf8') as fid:
+        index_file = local_dir.joinpath('index.txt')
+        with index_file.open(mode='w', encoding='utf8') as fid:
             for fi in sorted(grace_files):
                 print(fi, file=fid)
         # change permissions of index file
-        os.chmod(index_file, MODE)
+        index_file.chmod(mode=MODE)
 
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def http_pull_file(remote_file,remote_mtime,local_file,
     TIMEOUT=0,LIST=False,CLOBBER=False,MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         logging.info(f'{remote_file} --> ')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # if executing copy command (not only printing the files)
         if not LIST:
             # Create and submit request. There are a wide range of exceptions
             # that can be thrown here, including HTTPError and URLError.
             request = gravtk.utilities.urllib2.Request(remote_file)
             response = gravtk.utilities.urllib2.urlopen(request,
                 timeout=TIMEOUT)
             # chunked transfer encoding size
             CHUNK = 16 * 1024
             # copy contents to local file using chunked transfer encoding
             # transfer should work properly with ascii and binary data formats
-            with open(local_file, 'wb') as f:
+            with local_file.open(mode='wb') as f:
                 shutil.copyfileobj(response, f, CHUNK)
             # keep remote modification time of file and local access time
-            os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-            os.chmod(local_file, MODE)
+            os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+            local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs GRACE/GRACE-FO and auxiliary data from the
             ITSG GRAZ server
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # ITSG GRAZ releases
     choices = ['Grace2014','Grace2016','Grace2018','Grace_operational']
     parser.add_argument('--release','-r',
         type=str, nargs='+', metavar='DREL',
         default=['Grace2018','Grace_operational'],choices=choices,
         help='GRAZ Data Releases to sync')
```

### Comparing `gravity-toolkit-1.2.0/scripts/make_grace_index.py` & `gravity-toolkit-1.2.1/scripts/make_grace_index.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 make_grace_index.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 Creates index files of GRACE/GRACE-FO Level-2 data
 
 CALLING SEQUENCE:
     python make_grace_index.py --version 0 1
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
@@ -19,80 +19,83 @@
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 08/2022: make the data product optional
     Written 08/2022
 """
 from __future__ import print_function
 
 import sys
-import os
 import logging
 import argparse
+import pathlib
 import gravity_toolkit as gravtk
 
 # PURPOSE: Creates index files of GRACE/GRACE-FO data
 def make_grace_index(DIRECTORY, PROC=[], DREL=[], DSET=[],
     VERSION=[], MODE=None):
 
+    # input directory setup
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
     # mission shortnames
     shortname = {'grace':'GRAC', 'grace-fo':'GRFO'}
     # GRACE/GRACE-FO level-2 spherical harmonic products
     logging.info('GRACE/GRACE-FO L2 Global Spherical Harmonics:')
     # for each processing center (CSR, GFZ, JPL)
     for pr in PROC:
         # for each data release (RL04, RL05, RL06)
         for rl in DREL:
             # for each level-2 product
             for ds in DSET:
                 # local directory for exact data product
-                local_dir = os.path.join(DIRECTORY, pr, rl, ds)
+                local_dir = DIRECTORY.joinpath( pr, rl, ds)
                 # check if local directory exists
-                if not os.access(local_dir, os.F_OK):
+                if not local_dir.exists():
                     continue
                 # list of GRACE/GRACE-FO files for index
                 grace_files = []
                 # for each satellite mission (grace, grace-fo)
                 for i,mi in enumerate(['grace','grace-fo']):
                     # print string of exact data product
                     logging.info(f'{mi} {pr}/{rl}/{ds}')
                     # regular expression operator for data product
                     rx = gravtk.utilities.compile_regex_pattern(pr, rl, ds,
                         mission=shortname[mi], version=VERSION[i])
                     # find local GRACE/GRACE-FO files to create index
-                    granules = [f for f in os.listdir(local_dir) if rx.match(f)]
+                    granules = [f.name for f in local_dir.iterdir()
+                        if rx.match(f.name)]
                     # extend list of GRACE/GRACE-FO files
                     grace_files.extend(granules)
 
                 # outputting GRACE/GRACE-FO filenames to index
-                index_file = os.path.join(local_dir,'index.txt')
-                with open(index_file, mode='w', encoding='utf8') as fid:
+                index_file = local_dir.joinpath('index.txt')
+                with index_file.open(mode='w', encoding='utf8') as fid:
                     for fi in sorted(grace_files):
                         print(fi, file=fid)
                 # change permissions of index file
-                os.chmod(index_file, MODE)
+                index_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Creates index files of GRACE/GRACE-FO
             monthly Level-2 data
             """
     )
     # command line parameters
     # # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'], choices=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO processing center')
     # GRACE/GRACE-FO data release
```

### Comparing `gravity-toolkit-1.2.0/scripts/mascon_reconstruct.py` & `gravity-toolkit-1.2.1/scripts/mascon_reconstruct.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 mascon_reconstruct.py
-Written by Tyler Sutterley (02/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates the equivalent spherical harmonics from a mascon time series
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -O X, --output-directory X: output directory for mascon files
     -p X, --product X: GRACE/GRACE-FO Level-2 data product
@@ -72,14 +72,15 @@
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 02/2023: use love numbers class with additional attributes
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use wrapper function for reading load Love numbers
         include utf-8 encoding in reads to be windows compliant
         use argparse descriptions within sphinx documentation
     Updated 12/2021: can use variable loglevels for verbose output
@@ -113,33 +114,30 @@
     Updated 05/2014
 """
 from __future__ import print_function
 
 import sys
 import os
 import re
+import pathlib
 import logging
 import argparse
 import numpy as np
 import traceback
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
-# PURPOSE: tilde-compress a file path string
-def tilde_compress(file_path):
-    return file_path.replace(os.path.expanduser('~'),'~')
-
 # PURPOSE: Reconstruct spherical harmonic fields from the mascon
 # time series calculated in calc_mascon
 def mascon_reconstruct(DSET, LMAX, RAD,
     START=None,
     END=None,
     MMAX=None,
     DESTRIPE=False,
@@ -152,14 +150,18 @@
     MASCON_FILE=None,
     REDISTRIBUTE_MASCONS=False,
     RECONSTRUCT_FILE=None,
     LANDMASK=None,
     OUTPUT_DIRECTORY=None,
     MODE=0o775):
 
+    # create output directory if currently non-existent
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
     # for datasets not GSM: will add a label for the dataset
     dset_str = '' if (DSET == 'GSM') else f'_{DSET}'
     # atmospheric ECMWF "jump" flag (if ATM)
     atm_str = '_wATM' if ATM else ''
     # Gaussian smoothing string for radius RAD
     gw_str = f'_r{RAD:0.0f}km' if (RAD != 0) else ''
     # input GIA spherical harmonic datafiles
@@ -174,15 +176,16 @@
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
     # file parser for reading index files
     # removes commented lines (can comment out files in the index)
     # removes empty lines (if there are extra empty lines)
     parser = re.compile(r'^(?!\#|\%|$)', re.VERBOSE)
 
     # create initial reconstruct index for calc_mascon.py
-    fid = open(RECONSTRUCT_FILE, mode='w', encoding='utf8')
+    RECONSTRUCT_FILE = pathlib.Path(RECONSTRUCT_FILE).expanduser().absolute()
+    fid = RECONSTRUCT_FILE.open(mode='w', encoding='utf8')
     # output file format
     file_format = '{0}{1}{2}{3}{4}_L{5:d}{6}{7}{8}_{9:03d}-{10:03d}.{11}'
 
     # read load love numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
         REFERENCE=REFERENCE, FORMAT='class')
     # Earth Parameters
@@ -198,21 +201,22 @@
             LOVE=LOVE)
         ocean_str = '_OCN'
     else:
         # not distributing uniformly over ocean
         ocean_str = ''
 
     # input mascon spherical harmonic datafiles
-    with open(MASCON_FILE, mode='r', encoding='utf8') as f:
+    MASCON_FILE = pathlib.Path(MASCON_FILE).expanduser().absolute()
+    with MASCON_FILE.open(mode='r', encoding='utf8') as f:
         mascon_files = [l for l in f.read().splitlines() if parser.match(l)]
 
     # for each mascon file
-    for k,fi in enumerate(mascon_files):
+    for k,mascon_file in enumerate(mascon_files):
         # read mascon spherical harmonics
-        Ylms = gravtk.harmonics().from_file(os.path.expanduser(fi),
+        Ylms = gravtk.harmonics().from_file(mascon_file,
             format=DATAFORM, date=False)
         # Calculating the total mass of each mascon (1 cmwe uniform)
         total_area = 4.0*np.pi*(rad_e**3)*rho_e*Ylms.clm[0,0]/3.0
         # distribute mascon mass uniformly over the ocean
         if REDISTRIBUTE_MASCONS:
             # calculate ratio between total mascon mass and
             # a uniformly distributed cm of water over the ocean
@@ -222,70 +226,65 @@
                 for l in range(m,LMAX+1):# LMAX+1 to include LMAX
                     # remove ratio*ocean Ylms from mascon Ylms
                     # note: x -= y is equivalent to x = x - y
                     Ylms.clm[l,m] -= ratio*ocean_Ylms.clm[l,m]
                     Ylms.slm[l,m] -= ratio*ocean_Ylms.slm[l,m]
         # truncate mascon spherical harmonics to d/o LMAX/MMAX
         Ylms = Ylms.truncate(lmax=LMAX, mmax=MMAX)
-        # mascon base is the file without directory or suffix
-        mascon_base = os.path.basename(fi)
-        mascon_base = os.path.splitext(mascon_base)[0]
-        # if lower case, will capitalize
-        mascon_base = mascon_base.upper()
-        # if mascon name contains degree and order info, remove
-        mascon_name = mascon_base.replace(f'_L{LMAX:d}', '')
+        # mascon_name is the mascon file without directory or suffix
+        # if lower case: will capitalize
+        # if mascon name contains degree and order info: scrub from string
+        mascon_name = re.sub(r'_L(\d+)(M\d+)?', r'', Ylms.filename.stem.upper())
 
         # input filename format (for both LMAX==MMAX and LMAX != MMAX cases):
         # mascon name, GRACE dataset, GIA model, LMAX, (MMAX,)
         # Gaussian smoothing, filter flag, remove reconstructed fields flag
         # output GRACE error file
         args = (mascon_name,dset_str,gia_str.upper(),atm_str,ocean_str,
             LMAX,order_str,gw_str,ds_str)
         file_input = '{0}{1}{2}{3}{4}_L{5:d}{6}{7}{8}.txt'.format(*args)
-        mascon_data_input = np.loadtxt(os.path.join(OUTPUT_DIRECTORY,file_input))
+        mascon_data_input = np.loadtxt(OUTPUT_DIRECTORY.joinpath(file_input))
 
         # convert mascon time-series from Gt to cmwe
         mascon_sigma = 1e15*mascon_data_input[:,2]/total_area
         # mascon time-series Ylms
         mascon_Ylms = Ylms.scale(mascon_sigma)
         mascon_Ylms.time = mascon_data_input[:,1].copy()
         mascon_Ylms.month = mascon_data_input[:,0].astype(np.int64)
 
         # output to file: no ascii option
         args = (mascon_name,dset_str,gia_str.upper(),atm_str,ocean_str,
             LMAX,order_str,gw_str,ds_str,START,END,suffix[DATAFORM])
-        FILE = file_format.format(*args)
+        output_file = OUTPUT_DIRECTORY.joinpath(file_format.format(*args))
         # attributes for output files
         attributes = {}
-        attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+        attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
         # output harmonics to file
-        mascon_Ylms.to_file(os.path.join(OUTPUT_DIRECTORY,FILE),
-            format=DATAFORM, **attributes)
+        mascon_Ylms.to_file(output_file, format=DATAFORM, **attributes)
         # print file name to index
-        print(tilde_compress(os.path.join(OUTPUT_DIRECTORY,FILE)),file=fid)
+        print(mascon_Ylms.compressuser(output_file), file=fid)
         # change the permissions mode
-        os.chmod(os.path.join(OUTPUT_DIRECTORY,FILE),MODE)
+        output_file.chmod(mode=MODE)
     # close the reconstruct index
     fid.close()
     # change the permissions mode of the index file
-    os.chmod(RECONSTRUCT_FILE, MODE)
+    RECONSTRUCT_FILE.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
             description="""Calculates the equivalent spherical
             harmonics from a mascon time series
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for mascon files')
     # GRACE/GRACE-FO Level-2 data product
     parser.add_argument('--product','-p',
         metavar='DSET', type=str, default='GSM',
         help='GRACE/GRACE-FO Level-2 data product')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
@@ -339,39 +338,39 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input data format for auxiliary files')
     # mascon index file and parameters
     parser.add_argument('--mascon-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Index file of mascons spherical harmonics')
     parser.add_argument('--redistribute-mascons',
         default=False, action='store_true',
         help='Redistribute mascon mass over the ocean')
     # mascon reconstruct parameters
     parser.add_argument('--reconstruct-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Reconstructed mascon time series file')
     # land-sea mask for redistributing mascon mass
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing mascon mass')
     # print information about processing run
     parser.add_argument('--verbose','-V',
         action='count', default=0,
         help='Verbose output of processing run')
     # permissions mode of the local directories and files (number in octal)
     parser.add_argument('--mode','-M',
```

### Comparing `gravity-toolkit-1.2.0/scripts/monte_carlo_degree_one.py` & `gravity-toolkit-1.2.1/scripts/monte_carlo_degree_one.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 monte_carlo_degree_one.py
-Written by Tyler Sutterley (03/2023)
+Written by Tyler Sutterley (05/2023)
 
 Calculates degree 1 errors using GRACE coefficients of degree 2 and greater,
     and ocean bottom pressure variations from OMCT/MPIOM in a Monte Carlo scheme
 
 Relation between Geocenter Motion in mm and Normalized Geoid Coefficients
     X = sqrt(3)*a*C11
     Y = sqrt(3)*a*S11
@@ -84,26 +84,34 @@
         netCDF4
         HDF5
     --ocean-file X: Index file for ocean model harmonics
     --mean-file X: GRACE/GRACE-FO mean file to remove from the harmonic data
     --mean-format X: Input data format for GRACE/GRACE-FO mean file
     --error-file X: Additional error files to use in monte carlo analysis
     --iterative: Iterate degree one solutions
+    -s X, --solver X: Least squares solver for degree one solutions
+        inv: matrix inversion
+        lstsq: least squares solution
+        gelsy: complete orthogonal factorization
+        gelss: singular value decomposition (SVD)
+        gelsd: singular value decomposition (SVD) with divide and conquer method
     --fingerprint: Redistribute land-water flux using sea level fingerprints
     -e X, --expansion X: Spherical harmonic expansion for sea level fingerprints
     --mask X: Land-sea mask for calculating ocean mass and land water flux
     -p, --plot: Create output plots for components and iterations
     --log: Output log of files created for each job
     -V, --verbose: Verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
+    scipy: Scientific Tools for Python
+        https://scipy.org
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         http://www.h5py.org/
     matplotlib: Python 2D plotting library
@@ -145,14 +153,16 @@
         https://doi.org/10.1029/2007JB005338
 
     J Wahr, S C Swenson, and I Velicogna, "Accuracy of GRACE mass estimates",
         Geophysical Research Letters, 33(6), L06401, (2006).
         https://doi.org/10.1029/2005GL025305
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: add options for least-squares solver
     Updated 03/2023: place matplotlib import within try/except statement
     Updated 02/2023: use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
         refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
@@ -196,18 +206,20 @@
 from __future__ import print_function
 
 import sys
 import os
 import re
 import time
 import logging
+import pathlib
 import argparse
 import warnings
 import traceback
 import numpy as np
+import scipy.linalg
 import gravity_toolkit as gravtk
 
 # attempt imports
 try:
     import matplotlib.pyplot as plt
     import matplotlib.cm as cm
     import matplotlib.offsetbox
@@ -221,15 +233,15 @@
     warnings.filterwarnings("module")
     warnings.warn("netCDF4 not available", ImportWarning)
 # ignore warnings
 warnings.filterwarnings("ignore")
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: model the seasonal component of an initial degree 1 model
@@ -285,14 +297,15 @@
     SLR_C30=None,
     SLR_C40=None,
     SLR_C50=None,
     DATAFORM=None,
     MEAN_FILE=None,
     MEANFORM=None,
     ERROR_FILES=[],
+    SOLVER=None,
     FINGERPRINT=False,
     EXPANSION=None,
     LANDMASK=None,
     PLOT=False,
     MODE=0o775):
 
     # GRACE/GRACE-FO dataset
@@ -336,15 +349,15 @@
         C50_str = ''
     # combine satellite laser ranging flags
     slr_str = ''.join([C21_str,C22_str,C30_str,C40_str,C50_str])
     # suffix for input ascii, netcdf and HDF5 files
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
 
     # output directory
-    DIRECTORY = os.path.join(base_dir,'geocenter')
+    DIRECTORY = base_dir.joinpath('geocenter')
     # list object of output files for file logs (full path)
     output_files = []
 
     # read load love numbers
     LOVE = gravtk.load_love_numbers(EXPANSION,
         LOVE_NUMBERS=LOVE_NUMBERS, REFERENCE='CF',
         FORMAT='class')
@@ -367,23 +380,23 @@
     # smoothed functions are from the read_ocean_function.py program
     # Open the land-sea NetCDF file for reading
     landsea = gravtk.spatial().from_netCDF4(LANDMASK,
         date=False, varname='LSMASK')
     # degree spacing and grid dimensions
     # will create GRACE spatial fields with same dimensions
     dlon,dlat = landsea.spacing
-    nlat,nlon = landsea.shape
+    nlat, nlon = landsea.shape
     # spatial parameters in radians
     dphi = dlon*np.pi/180.0
     dth = dlat*np.pi/180.0
     # longitude and colatitude in radians
     phi = landsea.lon[np.newaxis,:]*np.pi/180.0
     th = (90.0 - np.squeeze(landsea.lat))*np.pi/180.0
     # create land function
-    land_function = np.zeros((nlon,nlat),dtype=np.float64)
+    land_function = np.zeros((nlon, nlat),dtype=np.float64)
     # extract land function from file
     # combine land and island levels for land function
     indx,indy = np.nonzero((landsea.data.T >= 1) & (landsea.data.T <= 3))
     land_function[indx,indy] = 1.0
     # calculate ocean function from land function
     ocean_function = 1.0 - land_function
 
@@ -427,15 +440,15 @@
         # destriping GRACE/GRACE-FO coefficients
         ds_str = '_FL'
         GSM_Ylms = GSM_Ylms.destripe()
     else:
         # using standard GRACE/GRACE-FO harmonics
         ds_str = ''
     # full path to directory for specific GRACE/GRACE-FO product
-    GSM_Ylms.directory = Ylms['directory']
+    GSM_Ylms.directory = pathlib.Path(Ylms['directory']).expanduser().absolute()
     # GRACE dates
     tdec = np.copy(GSM_Ylms.time)
     months = np.copy(GSM_Ylms.month)
     # number of months considered
     n_files = len(GSM_Ylms.month)
 
     # input GIA spherical harmonic datafiles
@@ -470,29 +483,29 @@
         # file in ascii, netCDF4 or HDF5 formats
         Ylms = gravtk.harmonics().from_file(ERROR_FILE, format=DATAFORM)
         # truncate to degree and order and append to list
         error_Ylms.append(Ylms.truncate(lmax=LMAX, mmax=MMAX))
 
     # calculating GRACE/GRACE-FO error (Wahr et al. 2006)
     # output GRACE error file (for both LMAX==MMAX and LMAX != MMAX cases)
-    args = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GSM_Ylms.month[0],
+    fargs = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GSM_Ylms.month[0],
         GSM_Ylms.month[-1], suffix[DATAFORM])
     delta_format = '{0}_{1}_{2}_DELTA_CLM_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
-    DELTA_FILE = os.path.join(GSM_Ylms.directory,delta_format.format(*args))
+    DELTA_FILE = GSM_Ylms.directory.joinpath(delta_format.format(*fargs))
     # check full path of the GRACE directory for delta file
     # if file was previously calculated: will read file
     # else: will calculate the GRACE/GRACE-FO error
-    if not os.access(DELTA_FILE, os.F_OK):
+    if not DELTA_FILE.exists():
         # add output delta file to list object
         output_files.append(DELTA_FILE)
 
         # Delta coefficients of GRACE time series (Error components)
-        delta_Ylms = gravtk.harmonics(lmax=LMAX,mmax=MMAX)
-        delta_Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-        delta_Ylms.slm = np.zeros((LMAX+1,MMAX+1))
+        delta_Ylms = gravtk.harmonics(lmax=LMAX, mmax=MMAX)
+        delta_Ylms.clm = np.zeros((LMAX+1, MMAX+1))
+        delta_Ylms.slm = np.zeros((LMAX+1, MMAX+1))
         # Smoothing Half-Width (CNES is a 10-day solution)
         # All other solutions are monthly solutions (HFWTH for annual = 6)
         if ((PROC == 'CNES') and (DREL in ('RL01','RL02'))):
             HFWTH = 19
         else:
             HFWTH = 6
         # Equal to the noise of the smoothed time-series
@@ -514,21 +527,21 @@
                     # variance of data-(smoothed+annual+semi)
                     val2 = getattr(delta_Ylms, csharm)
                     val2[l,m] = np.sqrt(np.sum(smth['noise']**2)/nsmth)
 
         # attributes for output files
         attributes = {}
         attributes['title'] = 'GRACE/GRACE-FO Spherical Harmonic Errors'
-        attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+        attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
         # save GRACE/GRACE-FO delta harmonics to file
         delta_Ylms.time = np.copy(tsmth)
         delta_Ylms.month = np.int64(nsmth)
         delta_Ylms.to_file(DELTA_FILE, format=DATAFORM, **attributes)
         # set the permissions mode of the output harmonics file
-        os.chmod(DELTA_FILE, MODE)
+        DELTA_FILE.chmod(mode=MODE)
         # append delta harmonics file to output files list
         output_files.append(DELTA_FILE)
     else:
         # read GRACE/GRACE-FO delta harmonics from file
         delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE,
             format=DATAFORM)
         # truncate GRACE/GRACE-FO delta clm and slm to d/o LMAX/MMAX
@@ -546,15 +559,15 @@
     ssin = np.sin(np.dot(m,phi))
 
     # Legendre polynomials for degree 1
     P10 = np.squeeze(PLM[1,0,:])
     P11 = np.squeeze(PLM[1,1,:])
     # PLM for spherical harmonic degrees 2+
     # converted into mass and smoothed if specified
-    plmout = np.zeros((LMAX+1,MMAX+1,nlat))
+    plmout = np.zeros((LMAX+1, MMAX+1, nlat))
     for l in range(1,LMAX+1):
         m = np.arange(0,np.min([l,MMAX])+1)
         # convert to smoothed coefficients of mass
         # Convolving plms with degree dependent factor and smoothing
         plmout[l,m,:] = PLM[l,m,:]*dfactor[l]*wt[l]
 
     # Initializing 3x3 I-Parameter matrix
@@ -614,15 +627,15 @@
             pcos = np.zeros((MMAX+1, nlat))#-[m,lat]
             psin = np.zeros((MMAX+1, nlat))#-[m,lat]
             # Summing product of plms and c/slms over all SH degrees >= 2
             for i in range(0, nlat):
                 l = np.arange(2,LMAX+1)
                 pcos[:,i] = np.sum(plmout[l,:,i]*(GRACE_Ylms.clm[l,:]+Ylms.clm[l,:]), axis=0)
                 psin[:,i] = np.sum(plmout[l,:,i]*(GRACE_Ylms.slm[l,:]+Ylms.slm[l,:]), axis=0)
-            # Multiplying by c/s(phi#m) to get surface density in cmH2Oeq (lon,lat)
+            # Multiplying by c/s(phi#m) to get surface density in cmwe (lon,lat)
             # ccos/ssin are mXphi, pcos/psin are mXtheta: resultant matrices are phiXtheta
             # The summation over spherical harmonic order is in this multiplication
             rmass = np.dot(np.transpose(ccos),pcos) + np.dot(np.transpose(ssin),psin)
             # calculate G matrix parameters through a summation of each latitude
             for i in range(0,nlat):
                 # summation of integration factors, Legendre polynomials,
                 # (convolution of order and harmonics) and the ocean mass at t
@@ -696,21 +709,25 @@
                 scale_factor = eustatic_ratio*dfactor[1]
                 eustatic = gravtk.geocenter().from_harmonics(ocean_Ylms).scale(scale_factor)
 
             # eustatic coefficients of degree 1
             CMAT = np.array([eustatic.C10,eustatic.C11,eustatic.S11])
             # G Matrix for time t
             GMAT = np.array([G.C10, G.C11, G.S11])
-            # calculate inversion for degree 1 solutions
+            # calculate degree 1 solution for iteration
             # this is mathematically equivalent to an iterative procedure
             # whereby the initial degree one coefficients are used to update
             # the G Matrix until (C10, C11, S11) converge
-            DMAT = np.dot(np.linalg.inv(IMAT), (CMAT-GMAT))
-            # could also use pseudo-inverse in least-squares
-            #DMAT = np.linalg.lstsq(IMAT,(CMAT-GMAT),rcond=-1)[0]
+            if (SOLVER == 'inv'):
+                DMAT = np.dot(np.linalg.inv(IMAT), (CMAT-GMAT))
+            elif (SOLVER == 'lstsq'):
+                DMAT = np.linalg.lstsq(IMAT, (CMAT-GMAT), rcond=-1)[0]
+            elif SOLVER in ('gelsd', 'gelsy', 'gelss'):
+                DMAT, res, rnk, s = scipy.linalg.lstsq(IMAT, (CMAT-GMAT),
+                    lapack_driver=SOLVER)
             # save geocenter for iteration and time t after restoring GIA+ATM
             iteration.C10[t,n_iter] = DMAT[0]+gia.C10[t]+atm.C10[t]
             iteration.C11[t,n_iter] = DMAT[1]+gia.C11[t]+atm.C11[t]
             iteration.S11[t,n_iter] = DMAT[2]+gia.S11[t]+atm.S11[t]
         # remove mean of each solution for iteration
         iteration.C10[:,n_iter] -= iteration.C10[:,n_iter].mean()
         iteration.C11[:,n_iter] -= iteration.C11[:,n_iter].mean()
@@ -741,16 +758,16 @@
     # output degree 1 coefficients
     file_format = '{0}_{1}_{2}{3}{4}{5}{6}{7}.{8}'
     output_format = ('{0:11.4f}{1:14.6e}{2:14.6e}{3:14.6e}'
         '{4:14.6e}{5:14.6e}{6:14.6e} {7:03d}\n')
     # public file format in fully normalized spherical harmonics
     # local version with all descriptor flags
     a1=(PROC,DREL,model_str,slf_str,'',gia_str,delta_str,ds_str,'txt')
-    FILE1=os.path.join(DIRECTORY,file_format.format(*a1))
-    fid1 = open(FILE1, mode='w', encoding='utf8')
+    FILE1 = DIRECTORY.joinpath(file_format.format(*a1))
+    fid1 = FILE1.open(mode='w', encoding='utf8')
     # print headers for cases with and without dealiasing
     print_header(fid1)
     print_harmonic(fid1,LOVE.kl[1])
     print_global(fid1,PROC,DREL,model_str.replace('_',' '),GIA_Ylms_rate,
         SLR_C20,SLR_21,months)
     print_variables(fid1,'single precision','fully normalized')
     # for each GRACE/GRACE-FO month
@@ -758,21 +775,21 @@
         # output geocenter coefficients to file
         fid1.write(output_format.format(tdec[t],
             DEG1.C10[t],DEG1.C11[t],DEG1.S11[t],
             ERROR.C10[t],ERROR.C11[t],ERROR.S11[t],mon))
     # close the output file
     fid1.close()
     # set the permissions mode of the output file
-    os.chmod(FILE1, MODE)
+    FILE1.chmod(mode=MODE)
     output_files.append(FILE1)
 
     # output all degree 1 coefficients as a netCDF4 file
     a2=(PROC,DREL,model_str,slf_str,'',gia_str,delta_str,ds_str,'nc')
-    FILE2 = os.path.join(DIRECTORY,file_format.format(*a2))
-    fileID = netCDF4.Dataset(FILE2,'w',format="NETCDF4")
+    FILE2 = DIRECTORY.joinpath(file_format.format(*a2))
+    fileID = netCDF4.Dataset(FILE2, mode='w', format="NETCDF4")
     # Defining the NetCDF4 dimensions
     fileID.createDimension('run', RUNS)
     fileID.createDimension('time', n_files)
     # defining the NetCDF4 variables
     nc = {}
     nc['time'] = fileID.createVariable('time',tdec.dtype,('time',))
     nc['month'] = fileID.createVariable('month',months.dtype,('time',))
@@ -801,15 +818,15 @@
     nc['S11'].units = 'fully_normalized'
     nc['S11'].long_name = 'sine_spherical_harmonic_of_degree_1,_order_1'
     # define global attributes
     fileID.date_created = time.strftime('%Y-%m-%d',time.localtime())
     # close the output file
     fileID.close()
     # set the permissions mode of the output file
-    os.chmod(FILE2, MODE)
+    FILE2.chmod(mode=MODE)
     output_files.append(FILE2)
 
     # create plot showing monte carlo iterations
     if PLOT:
         # 3 row plot (C10, C11 and S11)
         ax = {}
         fig,(ax[0],ax[1],ax[2])=plt.subplots(nrows=3,sharex=True,figsize=(6,9))
@@ -847,19 +864,20 @@
             # adjust ticks
             ax[i].get_xaxis().set_tick_params(which='both', direction='in')
             ax[i].get_yaxis().set_tick_params(which='both', direction='in')
         # adjust locations of subplots and save to file
         fig.subplots_adjust(left=0.12,right=0.94,bottom=0.06,top=0.98,hspace=0.1)
         args = (PROC,DREL,model_str,ds_str)
         FILE = 'Geocenter_Monte_Carlo_{0}_{1}_{2}{3}.pdf'.format(*args)
-        plt.savefig(os.path.join(DIRECTORY,FILE), format='pdf')
+        PLOT1 = DIRECTORY.joinpath(FILE)
+        plt.savefig(PLOT1, format='pdf')
         plt.clf()
         # set the permissions mode of the output files
-        os.chmod(os.path.join(DIRECTORY,FILE), MODE)
-        output_files.append(os.path.join(DIRECTORY,FILE))
+        PLOT1.chmod(mode=MODE)
+        output_files.append(PLOT1)
 
     # return the list of output files
     return output_files
 
 # PURPOSE: print YAML header to top of file
 def print_header(fid):
     # print header
@@ -938,15 +956,15 @@
     ack.append(('Work was supported by an appointment to the NASA Postdoctoral '
         'Program at NASA Goddard Space Flight Center, administered by '
         'Universities Space Research Association under contract with NASA'))
     ack.append('GRACE is a joint mission of NASA (USA) and DLR (Germany)')
     if (DREL == 'RL06'):
         ack.append('GRACE-FO is a joint mission of NASA (USA) and GFZ (Germany)')
     fid.write('    {0:22}: {1}\n'.format('acknowledgement','.  '.join(ack)))
-    PRODUCT_VERSION = 'Release-{0}'.format(DREL[2:])
+    PRODUCT_VERSION = f'Release-{DREL[2:]}'
     fid.write('    {0:22}: {1}\n'.format('product_version',PRODUCT_VERSION))
     fid.write('    {0:22}:\n'.format('references'))
     reference = []
     # geocenter citations
     reference.append(('T. C. Sutterley, and I. Velicogna, "Improved estimates '
         'of geocenter variability from time-variable gravity and ocean model '
         'outputs", Remote Sensing, 11(18), 2108, (2019). '
@@ -1083,17 +1101,17 @@
     fid.write('\n\n# End of YAML header\n')
 
 # PURPOSE: print a file log for the GRACE degree one analysis
 def output_log_file(input_arguments, output_files):
     # format: monte_carlo_degree_one_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'monte_carlo_degree_one_run_{0}_PID-{1:d}.log'.format(*args)
-    DIRECTORY = os.path.join(input_arguments.directory,'geocenter')
+    DIRECTORY = pathlib.Path(input_arguments.directory).joinpath('geocenter')
     # create a unique log and open the log file
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print number of monte carlo iterations used in calculation
     logging.info('\n\nNUMBER OF ITERATIONS: {0:d}'.format(arguments.runs))
@@ -1105,17 +1123,17 @@
     fid.close()
 
 # PURPOSE: print a error file log for the GRACE degree one analysis
 def output_error_log_file(input_arguments):
     # format: monte_carlo_degree_one_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'monte_carlo_degree_one_failed_run_{0}_PID-{1:d}.log'.format(*args)
-    DIRECTORY = os.path.join(input_arguments.directory,'geocenter')
+    DIRECTORY = pathlib.Path(input_arguments.directory).joinpath('geocenter')
     # create a unique log and open the log file
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -1132,16 +1150,15 @@
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO data processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
         help='GRACE/GRACE-FO Processing Center')
     # GRACE/GRACE-FO data release
     parser.add_argument('--release','-r',
@@ -1206,15 +1223,15 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # correct for pole tide drift follow Wahr et al. (2015)
     parser.add_argument('--pole-tide',
@@ -1241,36 +1258,41 @@
         help='Replace C50 coefficients with SLR values')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input/Output data format for delta harmonics file')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # additional error files to be used in the monte carlo run
     parser.add_argument('--error-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         nargs='+', default=[],
         help='Additional error files to use in Monte Carlo analysis')
+    # least squares solver
+    choices = ('inv','lstsq','gelsd', 'gelsy', 'gelss')
+    parser.add_argument('--solver','-s',
+        type=str, default='lstsq', choices=choices,
+        help='Least squares solver for degree one solutions')
     # run with sea level fingerprints
     parser.add_argument('--fingerprint',
         default=False, action='store_true',
         help='Redistribute land-water flux using sea level fingerprints')
     parser.add_argument('--expansion','-e',
         type=int, default=240,
         help='Spherical harmonic expansion for sea level fingerprints')
     # land-sea mask for calculating ocean mass and land water flux
     land_mask_file = gravtk.utilities.get_data_path(['data','land_fcn_300km.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         default=land_mask_file,
         help='Land-sea mask for calculating ocean mass and land water flux')
     # create output plots
     parser.add_argument('--plot','-p',
         default=False, action='store_true',
         help='Create output plots for Monte Carlo iterations')
     # Output log file for each job in forms
@@ -1328,14 +1350,15 @@
             SLR_C30=args.slr_c30,
             SLR_C40=args.slr_c40,
             SLR_C50=args.slr_c50,
             DATAFORM=args.format,
             MEAN_FILE=args.mean_file,
             MEANFORM=args.mean_format,
             ERROR_FILES=args.error_file,
+            SOLVER=args.solver,
             FINGERPRINT=args.fingerprint,
             EXPANSION=args.expansion,
             LANDMASK=args.mask,
             PLOT=args.plot,
             MODE=args.mode)
     except Exception as exc:
         # if there has been an error exception
```

### Comparing `gravity-toolkit-1.2.0/scripts/podaac_cumulus.py` & `gravity-toolkit-1.2.1/scripts/podaac_cumulus.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 podaac_cumulus.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 
 Syncs GRACE/GRACE-FO data from NASA JPL PO.DAAC Cumulus AWS S3 bucket
 S3 Cumulus syncs are only available in AWS instances in us-west-2
 
 Register with NASA Earthdata Login system:
 https://urs.earthdata.nasa.gov
 
@@ -47,14 +47,16 @@
     future: Compatibility layer between Python 2 and Python 3
         https://python-future.org/
 
 PROGRAM DEPENDENCIES:
     utilities.py: download and management utilities for syncing files
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 04/2023: different openers for s3 and data endpoints
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: added CMR queries for GRACE/GRACE-FO technical notes
         recursively create geocenter directory if not in file system
     Updated 08/2022: moved regular expression function to utilities
         Dynamically select newest version of granules for index
     Updated 04/2022: added option for GRACE/GRACE-FO Level-2 data version
         refactor to always try syncing from both grace and grace-fo missions
@@ -67,40 +69,41 @@
 import sys
 import os
 import re
 import gzip
 import time
 import shutil
 import logging
+import pathlib
 import argparse
 import gravity_toolkit as gravtk
 
 # PURPOSE: sync local GRACE/GRACE-FO files with JPL PO.DAAC AWS S3 bucket
 def podaac_cumulus(client, DIRECTORY, PROC=[], DREL=[], VERSION=[],
     AOD1B=False, ENDPOINT='s3', TIMEOUT=None, GZIP=False, LOG=False,
     CLOBBER=False, MODE=None):
 
     # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
+    DIRECTORY = pathlib.Path(DIRECTORY).expanduser().absolute()
+    DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
 
     # mission shortnames
     shortname = {'grace':'GRAC', 'grace-fo':'GRFO'}
     # datasets for each processing center
     DSET = {}
     DSET['CSR'] = ['GAC', 'GAD', 'GSM']
     DSET['GFZ'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
     DSET['JPL'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
 
     # create log file with list of synchronized files (or print to terminal)
     if LOG:
         # format: PODAAC_sync_2002-04-01.log
         today = time.strftime('%Y-%m-%d', time.localtime())
-        LOGFILE = f'PODAAC_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
+        LOGFILE = DIRECTORY.joinpath(f'PODAAC_sync_{today}.log')
+        logging.basicConfig(filename=LOGFILE, level=logging.INFO)
         logging.info(f'PO.DAAC Cumulus Sync Log ({today})')
         logging.info('CENTERS={0}'.format(','.join(PROC)))
         logging.info('RELEASES={0}'.format(','.join(DREL)))
     else:
         # standard output (terminal output)
         logging.basicConfig(level=logging.INFO)
 
@@ -108,16 +111,16 @@
     logging.info('Degree 1 Coefficients:')
     # SLR C2,0 and C3,0 coefficients
     logging.info('C2,0 and C3,0 Coefficients:')
     # compile regular expression operator for remote files
     R1 = re.compile(r'TN-13_GEOC_(CSR|GFZ|JPL)_(.*?).txt', re.VERBOSE)
     R2 = re.compile(r'TN-(14)_C30_C20_GSFC_SLR.txt', re.VERBOSE)
     # check if geocenter directory exists and recursively create if not
-    local_dir = os.path.join(DIRECTORY,'geocenter')
-    os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
+    local_dir = DIRECTORY.joinpath('geocenter')
+    local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
     # current time stamp to use for local files
     mtime = time.time()
     # for each processing center (CSR, GFZ, JPL)
     for pr in PROC:
         # for each data release (RL04, RL05, RL06)
         for rl in DREL:
             # for each unique version of data to sync
@@ -127,30 +130,30 @@
                     mission='grace-fo', center=pr, release=rl,
                     version=version, provider='POCLOUD',
                     endpoint=ENDPOINT)
 
                 # TN-13 JPL degree 1 files
                 url, = [url for url in urls if R1.search(url)]
                 granule = gravtk.utilities.url_split(url)[-1]
-                local_file = os.path.join(DIRECTORY,'geocenter',granule)
+                local_file = local_dir.joinpath(granule)
                 # access auxiliary data from endpoint
                 if (ENDPOINT == 'data'):
                     http_pull_file(url, mtime, local_file,
                         TIMEOUT=TIMEOUT, CLOBBER=CLOBBER, MODE=MODE)
                 elif (ENDPOINT == 's3'):
                     bucket = gravtk.utilities.s3_bucket(url)
                     key = gravtk.utilities.s3_key(url)
                     response = client.get_object(Bucket=bucket, Key=key)
                     s3_pull_file(response, mtime, local_file,
                         CLOBBER=CLOBBER, MODE=MODE)
 
                 # TN-14 SLR C2,0 and C3,0 files
                 url, = [url for url in urls if R2.search(url)]
                 granule = gravtk.utilities.url_split(url)[-1]
-                local_file = os.path.join(DIRECTORY,granule)
+                local_file = DIRECTORY.joinpath(granule)
                 # access auxiliary data from endpoint
                 if (ENDPOINT == 'data'):
                     http_pull_file(url, mtime, local_file,
                         TIMEOUT=TIMEOUT, CLOBBER=CLOBBER, MODE=MODE)
                 elif (ENDPOINT == 's3'):
                     bucket = gravtk.utilities.s3_bucket(url)
                     key = gravtk.utilities.s3_key(url)
@@ -162,28 +165,27 @@
     if AOD1B:
         logging.info('GRACE L1B Dealiasing Products:')
         # for each data release (RL04, RL05, RL06)
         for rl in DREL:
             # print string of exact data product
             logging.info(f'GFZ/AOD1B/{rl}')
             # local directory for exact data product
-            local_dir = os.path.join(DIRECTORY,'AOD1B',rl)
+            local_dir = DIRECTORY.joinpath('AOD1B',rl)
             # check if directory exists and recursively create if not
-            if not os.path.exists(local_dir):
-                os.makedirs(local_dir,MODE)
+            local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
             # query CMR for dataset
             ids,urls,mtimes = gravtk.utilities.cmr(
                 mission='grace', level='L1B', center='GFZ', release=rl,
                 product='AOD1B', start_date='2002-01-01T00:00:00',
                 provider='POCLOUD', endpoint=ENDPOINT)
             # for each model id and url
             for id,url,mtime in zip(ids,urls,mtimes):
                 # retrieve GRACE/GRACE-FO files
                 granule = gravtk.utilities.url_split(url)[-1]
-                local_file = os.path.join(local_dir,granule)
+                local_file = local_dir.joinpath(granule)
                 # access data from endpoint
                 if (ENDPOINT == 'data'):
                     http_pull_file(url, mtime, local_file,
                         TIMEOUT=TIMEOUT, CLOBBER=CLOBBER, MODE=MODE)
                 elif (ENDPOINT == 's3'):
                     bucket = gravtk.utilities.s3_bucket(url)
                     key = gravtk.utilities.s3_key(url)
@@ -196,18 +198,17 @@
     # for each processing center (CSR, GFZ, JPL)
     for pr in PROC:
         # for each data release (RL04, RL05, RL06)
         for rl in DREL:
             # for each level-2 product (GAC, GAD, GSM, GAA, GAB)
             for ds in DSET[pr]:
                 # local directory for exact data product
-                local_dir = os.path.join(DIRECTORY, pr, rl, ds)
+                local_dir = DIRECTORY.joinpath(pr, rl, ds)
                 # check if directory exists and recursively create if not
-                if not os.path.exists(local_dir):
-                    os.makedirs(local_dir,MODE)
+                local_dir.mkdir(mode=MODE, parents=True, exist_ok=True)
                 # list of GRACE/GRACE-FO files for index
                 grace_files = []
                 # for each satellite mission (grace, grace-fo)
                 for i,mi in enumerate(['grace','grace-fo']):
                     # print string of exact data product
                     logging.info(f'{mi} {pr}/{rl}/{ds}')
                     # query CMR for dataset
@@ -219,123 +220,126 @@
                     rx = gravtk.utilities.compile_regex_pattern(
                         pr, rl, ds, mission=shortname[mi])
                     # for each model id and url
                     for id,url,mtime in zip(ids,urls,mtimes):
                         # retrieve GRACE/GRACE-FO files
                         granule = gravtk.utilities.url_split(url)[-1]
                         suffix = '.gz' if GZIP else ''
-                        local_file = os.path.join(local_dir, f'{granule}{suffix}')
+                        local_file = local_dir.joinpath(f'{granule}{suffix}')
                         # access data from endpoint
                         if (ENDPOINT == 'data'):
                             http_pull_file(url, mtime, local_file,
                                 GZIP=GZIP, TIMEOUT=TIMEOUT,
                                 CLOBBER=CLOBBER, MODE=MODE)
                         elif (ENDPOINT == 's3'):
                             bucket = gravtk.utilities.s3_bucket(url)
                             key = gravtk.utilities.s3_key(url)
                             response = client.get_object(Bucket=bucket, Key=key)
                             s3_pull_file(response, mtime, local_file,
                                 GZIP=GZIP, CLOBBER=CLOBBER, MODE=MODE)
                     # find local GRACE/GRACE-FO files to create index
-                    granules = sorted([f for f in os.listdir(local_dir) if rx.match(f)])
+                    granules = sorted([f.name for f in local_dir.iterdir()
+                        if rx.match(f.name)])
                     # reduce list of GRACE/GRACE-FO files to unique dates
                     granules = gravtk.time.reduce_by_date(granules)
                     # extend list of GRACE/GRACE-FO files with granules
                     grace_files.extend(granules)
 
                 # outputting GRACE/GRACE-FO filenames to index
-                index_file = os.path.join(local_dir,'index.txt')
-                with open(index_file, mode='w', encoding='utf8') as fid:
+                index_file = local_dir.joinpath('index.txt')
+                with index_file.open(mode='w', encoding='utf8') as fid:
                     for fi in sorted(grace_files):
                         print(fi, file=fid)
                 # change permissions of index file
-                os.chmod(index_file, MODE)
+                index_file.chmod(mode=MODE)
 
     # close log file and set permissions level to MODE
     if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
+        LOGFILE.chmod(mode=MODE)
 
 # PURPOSE: pull file from a remote host checking if file exists locally
 # and if the remote file is newer than the local file
 def http_pull_file(remote_file, remote_mtime, local_file,
     GZIP=False, TIMEOUT=120, CLOBBER=False, MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
         logging.info(f'{remote_file} -->')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
+        logging.info(f'\t{str(local_file)}{OVERWRITE}\n')
         # chunked transfer encoding size
         CHUNK = 16 * 1024
         # Create and submit request.
         # There are a range of exceptions that can be thrown here
         # including HTTPError and URLError.
         request = gravtk.utilities.urllib2.Request(remote_file)
         response = gravtk.utilities.urllib2.urlopen(request,
             timeout=TIMEOUT)
         # copy remote file contents to local file
         if GZIP:
             with gzip.GzipFile(local_file, 'wb', 9, None, remote_mtime) as f:
                 shutil.copyfileobj(response, f)
         else:
-            with open(local_file, 'wb') as f:
+            with local_file.open(mode='wb') as f:
                 shutil.copyfileobj(response, f, CHUNK)
         # keep remote modification time of file and local access time
-        os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-        os.chmod(local_file, MODE)
+        os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+        local_file.chmod(mode=MODE)
 
 # PURPOSE: pull file from AWS s3 bucket checking if file exists locally
 # and if the remote file is newer than the local file
 def s3_pull_file(response, remote_mtime, local_file,
     GZIP=False, CLOBBER=False, MODE=0o775):
     # if file exists in file system: check if remote file is newer
     TEST = False
     OVERWRITE = ' (clobber)'
     # check if local version of file exists
-    if os.access(local_file, os.F_OK):
+    local_file = pathlib.Path(local_file).expanduser().absolute()
+    if local_file.exists():
         # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
+        local_mtime = local_file.stat().st_mtime
         # if remote file is newer: overwrite the local file
         if (gravtk.utilities.even(remote_mtime) >
             gravtk.utilities.even(local_mtime)):
             TEST = True
             OVERWRITE = ' (overwrite)'
     else:
         TEST = True
         OVERWRITE = ' (new)'
     # if file does not exist locally, is to be overwritten, or CLOBBER is set
     if TEST or CLOBBER:
         # Printing files transferred
-        logging.info(f'{local_file}{OVERWRITE}')
+        logging.info(f'{str(local_file)}{OVERWRITE}')
         # chunked transfer encoding size
         CHUNK = 16 * 1024
         # copy remote file contents to local file
         if GZIP:
             with gzip.GzipFile(local_file, 'wb', 9, None, remote_mtime) as f:
                 shutil.copyfileobj(response['Body'], f)
         else:
-            with open(local_file, 'wb') as f:
+            with local_file.open(mode='wb') as f:
                 shutil.copyfileobj(response['Body'], f, CHUNK)
         # keep remote modification time of file and local access time
-        os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-        os.chmod(local_file, MODE)
+        os.utime(local_file, (local_file.stat().st_atime, remote_mtime))
+        local_file.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Syncs GRACE/GRACE-FO and auxiliary data from the
             NASA JPL PO.DAAC Cumulus AWS bucket.
             """
@@ -345,21 +349,19 @@
     parser.add_argument('--user','-U',
         type=str, default=os.environ.get('EARTHDATA_USERNAME'),
         help='Username for NASA Earthdata Login')
     parser.add_argument('--password','-W',
         type=str, default=os.environ.get('EARTHDATA_PASSWORD'),
         help='Password for NASA Earthdata Login')
     parser.add_argument('--netrc','-N',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.path.join(os.path.expanduser('~'),'.netrc'),
+        type=pathlib.Path, default=pathlib.Path.home().joinpath('.netrc'),
         help='Path to .netrc file for authentication')
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # GRACE/GRACE-FO processing center
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'], choices=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO processing center')
     # GRACE/GRACE-FO data release
@@ -408,27 +410,33 @@
 def main():
     # Read the system arguments listed after the program
     parser = arguments()
     args,_ = parser.parse_known_args()
 
     # NASA Earthdata hostname
     URS = 'urs.earthdata.nasa.gov'
-    # check internet connection before attempting to run program
-    opener = gravtk.utilities.attempt_login(URS,
-        username=args.user, password=args.password,
-        netrc=args.netrc)
-
-    # Create and submit request to create AWS session
+    # host for retrieving AWS S3 credentials
+    HOST = 'https://archive.podaac.earthdata.nasa.gov/s3credentials'
     # There are a range of exceptions that can be thrown here
     # including HTTPError and URLError.
-    HOST = 'https://archive.podaac.earthdata.nasa.gov/s3credentials'
-    # get aws s3 client object
-    client = gravtk.utilities.s3_client(HOST, args.timeout)
+    if (args.endpoint == 's3'):
+        # build opener for s3 client access
+        opener = gravtk.utilities.attempt_login(URS,
+            username=args.user, password=args.password,
+            netrc=args.netrc)
+        # Create and submit request to create AWS session
+        client = gravtk.utilities.s3_client(HOST, args.timeout)
+    else:
+        # build opener for data client access
+        opener = gravtk.utilities.attempt_login(URS,
+            username=args.user, password=args.password,
+            netrc=args.netrc, authorization_header=False)
+        client = None
 
-    # retrieve data objects from s3 client
+    # retrieve data objects from s3 client or data endpoints
     podaac_cumulus(client, args.directory, PROC=args.center,
         DREL=args.release, VERSION=args.version, AOD1B=args.aod1b,
         ENDPOINT=args.endpoint, TIMEOUT=args.timeout,
         GZIP=args.gzip, LOG=args.log, CLOBBER=args.clobber,
         MODE=args.mode)
 
 # run main program
```

### Comparing `gravity-toolkit-1.2.0/scripts/podaac_grace_sync.py` & `gravity-toolkit-1.2.1/scripts/piecewise_grace_maps.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,594 +1,652 @@
 #!/usr/bin/env python
 u"""
-podaac_grace_sync.py
-Written by Tyler Sutterley (12/2022)
+piecewise_grace_maps.py
+Written by Tyler Sutterley (06/2023)
 
-Syncs GRACE/GRACE-FO and auxiliary data from the NASA JPL PO.DAAC Drive Server
-Syncs CSR/GFZ/JPL files for RL04/RL05/RL06 GAA/GAB/GAC/GAD/GSM
-    GAA and GAB are GFZ/JPL only
-Syncs GFZ AOD1b files for RL04/RL05/RL06
-Gets the latest technical note (TN) files
-Gets the monthly GRACE/GRACE-FO newsletters
-
-https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+Python
-https://nsidc.org/support/faq/what-options-are-available-bulk-downloading-data-
-    https-earthdata-login-enabled
-http://www.voidspace.org.uk/python/articles/authentication.shtml#base64
-
-Register with NASA Earthdata Login system:
-https://urs.earthdata.nasa.gov
-
-Add PO.DAAC Drive OPS to NASA Earthdata Applications and get WebDAV Password
-https://podaac-tools.jpl.nasa.gov/drive
-
-CALLING SEQUENCE:
-    python podaac_grace_sync.py --user <username>
-    where <username> is your NASA Earthdata username
-
-OUTPUTS:
-    CSR RL04/RL05/RL06: GAC/GAD/GSM
-    GFZ RL04/RL05/RL06: GAA/GAB/GAC/GAD/GSM
-    JPL RL04/RL05/RL06: GAA/GAB/GAC/GAD/GSM
-    Tellus degree one coefficients (TN-13)
-    Technical notes for satellite laser ranging coefficients
-    Technical notes for Release-05 atmospheric corrections
-    GFZ RL04/RL05/RL06: Level-1b dealiasing solutions
-    Monthly GRACE/GRACE-FO newsletters
+Reads in GRACE/GRACE-FO spatial files and fits a piecewise regression
+    model at each grid point for breakpoint analysis
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
-    -U X, --user X: username for NASA Earthdata Login
-    -W X, --webdav X: WebDAV password for JPL PO.DAAC Drive Login
-    -N X, --netrc X: path to .netrc file for authentication
-    -D X, --directory X: working data directory
-    -c X, --center X: GRACE/GRACE-FO Processing Center
-    -r X, --release X: GRACE/GRACE-FO Data Releases to sync
-    -v X, --version X: GRACE/GRACE-FO Level-2 Data Version to sync
-    -a, --aod1b: sync GRACE/GRACE-FO Level-1B dealiasing products
-    -n, --newsletters: sync GRACE/GRACE-FO newsletters
-    -t X, --timeout X: Timeout in seconds for blocking operations
-    -L, --list: print files to be transferred, but do not execute transfer
-    -l, --log: output log of files downloaded
-    -C, --clobber: Overwrite existing data in transfer
-    --checksum: compare hashes to check if overwriting existing data
-    -M X, --mode X: Local permissions mode of the directories and files synced
+    -O X, --output-directory X: output directory for spatial files
+    -P X, --file-prefix X: prefix string for input and output files
+    -S X, --start X: starting GRACE month for time series regression
+    -E X, --end X: ending GRACE month for time series regression
+    -B X, --breakpoint X: breakpoint GRACE month for piecewise regression
+    -N X, --missing X: Missing GRACE/GRACE-FO months
+    -l X, --lmax X: maximum spherical harmonic degree
+    -m X, --mmax X: maximum spherical harmonic order
+    -R X, --radius X: Gaussian smoothing radius (km)
+    -d, --destripe: use decorrelation filter (destriping filter)
+    -U X, --units X: output units
+        1: cm of water thickness
+        2: mm of geoid height
+        3: mm of elastic crustal deformation [Davis 2004]
+        4: microGal gravitational perturbation
+        5: mbar equivalent surface pressure
+    --spacing X: spatial resolution of output data (dlon,dlat)
+    --interval X: output grid interval
+        1: (0:360, 90:-90)
+        2: (degree spacing/2)
+        3: non-global grid (set with defined bounds)
+    --bounds X: non-global grid bounding box (minlon,maxlon,minlat,maxlat)
+    -F X, --format X: input/output data format
+        ascii
+        netCDF4
+        HDF5
+    --redistribute-removed: redistribute removed mass fields over the ocean
+    --cycles X: regression fit cyclical terms
+    --log: Output log of files created for each job
+    -V, --verbose: verbose output of processing run
+    -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
+    scipy: Scientific Tools for Python
+        https://docs.scipy.org/doc/
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
-    lxml: Pythonic XML and HTML processing library using libxml2/libxslt
-        https://lxml.de/
-        https://github.com/lxml/lxml
-    future: Compatibility layer between Python 2 and Python 3
-        https://python-future.org/
+    netCDF4: Python interface to the netCDF C library
+        https://unidata.github.io/netcdf4-python/netCDF4/index.html
+    h5py: Pythonic interface to the HDF5 binary data format.
+        https://www.h5py.org/
 
 PROGRAM DEPENDENCIES:
-    time.py: utilities for calculating time operations
-    utilities.py: download and management utilities for syncing files
+    time_series.piecewise.py: calculates piecewise trend coefficients
+    time_series.amplitude.py: calculates the amplitude and phase of a harmonic
+    spatial.py: spatial data class for reading, writing and processing data
+    utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 06/2023: append amplitude and phase titles when creating flags
+        more tidal aliasing periods using values from Ray and Luthcke (2006)
+    Updated 05/2023: split S2 tidal aliasing terms into GRACE and GRACE-FO eras
+        output data and error variables into single files
+        use fit module for getting tidal aliasing terms
+        use pathlib to define and operate on paths
+    Updated 04/2023: updated for public release
+    Updated 03/2023: updated inputs to spatial from_ascii function
+        use attributes from units class for writing to netCDF4/HDF5 files
+    Updated 01/2023: refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
-    Updated 08/2022: moved regular expression function to utilities
-        Dynamically select newest version of granules for index
-    Updated 04/2022: added option for GRACE/GRACE-FO Level-2 data version
-        refactor to always try syncing from both grace and grace-fo missions
-        use granule identifiers from CMR query to build output file index
-        use argparse descriptions within sphinx documentation
-    Updated 03/2022: update regular expression pattern for finding files
-        use CMR queries for finding GRACE/GRACE-FO level-2 product urls
+    Updated 04/2022: use argparse descriptions within documentation
+    Updated 12/2021: can use variable loglevels for verbose output
     Updated 10/2021: using python logging for handling verbose output
-    Updated 05/2021: added option for connection timeout (in seconds)
-        use try/except for retrieving netrc credentials
-    Updated 04/2021: set a default netrc file and check access
-        default credentials from environmental variables
-    Updated 12/2020: generalized podaac_list() by renaming to drive_list()
+    Updated 06/2021: switch from parameter files to argparse arguments
+    Updated 05/2021: define int/float precision to prevent deprecation warning
+    Updated 02/2021: replaced numpy bool to prevent deprecation warning
     Updated 10/2020: use argparse to set command line parameters
-    Updated 08/2020: flake8 compatible regular expression strings
-        moved urllib opener to utilities. add credential check
-        moved urllib directory listing to utilities
-    Updated 07/2020: add back snippets to sync Level-1b dealiasing products
-    Updated 06/2020: increased timeout to 2 minutes
-    Updated 05/2020: simplified PO.DAAC Drive login
-        added netrc option for alternative authentication method
-    Updated 03/2020 for public release.  Set default release to RL06
-    Updated 12/2019: GSFC TN-14 oblateness and C30 file in gracefo documents
-        convert last modified time in a function for a given format
-    Updated 10/2019: add GRACE-FO newsletters
-    Updated 09/2019: added ssl context to urlopen headers
-        added checksum option to not overwrite existing data files
-    Updated 07/2019: added GSFC TN-14 oblateness and C30 files
-    Updated 06/2019: added JPL TN-13 geocenter files (CSR/GFZ/JPL)
-        added GRACE-FO spherical harmonic coefficient files.  RL06 AOD1b *.tgz
-    Updated 04/2019: new podaac drive website https://podaac-tools.jpl.nasa.gov
-    Updated 12/2018: decode authorization header for python3 compatibility
-    Updated 11/2018: encode base64 strings for python3 compatibility
-    Updated 08/2018: regular expression pattern for GFZ Release 6
-        new lxml expressions to match PO.DAAC Drive website updates
-    Updated 06/2018: using python3 compatible octal, input and urllib
-        updated regular expression pattern for JPL Release 6
-    Updated 05/2018: updated for CSR Release 6
-    Updated 03/2018: updated header for log file output
-    Updated 11/2017: increased urllib2.urlopen timeout to 20.  PROC as option
-    Updated 08/2017: use raw_input() to enter NASA Earthdata credentials rather
-        than exiting with error
-    Updated 05/2017: exception if NASA Earthdata credentials weren't entered
-        using os.makedirs to recursively create directories
-        using getpass to enter server password securely (remove --password)
-    Updated 04/2017: slight modification to AOD1b regular expression
-        changed from using --rl04 option to setting with --release
-        parsing HTML with lxml libraries (extract names and last modified dates)
-        Authorization header in urllib2 opener. changes to check_connection()
-    Updated 01-02/2017: converted to https (urllib2) for NASA Earthdata servers
-        GRACE newsletters no longer standard, use --newsletters option
-    Updated 01/2017: added --mode to set file and directory permissions. Cleanup
-    Updated 09/2016: rewritten to use libftp within python (no external calls)
-        previous lftp version renamed podaac_grace_lftp.py
-    Updated 06/2016: added --list option for a dry-run (do not transfer files)
-        absolute import of shutil package
-    Updated 05/2016: using __future__ print function
-    Updated 03/2016: using getopt to set parameters, whether or not to output a
-        log file, added new help module
-    Updated 02/2016: added option for CSR weekly 5X5 harmonics
-    Updated 01/2016: parallel downloading for lftp mirror portions (8 files)
-    Updated 12/2015: added ECMWF GAG "jump" correction. New TN-09 GAF correction
-        using clobber flag for mget portions (rather than initial delete)
-    Updated 08/2015: changed sys.exit to raise RuntimeError
-        Sync the TN-08 and TN-09 GAE and GAF products
-    Updated 05/2015: updated for Jan/Feb 2015 months (don't include LMAXx30)
-        improved regular expression usage to create indices
-    Updated 03/2015: update for JPL RL05.1 (see L2-JPL_ProcStds_v5.1.pdf)
-    Updated 01/2015: added internet connectivity check
-    Updated 11/2014: added main definition for parameters
-    Updated 09/2014: updates to code structure.  Removed globs
-    Updated 02/2014: minor update to if statements
-    Updated 12/2013: Updated for GFZ RL05a GSM products
-        These products are less constrained to the background model
-        and are denoted with *_005a.gz
-        Final directory is starting working directory (using getcwd)
-    Updated 10/2013: Adding option to create sync logs.
-        Updated filepaths with path.join to standardize for different OS
-    Updated 09/2013: added subprocess wait commands to prevent interruptions
-        in the system call
-    Updated 05/2013: added RETIRE flag for RL04 as podaac directory moved
-        added sorted to glob for linux computers
-        glob parallels ls -U and has to be sorted
-    Updated 03/2013: added argument tags for AOD1B, RL04, ECCO, and GLDAS
-    Updated 01/2013: downloads daily AOD1B products (GFZ RL04 and RL05)
-    Updated 11/2012: Turned off loop for RL04 as it is no longer updated and
-    Updated 07/2012: adjusted the output directory for generalization
-    Updated 06/2012: added in new SLR dataset for release 5 TN-07_C20_SLR.txt
-    Updated 06/2012: index file created only lists the wanted *.gz files
-    Updated 04/2012: added loops to run through the different products
-    Written 04/2012
+    Updated 06/2020: using spatial data class for input and output operations
+    Updated 01/2020: output seasonal amplitude and phase
+    Updated 10/2019: changing Y/N flags to True/False
+    Updated 01/2019: include 161-day S2 tidal aliasing terms in regression
+    Updated 12/2018: added parallel processing with multiprocessing
+    Updated 11/2018: using future division for python3 compatibility
+    Updated 06/2018: using python3 compatible octal and input
+        can run for different sets of months using the --start and --end options
+        can run for different fit types using the --fit option
+        use output_data wrapper function for writing data to file
+    Updated 04/2018: changed the names of the output log files
+    Updated 03/2018: added option --mode to set the output file permissions
+        added option for output in equivalent surface pressure (UNITS=5)
+    Updated 06/2016: using __future__ print function, MMAX for LMAX != MMAX
+    Updated 06/2015: added output_files for log files
+    Written 09/2013
 """
-from __future__ import print_function
+from __future__ import print_function, division
 
 import sys
 import os
-import re
-import io
 import time
-import netrc
-import shutil
-import getpass
 import logging
+import pathlib
 import argparse
-import builtins
-import posixpath
-import lxml.etree
+import traceback
+import numpy as np
 import gravity_toolkit as gravtk
 
-# PURPOSE: sync local GRACE/GRACE-FO files with JPL PO.DAAC drive server
-def podaac_grace_sync(DIRECTORY, PROC=[], DREL=[], VERSION=[],
-    AOD1B=False, NEWSLETTERS=False, TIMEOUT=None, LOG=False, LIST=False,
-    CLOBBER=False, CHECKSUM=False, MODE=None):
-
-    # check if directory exists and recursively create if not
-    os.makedirs(DIRECTORY,MODE) if not os.path.exists(DIRECTORY) else None
-
-    # remote https server for GRACE data
-    HOST = 'https://podaac-tools.jpl.nasa.gov'
-    # mission shortnames
-    shortname = {'grace':'GRAC', 'grace-fo':'GRFO'}
-    # RL04/RL05 have been moved on PO.DAAC to the retired directory
-    retired = {}
-    retired['RL04'] = 'retired'
-    retired['RL05'] = 'retired'
-    retired['RL06'] = ''
-    # datasets for each processing center
-    DSET = {}
-    DSET['CSR'] = ['GAC', 'GAD', 'GSM']
-    DSET['GFZ'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
-    DSET['JPL'] = ['GAA', 'GAB', 'GAC', 'GAD', 'GSM']
-    # remote subdirectories for newsletters (note capital for grace-fo)
-    newsletter_sub = {}
-    newsletter_sub['grace'] = ['grace','docs','newsletters']
-    newsletter_sub['grace-fo'] = ['gracefo','docs','Newsletters']
-    # compile HTML parser for lxml
-    parser = lxml.etree.HTMLParser()
-
-    # create log file with list of synchronized files (or print to terminal)
-    if LOG:
-        # format: PODAAC_sync_2002-04-01.log
-        today = time.strftime('%Y-%m-%d',time.localtime())
-        LOGFILE = f'PODAAC_sync_{today}.log'
-        logging.basicConfig(filename=os.path.join(DIRECTORY,LOGFILE),
-            level=logging.INFO)
-        logging.info(f'PO.DAAC Sync Log ({today})')
-        logging.info('CENTERS={0}'.format(','.join(PROC)))
-        logging.info('RELEASES={0}'.format(','.join(DREL)))
-    else:
-        # standard output (terminal output)
-        logging.basicConfig(level=logging.INFO)
-
-    # Degree 1 (geocenter) coefficients
-    logging.info('Degree 1 Coefficients:')
-    PATH = [HOST,'drive','files','allData','gracefo','docs']
-    remote_dir = posixpath.join(*PATH)
-    local_dir = os.path.join(DIRECTORY,'geocenter')
-    # check if geocenter directory exists and recursively create if not
-    os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
-    # TN-13 JPL degree 1 files
-    # compile regular expression operator for remote files
-    R1 = re.compile(r'TN-13_GEOC_(CSR|GFZ|JPL)_(.*?).txt', re.VERBOSE)
-    # open connection with PO.DAAC drive server at remote directory
-    files,mtimes = gravtk.utilities.drive_list(PATH,
-        timeout=TIMEOUT,build=False,parser=parser,pattern=R1,sort=True)
-    # for each file on the remote server
-    for colname,remote_mtime in zip(files,mtimes):
-        # remote and local versions of the file
-        remote_file = posixpath.join(remote_dir,colname)
-        local_file = os.path.join(local_dir,colname)
-        http_pull_file(remote_file, remote_mtime, local_file,
-            TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-            CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # SLR C2,0 coefficients
-    logging.info('C2,0 Coefficients:')
-    PATH = [HOST,'drive','files','allData','grace','docs']
-    remote_dir = posixpath.join(*PATH)
-    local_dir = os.path.expanduser(DIRECTORY)
-    # compile regular expression operator for remote files
-    R1 = re.compile(r'TN-(05|07|11)_C20_SLR.txt', re.VERBOSE)
-    # open connection with PO.DAAC drive server at remote directory
-    files,mtimes = gravtk.utilities.drive_list(PATH,
-        timeout=TIMEOUT,build=False,parser=parser,pattern=R1,sort=True)
-    # for each file on the remote server
-    for colname,remote_mtime in zip(files,mtimes):
-        # remote and local versions of the file
-        remote_file = posixpath.join(remote_dir,colname)
-        local_file = os.path.join(local_dir,colname)
-        http_pull_file(remote_file, remote_mtime, local_file,
-            TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-            CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # SLR C3,0 coefficients
-    logging.info('C3,0 Coefficients:')
-    PATH = [HOST,'drive','files','allData','gracefo','docs']
-    remote_dir = posixpath.join(*PATH)
-    local_dir = os.path.expanduser(DIRECTORY)
-    # compile regular expression operator for remote files
-    R1 = re.compile(r'TN-(14)_C30_C20_GSFC_SLR.txt', re.VERBOSE)
-    # open connection with PO.DAAC drive server at remote directory
-    files,mtimes = gravtk.utilities.drive_list(PATH,
-        timeout=TIMEOUT,build=False,parser=parser,pattern=R1,sort=True)
-    # for each file on the remote server
-    for colname,remote_mtime in zip(files,mtimes):
-        # remote and local versions of the file
-        remote_file = posixpath.join(remote_dir,colname)
-        local_file = os.path.join(local_dir,colname)
-        http_pull_file(remote_file, remote_mtime, local_file,
-            TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-            CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # TN-08 GAE, TN-09 GAF and TN-10 GAG ECMWF atmosphere correction products
-    logging.info('TN-08 GAE, TN-09 GAF and TN-10 GAG products:')
-    PATH = [HOST,'drive','files','allData','grace','docs']
-    remote_dir = posixpath.join(*PATH)
-    local_dir = os.path.expanduser(DIRECTORY)
-    ECMWF_files = []
-    ECMWF_files.append('TN-08_GAE-2_2006032-2010031_0000_EIGEN_G---_0005.gz')
-    ECMWF_files.append('TN-09_GAF-2_2010032-2015131_0000_EIGEN_G---_0005.gz')
-    ECMWF_files.append('TN-10_GAG-2_2015132-2099001_0000_EIGEN_G---_0005.gz')
-    # compile regular expression operator for remote files
-    R1 = re.compile(r'({0}|{1}|{2})'.format(*ECMWF_files), re.VERBOSE)
-    # open connection with PO.DAAC drive server at remote directory
-    files,mtimes = gravtk.utilities.drive_list(PATH,
-        timeout=TIMEOUT,build=False,parser=parser,pattern=R1,sort=True)
-    # for each file on the remote server
-    for colname,remote_mtime in zip(files,mtimes):
-        # remote and local versions of the file
-        remote_file = posixpath.join(remote_dir,colname)
-        local_file = os.path.join(local_dir,colname)
-        http_pull_file(remote_file, remote_mtime, local_file,
-            TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-            CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # GRACE and GRACE-FO newsletters
-    if NEWSLETTERS:
-        # local newsletter directory (place GRACE and GRACE-FO together)
-        local_dir = os.path.join(DIRECTORY,'newsletters')
-        # check if newsletters directory exists and recursively create if not
-        os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
-        # for each satellite mission (grace, grace-fo)
-        for i,mi in enumerate(['grace','grace-fo']):
-            logging.info(f'{mi} Newsletters:')
-            PATH = [HOST,'drive','files','allData',*newsletter_sub[mi]]
-            remote_dir = posixpath.join(*PATH)
-            # compile regular expression operator for remote files
-            NAME = mi.upper().replace('-','_')
-            R1 = re.compile(rf'{NAME}_SDS_NL_(\d+).pdf', re.VERBOSE)
-            # open connection with PO.DAAC drive server at remote directory
-            files,mtimes = gravtk.utilities.drive_list(PATH,
-                timeout=TIMEOUT, build=False, parser=parser,
-                pattern=R1, sort=True)
-            # for each file on the remote server
-            for colname,remote_mtime in zip(files,mtimes):
-                # remote and local versions of the file
-                remote_file = posixpath.join(remote_dir,colname)
-                local_file = os.path.join(local_dir,colname)
-                http_pull_file(remote_file, remote_mtime, local_file,
-                    TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-                    CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # GRACE/GRACE-FO AOD1B dealiasing products
-    if AOD1B:
-        logging.info('GRACE L1B Dealiasing Products:')
-        # for each data release (RL04, RL05, RL06)
-        for rl in DREL:
-            # print string of exact data product
-            logging.info(f'GFZ/AOD1B/{rl}')
-            # remote and local directory for exact data product
-            local_dir = os.path.join(DIRECTORY,'AOD1B',rl)
-            # check if AOD1B directory exists and recursively create if not
-            os.makedirs(local_dir,MODE) if not os.path.exists(local_dir) else None
-            # query CMR for dataset
-            ids,urls,mtimes = gravtk.utilities.cmr(
-                mission='grace', level='L1B', center='GFZ', release=rl,
-                product='AOD1B', start_date='2002-01-01T00:00:00',
-                provider='PODAAC', endpoint='data')
-            # for each id, url and modification time
-            for id,url,mtime in zip(ids,urls,mtimes):
-                # retrieve GRACE/GRACE-FO files
-                granule = gravtk.utilities.url_split(url)[-1]
-                http_pull_file(url, mtime, os.path.join(local_dir,granule),
-                    TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-                    CHECKSUM=CHECKSUM, MODE=MODE)
-
-    # GRACE/GRACE-FO level-2 spherical harmonic products
-    logging.info('GRACE/GRACE-FO L2 Global Spherical Harmonics:')
-    # for each processing center (CSR, GFZ, JPL)
-    for pr in PROC:
-        # for each data release (RL04, RL05, RL06)
-        for rl in DREL:
-            # for each level-2 product (GAC, GAD, GSM, GAA, GAB)
-            for ds in DSET[pr]:
-                # local directory for exact data product
-                local_dir = os.path.join(DIRECTORY, pr, rl, ds)
-                # check if directory exists and recursively create if not
-                if not os.path.exists(local_dir):
-                    os.makedirs(local_dir,MODE)
-                # list of GRACE/GRACE-FO files for index
-                grace_files = []
-                # for each satellite mission (grace, grace-fo)
-                for i,mi in enumerate(['grace','grace-fo']):
-                    # print string of exact data product
-                    logging.info(f'{mi} {pr}/{rl}/{ds}')
-                    # query CMR for dataset
-                    ids,urls,mtimes = gravtk.utilities.cmr(
-                        mission=mi, center=pr, release=rl, product=ds,
-                        version=VERSION[i], provider='PODAAC', endpoint='data')
-                    # regular expression operator for data product
-                    rx = gravtk.utilities.compile_regex_pattern(
-                        pr, rl, ds, mission=shortname[mi])
-                    # for each id, url and modification time
-                    for id,url,mtime in zip(ids,urls,mtimes):
-                        # retrieve GRACE/GRACE-FO files
-                        granule = gravtk.utilities.url_split(url)[-1]
-                        http_pull_file(url, mtime, os.path.join(local_dir,granule),
-                            TIMEOUT=TIMEOUT, LIST=LIST, CLOBBER=CLOBBER,
-                            CHECKSUM=CHECKSUM, MODE=MODE)
-                    # find local GRACE/GRACE-FO files to create index
-                    granules = [f for f in os.listdir(local_dir) if rx.match(f)]
-                    # reduce list of GRACE/GRACE-FO files to unique dates
-                    granules = gravtk.time.reduce_by_date(granules)
-                    # extend list of GRACE/GRACE-FO files with granules
-                    grace_files.extend(granules)
-
-                # outputting GRACE/GRACE-FO filenames to index
-                index_file = os.path.join(local_dir,'index.txt')
-                with open(index_file, mode='w', encoding='utf8') as fid:
-                    for fi in sorted(grace_files):
-                        print(fi, file=fid)
-                # change permissions of index file
-                os.chmod(index_file, MODE)
-
-    # close log file and set permissions level to MODE
-    if LOG:
-        os.chmod(os.path.join(DIRECTORY,LOGFILE), MODE)
-
-# PURPOSE: pull file from a remote host checking if file exists locally
-# and if the remote file is newer than the local file
-def http_pull_file(remote_file, remote_mtime, local_file, TIMEOUT=120,
-    LIST=False, CLOBBER=False, CHECKSUM=False, MODE=0o775):
-    # if file exists in file system: check if remote file is newer
-    TEST = False
-    OVERWRITE = ' (clobber)'
-    # check if local version of file exists
-    if CHECKSUM and os.access(local_file, os.F_OK):
-        # generate checksum hash for local file
-        # open the local_file in binary read mode
-        local_hash = gravtk.utilities.get_hash(local_file)
-        # Create and submit request.
-        # There are a wide range of exceptions that can be thrown here
-        # including HTTPError and URLError.
-        req = gravtk.utilities.urllib2.Request(remote_file)
-        resp = gravtk.utilities.urllib2.urlopen(req,timeout=TIMEOUT)
-        # copy gravtk file contents to bytesIO object
-        remote_buffer = io.BytesIO(resp.read())
-        remote_buffer.seek(0)
-        # generate checksum hash for remote file
-        remote_hash = gravtk.utilities.get_hash(remote_buffer)
-        # compare checksums
-        if (local_hash != remote_hash):
-            TEST = True
-            OVERWRITE = f' (checksums: {local_hash} {remote_hash})'
-    elif os.access(local_file, os.F_OK):
-        # check last modification time of local file
-        local_mtime = os.stat(local_file).st_mtime
-        # if remote file is newer: overwrite the local file
-        if (gravtk.utilities.even(remote_mtime) >
-            gravtk.utilities.even(local_mtime)):
-            TEST = True
-            OVERWRITE = ' (overwrite)'
-    else:
-        TEST = True
-        OVERWRITE = ' (new)'
-    # if file does not exist locally, is to be overwritten, or CLOBBER is set
-    if TEST or CLOBBER:
-        # Printing files transferred
-        logging.info(f'{remote_file} --> ')
-        logging.info(f'\t{local_file}{OVERWRITE}\n')
-        # if executing copy command (not only printing the files)
-        if not LIST:
-            # chunked transfer encoding size
-            CHUNK = 16 * 1024
-            # copy bytes or transfer file
-            if CHECKSUM and os.access(local_file, os.F_OK):
-                # store bytes to file using chunked transfer encoding
-                remote_buffer.seek(0)
-                with open(local_file, 'wb') as f:
-                    shutil.copyfileobj(remote_buffer, f, CHUNK)
-            else:
-                # Create and submit request.
-                # There are a range of exceptions that can be thrown here
-                # including HTTPError and URLError.
-                request = gravtk.utilities.urllib2.Request(remote_file)
-                response = gravtk.utilities.urllib2.urlopen(request,
-                    timeout=TIMEOUT)
-                # copy remote file contents to local file
-                with open(local_file, 'wb') as f:
-                    shutil.copyfileobj(response, f, CHUNK)
-            # keep remote modification time of file and local access time
-            os.utime(local_file, (os.stat(local_file).st_atime, remote_mtime))
-            os.chmod(local_file, MODE)
+# PURPOSE: keep track of threads
+def info(args):
+    logging.info(pathlib.Path(sys.argv[0]).name)
+    logging.info(args)
+    logging.info(f'module name: {__name__}')
+    if hasattr(os, 'getppid'):
+        logging.info(f'parent process: {os.getppid():d}')
+    logging.info(f'process id: {os.getpid():d}')
+
+# program module to run with specified parameters
+def piecewise_grace_maps(LMAX, RAD,
+    START=None,
+    END=None,
+    BREAKPOINT=None,
+    MISSING=None,
+    MMAX=None,
+    DESTRIPE=False,
+    UNITS=None,
+    DDEG=None,
+    INTERVAL=None,
+    BOUNDS=None,
+    DATAFORM=None,
+    REDISTRIBUTE_REMOVED=False,
+    CYCLES=None,
+    OUTPUT_DIRECTORY=None,
+    FILE_PREFIX=None,
+    VERBOSE=0,
+    MODE=0o775):
+
+    # create output directory if currently non-existent
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+    # output filename suffix
+    suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')[DATAFORM]
+
+    # flag for spherical harmonic order
+    order_str = f'M{MMAX:d}' if MMAX and (MMAX != LMAX) else ''
+    # Calculating the Gaussian smoothing for radius RAD
+    gw_str = f'_r{RAD:0.0f}km' if (RAD != 0) else ''
+    # destriped GRACE/GRACE-FO coefficients
+    ds_str = '_FL' if DESTRIPE else ''
+    # distributing removed mass uniformly over ocean
+    ocean_str = '_OCN' if REDISTRIBUTE_REMOVED else ''
+    # input and output spatial units
+    # 1: cmwe, centimeters water equivalent
+    # 2: mmGH, millimeters geoid height
+    # 3: mmCU, millimeters elastic crustal deformation
+    # 4: micGal, microGal gravity perturbations
+    # 5: mbar, millibars equivalent surface pressure
+    units = gravtk.units.bycode(UNITS)
+    units_name, units_longname = gravtk.units.get_attributes(units)
+
+    # input file format
+    input_format = '{0}{1}_L{2:d}{3}{4}{5}_{6:03d}.{7}'
+    # output file format
+    output_format = '{0}{1}_L{2:d}{3}{4}{5}_{6}{7}_{8:03d}-{9:03d}.{10}'
+
+    # GRACE months to read
+    months = sorted(set(np.arange(START,END+1)) - set(MISSING))
+
+    # Output Degree Spacing
+    dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
+    # Output Degree Interval
+    if (INTERVAL == 1):
+        # (-180:180,90:-90)
+        nlon = np.int64((360.0/dlon)+1.0)
+        nlat = np.int64((180.0/dlat)+1.0)
+    elif (INTERVAL == 2):
+        # (Degree spacing)/2
+        nlon = np.int64(360.0/dlon)
+        nlat = np.int64(180.0/dlat)
+    elif (INTERVAL == 3):
+        # non-global grid set with BOUNDS parameter
+        minlon,maxlon,minlat,maxlat = BOUNDS.copy()
+        lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+        lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
+        nlon = len(lon)
+        nlat = len(lat)
+
+    # input data spatial object
+    spatial_list = []
+    for t,grace_month in enumerate(months):
+        # input GRACE/GRACE-FO spatial file
+        fargs = (FILE_PREFIX, units, LMAX, order_str,
+            gw_str, ds_str, grace_month, suffix)
+        input_file = OUTPUT_DIRECTORY.joinpath(input_format.format(*fargs))
+        # read GRACE/GRACE-FO spatial file
+        if (DATAFORM == 'ascii'):
+            dinput = gravtk.spatial().from_ascii(input_file,
+                spacing=[dlon,dlat], nlon=nlon, nlat=nlat)
+        elif (DATAFORM == 'netCDF4'):
+            # netcdf (.nc)
+            dinput = gravtk.spatial().from_netCDF4(input_file)
+        elif (DATAFORM == 'HDF5'):
+            # HDF5 (.H5)
+            dinput = gravtk.spatial().from_HDF5(input_file)
+        # append to spatial list
+        dinput.month = grace_month
+        nlat, nlon = dinput.shape
+        spatial_list.append(dinput)
+
+    # concatenate list to single spatial object
+    grid = gravtk.spatial().from_list(spatial_list)
+    spatial_list = None
+    # find index of breakpoint within GRACE/GRACE-FO months
+    if BREAKPOINT not in grid.month:
+        raise ValueError(f'{BREAKPOINT} not found in GRACE/GRACE-FO months')
+    breakpoint_index, = np.nonzero(grid.month == BREAKPOINT)
+
+    # Setting output parameters
+    coef_str = ['x0', 'px1', 'px1']
+    unit_suffix = ['', ' yr^-1', ' yr^-1']
+    fit_longname = ['Constant', 'Piecewise Trend', 'Piecewise Trend']
+
+    # amplitude string for cyclical components
+    amp_str = []
+    # amplitude and phase titles for cyclical components
+    amp_title = {}
+    ph_title = {}
+    # unique tidal aliasing periods from Ray and Luthcke (2006)
+    tidal_aliasing = {}
+    tidal_aliasing['Q1'] = 9.1
+    tidal_aliasing['O1'] = 13.6
+    tidal_aliasing['P1'] = 171.2
+    tidal_aliasing['S1'] = 322.1
+    tidal_aliasing['K1'] = 2725.4
+    tidal_aliasing['J1'] = 27.8
+    tidal_aliasing['M2'] = 13.5
+    tidal_aliasing['S2'] = 161.0
+    tidal_aliasing['K2'] = 1362.7
+    # extra terms for tidal aliasing components or custom fits
+    TERMS = []
+    term_index = []
+    for i,c in enumerate(CYCLES):
+        # check if fitting with semi-annual or annual terms
+        if (c == 0.5):
+            coef_str.extend(['SS','SC'])
+            amp_str.append('SEMI')
+            amp_title['SEMI'] = 'Semi-Annual Amplitude'
+            ph_title['SEMI'] = 'Semi-Annual Phase'
+            fit_longname.extend(['Semi-Annual Sine', 'Semi-Annual Cosine'])
+            unit_suffix.extend(['',''])
+        elif (c == 1.0):
+            coef_str.extend(['AS','AC'])
+            amp_str.append('ANN')
+            amp_title['ANN'] = 'Annual Amplitude'
+            ph_title['ANN'] = 'Annual Phase'
+            fit_longname.extend(['Annual Sine', 'Annual Cosine'])
+            unit_suffix.extend(['',''])
+        # check if fitting with tidal aliasing terms
+        for t,period in tidal_aliasing.items():
+            if np.isclose(c, (period/365.25)):
+                # terms for tidal aliasing during GRACE and GRACE-FO periods
+                TERMS.extend(gravtk.time_series.aliasing_terms(grid.time,
+                    period=period))
+                # labels for tidal aliasing during GRACE period
+                coef_str.extend([f'{t}SGRC', f'{t}CGRC'])
+                amp_str.append(f'{t}GRC')
+                amp_title[f'{t}GRC'] = f'{t} Tidal Alias (GRACE) Amplitude'
+                ph_title[f'{t}GRC'] = f'{t} Tidal Alias (GRACE) Phase'
+                fit_longname.append(f'{t} Tidal Alias (GRACE) Sine')
+                fit_longname.append(f'{t} Tidal Alias (GRACE) Cosine')
+                unit_suffix.extend(['',''])
+                # labels for tidal aliasing during GRACE-FO period
+                coef_str.extend([f'{t}SGFO', f'{t}CGFO'])
+                amp_str.append(f'{t}GFO')
+                amp_title[f'{t}GFO'] = f'{t} Tidal Alias (GRACE-FO) Amplitude'
+                ph_title[f'{t}GFO'] = f'{t} Tidal Alias (GRACE-FO) Phase'
+                fit_longname.append(f'{t} Tidal Alias (GRACE-FO) Sine')
+                fit_longname.append(f'{t} Tidal Alias (GRACE-FO) Cosine')
+                unit_suffix.extend(['',''])
+                # index to remove the original tidal aliasing term
+                term_index.append(i)
+    # remove the original tidal aliasing terms
+    CYCLES = np.delete(CYCLES, term_index)
+
+    # Fitting seasonal components
+    ncomp = len(coef_str)
+    ncycles = 2*len(CYCLES) + len(TERMS)
+    # output start and end months with breakpoint
+    output_start = np.zeros((ncomp), dtype=int) + START
+    output_end = np.zeros((ncomp), dtype=int) + END
+    # first piecewise (index 1) ends with BREAKPOINT
+    output_end[1] = BREAKPOINT
+    # second piecewise (index 2) starts with BREAKPOINT
+    output_start[2] = BREAKPOINT
+    # confidence interval for regression fit errors
+    CONF = 0.95
+
+    # Allocating memory for output variables
+    out = dinput.zeros_like()
+    out.data = np.zeros((nlat, nlon, ncomp))
+    out.error = np.zeros((nlat, nlon, ncomp))
+    out.mask = np.ones((nlat, nlon, ncomp),dtype=bool)
+    # Fit Significance
+    FS = {}
+    # SSE: Sum of Squares Error
+    # AIC: Akaike information criterion
+    # BIC: Bayesian information criterion
+    # R2Adj: Adjusted Coefficient of Determination
+    for key in ['SSE','AIC','BIC','R2Adj']:
+        FS[key] = dinput.zeros_like()
+
+    # calculate the regression coefficients and fit significance
+    for i in range(nlat):
+        for j in range(nlon):
+            # Calculating the regression coefficients
+            tsbeta = gravtk.time_series.piecewise(grid.time, grid.data[i,j,:],
+                BREAKPOINT=breakpoint_index, CYCLES=CYCLES, TERMS=TERMS,
+                CONF=CONF)
+            # save regression components
+            for k in range(0, ncomp):
+                out.data[i,j,k] = tsbeta['beta'][k]
+                out.error[i,j,k] = tsbeta['error'][k]
+                out.mask[i,j,k] = False
+            # Fit significance terms
+            # Degrees of Freedom
+            nu = tsbeta['DOF']
+            # Converting Mean Square Error to Sum of Squares Error
+            FS['SSE'].data[i,j] = tsbeta['MSE']*nu
+            FS['AIC'].data[i,j] = tsbeta['AIC']
+            FS['BIC'].data[i,j] = tsbeta['BIC']
+            FS['R2Adj'].data[i,j] = tsbeta['R2Adj']
+
+    # list of output files
+    output_files = []
+    # Output spatial files
+    for i in range(0,ncomp):
+        # output spatial file name
+        f1 = (FILE_PREFIX, units, LMAX, order_str, gw_str, ds_str,
+            coef_str[i], '', output_start[i], output_end[i], suffix)
+        file1 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f1))
+        # full attributes
+        UNITS_TITLE = f'{units_name}{unit_suffix[i]}'
+        LONGNAME = units_longname
+        FILE_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{fit_longname[i]}'
+        # output regression fit to file
+        output = out.index(i, date=False)
+        output_data(output, FILENAME=file1, DATAFORM=DATAFORM,
+            UNITS=UNITS_TITLE, LONGNAME=LONGNAME, TITLE=FILE_TITLE,
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
+        # add output files to list object
+        output_files.append(file1)
+
+    # if fitting coefficients with cyclical components
+    # output amplitude and phase of cyclical components
+    for i,flag in enumerate(amp_str):
+        # Indice pointing to the cyclical components
+        j = 3 + 2*i
+        # Allocating memory for output amplitude and phase
+        amp = dinput.zeros_like()
+        ph = dinput.zeros_like()
+        # calculating amplitude and phase of spatial field
+        amp.data,ph.data = gravtk.time_series.amplitude(
+            out.data[:,:,j], out.data[:,:,j+1]
+        )
+        # convert phase from -180:180 to 0:360
+        ii,jj = np.nonzero(ph.data < 0)
+        ph.data[ii,jj] += 360.0
+        # Amplitude Error
+        comp1 = out.error[:,:,j]*out.data[:,:,j]/amp.data
+        comp2 = out.error[:,:,j+1]*out.data[:,:,j+1]/amp.data
+        amp.error = np.sqrt(comp1**2 + comp2**2)
+        # Phase Error (degrees)
+        comp1 = out.error[:,:,j]*out.data[:,:,j+1]/(amp.data**2)
+        comp2 = out.error[:,:,j+1]*out.data[:,:,j]/(amp.data**2)
+        ph.error = (180.0/np.pi)*np.sqrt(comp1**2 + comp2**2)
+
+        # output file names for amplitude, phase and errors
+        f2 = (FILE_PREFIX, units, LMAX, order_str,
+            gw_str, ds_str, flag, '', START, END, suffix)
+        f3 = (FILE_PREFIX, units, LMAX, order_str,
+            gw_str, ds_str, flag,'_PHASE', START, END, suffix)
+        file2 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f2))
+        file3 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f3))
+        # full attributes
+        AMP_UNITS = units_name
+        PH_UNITS = 'degrees'
+        LONGNAME = units_longname
+        AMP_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{amp_title[flag]}'
+        PH_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{ph_title[flag]}'
+        # Output seasonal amplitude and phase to files
+        output_data(amp, FILENAME=file2, DATAFORM=DATAFORM,
+            UNITS=AMP_UNITS, LONGNAME=LONGNAME, TITLE=AMP_TITLE,
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
+        output_data(ph, FILENAME=file3, DATAFORM=DATAFORM,
+            UNITS=PH_UNITS, LONGNAME='Phase', TITLE=PH_TITLE,
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
+        # add output files to list object
+        output_files.append(file2)
+        output_files.append(file3)
+
+    # Output fit significance
+    signif_longname = {}
+    signif_longname['SSE'] = 'Sum of Squares Error'
+    signif_longname['AIC'] = 'Akaike information criterion'
+    signif_longname['BIC'] = 'Bayesian information criterion'
+    signif_longname['R2Adj'] = 'Adjusted Coefficient of Determination'
+    # for each fit significance term
+    for key,fs in FS.items():
+        # output file names for fit significance
+        signif_str = f'{key}_'
+        f4 = (FILE_PREFIX, units, LMAX, order_str, gw_str, ds_str,
+            signif_str, 'px1', START, END, suffix)
+        file4 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f4))
+        # full attributes
+        LONGNAME = signif_longname[key]
+        # output fit significance to file
+        output_data(fs, FILENAME=file4, DATAFORM=DATAFORM,
+            UNITS=key, LONGNAME=LONGNAME, TITLE=nu,
+            VERBOSE=VERBOSE, MODE=MODE)
+        # add output files to list object
+        output_files.append(file4)
+
+    # return the list of output files
+    return output_files
+
+# PURPOSE: wrapper function for outputting data to file
+def output_data(data, FILENAME=None, DATAFORM=None, UNITS=None,
+    LONGNAME=None, TITLE=None, CONF=0, VERBOSE=0, MODE=0o775):
+    # field mapping for output regression data
+    field_mapping = {}
+    field_mapping['lat'] = 'lat'
+    field_mapping['lon'] = 'lon'
+    field_mapping['data'] = 'data'
+    # add data error to output field mapping
+    if hasattr(data, 'error'):
+        field_mapping['error'] = 'error'
+    # output attributes
+    attributes = dict(ROOT={})
+    attributes['lon'] = {}
+    attributes['lon']['long_name'] = 'longitude'
+    attributes['lon']['units'] = 'degrees_east'
+    attributes['lat'] = {}
+    attributes['lat']['long_name'] = 'latitude'
+    attributes['lat']['units'] = 'degrees_north'
+    attributes['data'] = {}
+    attributes['data']['description'] = 'Model_fit'
+    attributes['data']['long_name'] = LONGNAME
+    attributes['data']['units'] = UNITS
+    attributes['error'] = {}
+    attributes['error']['description'] = 'Uncertainty_in_model_fit'
+    attributes['error']['long_name'] = LONGNAME
+    attributes['error']['units'] = UNITS
+    attributes['error']['confidence'] = 100*CONF
+    # output global attributes
+    REFERENCE = f'Output from {pathlib.Path(sys.argv[0]).name}'
+    # write to output file
+    if (DATAFORM == 'ascii'):
+        # ascii (.txt)
+        data.to_ascii(FILENAME, date=False, verbose=VERBOSE)
+    elif (DATAFORM == 'netCDF4'):
+        # netcdf (.nc)
+        data.to_netCDF4(FILENAME, date=False, verbose=VERBOSE,
+            field_mapping=field_mapping, attributes=attributes,
+            title=TITLE, reference=REFERENCE)
+    elif (DATAFORM == 'HDF5'):
+        # HDF5 (.H5)
+        data.to_HDF5(FILENAME, date=False, verbose=VERBOSE,
+            field_mapping=field_mapping, attributes=attributes,
+            title=TITLE, reference=REFERENCE)
+    # change the permissions mode of the output file
+    FILENAME.chmod(mode=MODE)
+
+# PURPOSE: print a file log for the GRACE/GRACE-FO regression
+def output_log_file(input_arguments, output_files):
+    # format: GRACE_processing_run_2002-04-01_PID-70335.log
+    args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
+    LOGFILE = 'GRACE_processing_run_{0}_PID-{1:d}.log'.format(*args)
+    # create a unique log and open the log file
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
+    logging.basicConfig(stream=fid, level=logging.INFO)
+    # print argument values sorted alphabetically
+    logging.info('ARGUMENTS:')
+    for arg, value in sorted(vars(input_arguments).items()):
+        logging.info(f'{arg}: {value}')
+    # print output files
+    logging.info('\n\nOUTPUT FILES:')
+    for f in output_files:
+        logging.info(f)
+    # close the log file
+    fid.close()
+
+# PURPOSE: print a error file log for the GRACE/GRACE-FO regression
+def output_error_log_file(input_arguments):
+    # format: GRACE_processing_failed_run_2002-04-01_PID-70335.log
+    args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
+    LOGFILE = 'GRACE_processing_failed_run_{0}_PID-{1:d}.log'.format(*args)
+    # create a unique log and open the log file
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
+    logging.basicConfig(stream=fid, level=logging.INFO)
+    # print argument values sorted alphabetically
+    logging.info('ARGUMENTS:')
+    for arg, value in sorted(vars(input_arguments).items()):
+        logging.info(f'{arg}: {value}')
+    # print traceback error
+    logging.info('\n\nTRACEBACK ERROR:')
+    traceback.print_exc(file=fid)
+    # close the log file
+    fid.close()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
-        description="""Syncs GRACE/GRACE-FO and auxiliary data from the
-            NASA JPL PO.DAAC Drive Server.
-            Syncs GRACE/GRACE-FO Level-1b dealiasing products (AOD1B).
-            Gets the latest technical note (TN) files.
-            Gets the monthly GRACE/GRACE-FO newsletters.
-            """
+        description="""Reads in GRACE/GRACE-FO spatial files and calculates the
+            trends at each grid point following an input regression model
+            """,
+        fromfile_prefix_chars="@"
     )
+    parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
-    # NASA Earthdata credentials
-    parser.add_argument('--user','-U',
-        type=str, default=os.environ.get('EARTHDATA_USERNAME'),
-        help='Username for NASA Earthdata Login')
-    parser.add_argument('--webdav','-W',
-        type=str, default=os.environ.get('PODAAC_PASSWORD'),
-        help='WebDAV Password for JPL PO.DAAC Drive Login')
-    parser.add_argument('--netrc','-N',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.path.join(os.path.expanduser('~'),'.netrc'),
-        help='Path to .netrc file for authentication')
-    # working data directory
-    parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
-        help='Working data directory')
-    # GRACE/GRACE-FO processing center
-    parser.add_argument('--center','-c',
-        metavar='PROC', type=str, nargs='+',
-        default=['CSR','GFZ','JPL'], choices=['CSR','GFZ','JPL'],
-        help='GRACE/GRACE-FO processing center')
-    # GRACE/GRACE-FO data release
-    parser.add_argument('--release','-r',
-        metavar='DREL', type=str, nargs='+',
-        default=['RL06'], choices=['RL06'],
-        help='GRACE/GRACE-FO data release')
-    # GRACE/GRACE-FO data version
-    parser.add_argument('--version','-v',
-        metavar='VERSION', type=str, nargs=2,
-        default=['0','1'], choices=['0','1','2','3'],
-        help='GRACE/GRACE-FO Level-2 data version')
-    # GRACE/GRACE-FO dealiasing products
-    parser.add_argument('--aod1b','-a',
-        default=False, action='store_true',
-        help='Sync GRACE/GRACE-FO Level-1B dealiasing products')
-    # GRACE/GRACE-FO newsletters
-    parser.add_argument('--newsletters','-n',
+    parser.add_argument('--output-directory','-O',
+        type=pathlib.Path, default=pathlib.Path.cwd(),
+        help='Output directory for spatial files')
+    parser.add_argument('--file-prefix','-P',
+        type=str,
+        help='Prefix string for input and output files')
+    # maximum spherical harmonic degree and order
+    parser.add_argument('--lmax','-l',
+        type=int, default=60,
+        help='Maximum spherical harmonic degree')
+    parser.add_argument('--mmax','-m',
+        type=int, default=None,
+        help='Maximum spherical harmonic order')
+    # start and end GRACE/GRACE-FO months
+    parser.add_argument('--start','-S',
+        type=int, default=4,
+        help='Starting GRACE/GRACE-FO month for time series regression')
+    parser.add_argument('--end','-E',
+        type=int, default=232,
+        help='Ending GRACE/GRACE-FO month for time series regression')
+    parser.add_argument('--breakpoint','-B',
+        type=int, default=129,
+        help='Breakpoint GRACE/GRACE-FO month for piecewise regression')
+    MISSING = [6,7,18,109,114,125,130,135,140,141,146,151,156,162,166,167,
+        172,177,178,182,187,188,189,190,191,192,193,194,195,196,197,200,201]
+    parser.add_argument('--missing','-N',
+        metavar='MISSING', type=int, nargs='+', default=MISSING,
+        help='Missing GRACE/GRACE-FO months')
+    # Gaussian smoothing radius (km)
+    parser.add_argument('--radius','-R',
+        type=float, default=0,
+        help='Gaussian smoothing radius (km)')
+    # Use a decorrelation (destriping) filter
+    parser.add_argument('--destripe','-d',
         default=False, action='store_true',
-        help='Sync GRACE/GRACE-FO Newsletters')
-    # connection timeout
-    parser.add_argument('--timeout','-t',
-        type=int, default=360,
-        help='Timeout in seconds for blocking operations')
-    # Output log file in form
-    # PODAAC_sync_2002-04-01.log
-    parser.add_argument('--log','-l',
+        help='Use decorrelation (destriping) filter')
+    # output units
+    parser.add_argument('--units','-U',
+        type=int, default=1, choices=[1,2,3,4,5],
+        help='Output units')
+    # output grid parameters
+    parser.add_argument('--spacing',
+        type=float, nargs='+', default=[0.5,0.5], metavar=('dlon','dlat'),
+        help='Spatial resolution of output data')
+    parser.add_argument('--interval',
+        type=int, default=2, choices=[1,2,3],
+        help=('Output grid interval '
+            '(1: global, 2: centered global, 3: non-global)'))
+    parser.add_argument('--bounds',
+        type=float, nargs=4, metavar=('lon_min','lon_max','lat_min','lat_max'),
+        help='Bounding box for non-global grid')
+    # input data format (ascii, netCDF4, HDF5)
+    parser.add_argument('--format','-F',
+        type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
+        help='Input/output data format')
+    parser.add_argument('--redistribute-removed',
         default=False, action='store_true',
-        help='Output log file')
-    # sync options
-    parser.add_argument('--list','-L',
+        help='Redistribute removed mass fields over the ocean')
+    # regression parameters
+    # regression fit cyclical terms
+    parser.add_argument('--cycles',
+        type=float, default=[0.5,1.0,161.0/365.25], nargs='+',
+        help='Regression fit cyclical terms')
+    # Output log file for each job in forms
+    # GRACE_processing_run_2002-04-01_PID-00000.log
+    # GRACE_processing_failed_run_2002-04-01_PID-00000.log
+    parser.add_argument('--log',
         default=False, action='store_true',
-        help='Only print files that could be transferred')
-    parser.add_argument('--checksum',
-        default=False, action='store_true',
-        help='Compare hashes to check for overwriting existing data')
-    parser.add_argument('--clobber','-C',
-        default=False, action='store_true',
-        help='Overwrite existing data in transfer')
-    # permissions mode of the directories and files synced (number in octal)
+        help='Output log file for each job')
+    # print information about each input and output file
+    parser.add_argument('--verbose','-V',
+        action='count', default=0,
+        help='Verbose output of run')
+    # permissions mode of the local directories and files (number in octal)
     parser.add_argument('--mode','-M',
         type=lambda x: int(x,base=8), default=0o775,
-        help='Permission mode of directories and files synced')
+        help='Permissions mode of output files')
     # return the parser
     return parser
 
 # This is the main part of the program that calls the individual functions
 def main():
     # Read the system arguments listed after the program
     parser = arguments()
     args,_ = parser.parse_known_args()
 
-    # JPL PO.DAAC drive hostname
-    HOST = 'podaac-tools.jpl.nasa.gov'
-    # get NASA Earthdata and JPL PO.DAAC drive credentials
+    # create logger
+    loglevels = [logging.CRITICAL, logging.INFO, logging.DEBUG]
+    logging.basicConfig(level=loglevels[args.verbose])
+
+    # try to run the analysis with listed parameters
     try:
-        args.user,_,args.webdav = netrc.netrc(args.netrc).authenticators(HOST)
-    except:
-        # check that NASA Earthdata credentials were entered
-        if not args.user:
-            prompt = f'Username for {HOST}: '
-            args.user = builtins.input(prompt)
-        # enter WebDAV password securely from command-line
-        if not args.webdav:
-            prompt = f'Password for {args.user}@{HOST}: '
-            args.webdav = getpass.getpass(prompt)
-
-    # build a urllib opener for PO.DAAC Drive
-    # Add the username and password for NASA Earthdata Login system
-    gravtk.utilities.build_opener(args.user,args.webdav)
-
-    # check internet connection before attempting to run program
-    # check JPL PO.DAAC Drive credentials before attempting to run program
-    DRIVE = f'https://{HOST}/drive/files'
-    if gravtk.utilities.check_credentials(DRIVE):
-        podaac_grace_sync(args.directory, PROC=args.center,
-            DREL=args.release, VERSION=args.version,
-            AOD1B=args.aod1b, NEWSLETTERS=args.newsletters,
-            TIMEOUT=args.timeout, LIST=args.list, LOG=args.log,
-            CLOBBER=args.clobber, CHECKSUM=args.checksum,
+        info(args)
+        # run piecewise_grace_maps algorithm with parameters
+        output_files = piecewise_grace_maps(
+            args.lmax,
+            args.radius,
+            START=args.start,
+            END=args.end,
+            BREAKPOINT=args.breakpoint,
+            MISSING=args.missing,
+            MMAX=args.mmax,
+            DESTRIPE=args.destripe,
+            UNITS=args.units,
+            DDEG=args.spacing,
+            INTERVAL=args.interval,
+            BOUNDS=args.bounds,
+            DATAFORM=args.format,
+            REDISTRIBUTE_REMOVED=args.redistribute_removed,
+            CYCLES=args.cycles,
+            OUTPUT_DIRECTORY=args.output_directory,
+            FILE_PREFIX=args.file_prefix,
+            VERBOSE=args.verbose,
             MODE=args.mode)
+    except Exception as exc:
+        # if there has been an error exception
+        # print the type, value, and stack trace of the
+        # current exception being handled
+        logging.critical(f'process id {os.getpid():d} failed')
+        logging.error(traceback.format_exc())
+        if args.log:# write failed job completion log file
+            output_error_log_file(args)
+    else:
+        if args.log:# write successful job completion log file
+            output_log_file(args,output_files)
 
 # run main program
 if __name__ == '__main__':
     main()
```

### Comparing `gravity-toolkit-1.2.0/scripts/regress_grace_maps.py` & `gravity-toolkit-1.2.1/scripts/regress_grace_maps.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 #!/usr/bin/env python
 u"""
 regress_grace_maps.py
-Written by Tyler Sutterley (01/2023)
+Written by Tyler Sutterley (06/2023)
 
-Reads in GRACE/GRACE-FO spatial files from grace_spatial_maps.py and
-    fits a regression model at each grid point
+Reads in GRACE/GRACE-FO spatial files and fits a regression model
+    at each grid point
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -O X, --output-directory X: output directory for spatial files
     -P X, --file-prefix X: prefix string for input and output files
     -S X, --start X: starting GRACE month for time series regression
     -E X, --end X: ending GRACE month for time series regression
@@ -56,14 +56,22 @@
 PROGRAM DEPENDENCIES:
     time_series.regress.py: calculates trend coefficients using least-squares
     time_series.amplitude.py: calculates the amplitude and phase of a harmonic
     spatial.py: spatial data class for reading, writing and processing data
     utilities.py: download and management utilities for files
 
 UPDATE HISTORY:
+    Updated 06/2023: append amplitude and phase titles when creating flags
+        more tidal aliasing periods using values from Ray and Luthcke (2006)
+    Updated 05/2023: split S2 tidal aliasing terms into GRACE and GRACE-FO eras
+        output data and error variables into single files
+        use fit module for getting tidal aliasing terms
+        use pathlib to define and operate on paths
+    Updated 03/2023: updated inputs to spatial from_ascii function
+        use attributes from units class for writing to netCDF4/HDF5 files
     Updated 01/2023: refactored time series analysis functions
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
     Updated 10/2021: using python logging for handling verbose output
     Updated 06/2021: switch from parameter files to argparse arguments
@@ -89,22 +97,23 @@
 """
 from __future__ import print_function, division
 
 import sys
 import os
 import time
 import logging
+import pathlib
 import argparse
 import traceback
 import numpy as np
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # program module to run with specified parameters
@@ -123,30 +132,36 @@
     ORDER=None,
     CYCLES=None,
     OUTPUT_DIRECTORY=None,
     FILE_PREFIX=None,
     VERBOSE=0,
     MODE=0o775):
 
+    # create output directory if currently non-existent
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
     # output filename suffix
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')[DATAFORM]
 
     # flag for spherical harmonic order
     order_str = f'M{MMAX:d}' if MMAX and (MMAX != LMAX) else ''
     # Calculating the Gaussian smoothing for radius RAD
     gw_str = f'_r{RAD:0.0f}km' if (RAD != 0) else ''
     # destriped GRACE/GRACE-FO coefficients
     ds_str = '_FL' if DESTRIPE else ''
     # distributing removed mass uniformly over ocean
     ocean_str = '_OCN' if REDISTRIBUTE_REMOVED else ''
     # input and output spatial units
-    unit_list = ['cmwe', 'mmGH', 'mmCU', u'\u03BCGal', 'mbar']
-    unit_name = ['Equivalent_Water_Thickness', 'Geoid_Height',
-        'Elastic_Crustal_Uplift', 'Gravitational_Undulation',
-        'Equivalent_Surface_Pressure']
+    # 1: cmwe, centimeters water equivalent
+    # 2: mmGH, millimeters geoid height
+    # 3: mmCU, millimeters elastic crustal deformation
+    # 4: micGal, microGal gravity perturbations
+    # 5: mbar, millibars equivalent surface pressure
+    units = gravtk.units.bycode(UNITS)
+    units_name, units_longname = gravtk.units.get_attributes(units)
 
     # input file format
     input_format = '{0}{1}_L{2:d}{3}{4}{5}_{6:03d}.{7}'
     # output file format
     output_format = '{0}{1}_L{2:d}{3}{4}{5}_{6}{7}_{8:03d}-{9:03d}.{10}'
 
     # GRACE months to read
@@ -162,101 +177,143 @@
     elif (INTERVAL == 2):
         # (Degree spacing)/2
         nlon = np.int64(360.0/dlon)
         nlat = np.int64(180.0/dlat)
     elif (INTERVAL == 3):
         # non-global grid set with BOUNDS parameter
         minlon,maxlon,minlat,maxlat = BOUNDS.copy()
-        lon = np.arange(minlon+dlon/2.0,maxlon+dlon/2.0,dlon)
-        lat = np.arange(maxlat-dlat/2.0,minlat-dlat/2.0,-dlat)
+        lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+        lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
         nlon = len(lon)
         nlat = len(lat)
 
-    # Setting output parameters for each fit type
-    coef_str = ['x{0:d}'.format(o) for o in range(ORDER+1)]
-    unit_suffix = [' yr^{0:d}'.format(-o) if o else '' for o in range(ORDER+1)]
-    if (ORDER == 0):# Mean
-        unit_longname = ['Mean']
-    elif (ORDER == 1):# Trend
-        unit_longname = ['Constant','Trend']
-    elif (ORDER == 2):# Quadratic
-        unit_longname = ['Constant','Linear','Quadratic']
-    # filename strings for cyclical terms
-    cyclic_str = {}
-    cyclic_str['SEMI'] = ['SS','SC']
-    cyclic_str['ANN'] = ['AS','AC']
-    cyclic_str['S2'] = ['S2S','S2C']
-    # unit longnames for cyclical terms
-    cyclic_longname = {}
-    cyclic_longname['SEMI'] = ['Semi-Annual Sine', 'Semi-Annual Cosine']
-    cyclic_longname['ANN'] = ['Annual Sine', 'Annual Cosine']
-    cyclic_longname['S2'] = ['S2 Tidal Alias Sine', 'S2 Tidal Alias Cosine']
-    amp_str = []
-    for i,c in enumerate(CYCLES):
-        if (c == 0.5):
-            flag = 'SEMI'
-        elif (c == 1.0):
-            flag = 'ANN'
-        elif (c == (161.0/365.25)):
-            flag = 'S2'
-        coef_str.extend(cyclic_str[flag])
-        unit_longname.extend(cyclic_longname[flag])
-        unit_suffix.extend(['',''])
-        amp_str.append(flag)
-
     # input data spatial object
     spatial_list = []
     for t,grace_month in enumerate(months):
         # input GRACE/GRACE-FO spatial file
-        fargs = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
+        fargs = (FILE_PREFIX, units, LMAX, order_str,
             gw_str, ds_str, grace_month, suffix)
-        input_file = os.path.join(OUTPUT_DIRECTORY,input_format.format(*fargs))
+        input_file = OUTPUT_DIRECTORY.joinpath(input_format.format(*fargs))
         # read GRACE/GRACE-FO spatial file
         if (DATAFORM == 'ascii'):
-            dinput = gravtk.spatial(spacing=[dlon,dlat], nlon=nlon,
-                nlat=nlat).from_ascii(input_file)
+            dinput = gravtk.spatial().from_ascii(input_file,
+                spacing=[dlon,dlat], nlon=nlon, nlat=nlat)
         elif (DATAFORM == 'netCDF4'):
             # netcdf (.nc)
             dinput = gravtk.spatial().from_netCDF4(input_file)
         elif (DATAFORM == 'HDF5'):
             # HDF5 (.H5)
             dinput = gravtk.spatial().from_HDF5(input_file)
         # append to spatial list
         dinput.month = grace_month
-        nlat,nlon = dinput.shape
+        nlat, nlon = dinput.shape
         spatial_list.append(dinput)
 
     # concatenate list to single spatial object
     grid = gravtk.spatial().from_list(spatial_list)
     spatial_list = None
 
+    # Setting output parameters for each fit type
+    coef_str = ['x{0:d}'.format(o) for o in range(ORDER+1)]
+    unit_suffix = [' yr^{0:d}'.format(-o) if o else '' for o in range(ORDER+1)]
+    if (ORDER == 0):# Mean
+        fit_longname = ['Mean']
+    elif (ORDER == 1):# Trend
+        fit_longname = ['Constant','Trend']
+    elif (ORDER == 2):# Quadratic
+        fit_longname = ['Constant','Linear','Quadratic']
+
+    # amplitude string for cyclical components
+    amp_str = []
+    # amplitude and phase titles for cyclical components
+    amp_title = {}
+    ph_title = {}
+    # unique tidal aliasing periods from Ray and Luthcke (2006)
+    tidal_aliasing = {}
+    tidal_aliasing['Q1'] = 9.1
+    tidal_aliasing['O1'] = 13.6
+    tidal_aliasing['P1'] = 171.2
+    tidal_aliasing['S1'] = 322.1
+    tidal_aliasing['K1'] = 2725.4
+    tidal_aliasing['J1'] = 27.8
+    tidal_aliasing['M2'] = 13.5
+    tidal_aliasing['S2'] = 161.0
+    tidal_aliasing['K2'] = 1362.7
+    # extra terms for tidal aliasing components or custom fits
+    TERMS = []
+    term_index = []
+    for i,c in enumerate(CYCLES):
+        # check if fitting with semi-annual or annual terms
+        if (c == 0.5):
+            coef_str.extend(['SS','SC'])
+            amp_str.append('SEMI')
+            amp_title['SEMI'] = 'Semi-Annual Amplitude'
+            ph_title['SEMI'] = 'Semi-Annual Phase'
+            fit_longname.extend(['Semi-Annual Sine', 'Semi-Annual Cosine'])
+            unit_suffix.extend(['',''])
+        elif (c == 1.0):
+            coef_str.extend(['AS','AC'])
+            amp_str.append('ANN')
+            amp_title['ANN'] = 'Annual Amplitude'
+            ph_title['ANN'] = 'Annual Phase'
+            fit_longname.extend(['Annual Sine', 'Annual Cosine'])
+            unit_suffix.extend(['',''])
+        # check if fitting with tidal aliasing terms
+        for t,period in tidal_aliasing.items():
+            if np.isclose(c, (period/365.25)):
+                # terms for tidal aliasing during GRACE and GRACE-FO periods
+                TERMS.extend(gravtk.time_series.aliasing_terms(grid.time,
+                    period=period))
+                # labels for tidal aliasing during GRACE period
+                coef_str.extend([f'{t}SGRC', f'{t}CGRC'])
+                amp_str.append(f'{t}GRC')
+                amp_title[f'{t}GRC'] = f'{t} Tidal Alias (GRACE) Amplitude'
+                ph_title[f'{t}GRC'] = f'{t} Tidal Alias (GRACE) Phase'
+                fit_longname.append(f'{t} Tidal Alias (GRACE) Sine')
+                fit_longname.append(f'{t} Tidal Alias (GRACE) Cosine')
+                unit_suffix.extend(['',''])
+                # labels for tidal aliasing during GRACE-FO period
+                coef_str.extend([f'{t}SGFO', f'{t}CGFO'])
+                amp_str.append(f'{t}GFO')
+                amp_title[f'{t}GFO'] = f'{t} Tidal Alias (GRACE-FO) Amplitude'
+                ph_title[f'{t}GFO'] = f'{t} Tidal Alias (GRACE-FO) Phase'
+                fit_longname.append(f'{t} Tidal Alias (GRACE-FO) Sine')
+                fit_longname.append(f'{t} Tidal Alias (GRACE-FO) Cosine')
+                unit_suffix.extend(['',''])
+                # index to remove the original tidal aliasing term
+                term_index.append(i)
+    # remove the original tidal aliasing terms
+    CYCLES = np.delete(CYCLES, term_index)
+
     # Fitting seasonal components
     ncomp = len(coef_str)
-    ncycles = 2*len(CYCLES)
+    ncycles = 2*len(CYCLES) + len(TERMS)
+    # confidence interval for regression fit errors
+    CONF = 0.95
 
     # Allocating memory for output variables
     out = dinput.zeros_like()
-    out.data = np.zeros((nlat,nlon,ncomp))
-    out.error = np.zeros((nlat,nlon,ncomp))
-    out.mask = np.ones((nlat,nlon,ncomp),dtype=bool)
+    out.data = np.zeros((nlat, nlon, ncomp))
+    out.error = np.zeros((nlat, nlon, ncomp))
+    out.mask = np.ones((nlat, nlon, ncomp),dtype=bool)
     # Fit Significance
     FS = {}
     # SSE: Sum of Squares Error
     # AIC: Akaike information criterion
     # BIC: Bayesian information criterion
     # R2Adj: Adjusted Coefficient of Determination
     for key in ['SSE','AIC','BIC','R2Adj']:
         FS[key] = dinput.zeros_like()
 
     # calculate the regression coefficients and fit significance
     for i in range(nlat):
         for j in range(nlon):
             # Calculating the regression coefficients
             tsbeta = gravtk.time_series.regress(grid.time, grid.data[i,j,:],
-                ORDER=ORDER, CYCLES=CYCLES, CONF=0.95)
+                ORDER=ORDER, CYCLES=CYCLES, TERMS=TERMS, CONF=CONF)
             # save regression components
             for k in range(0, ncomp):
                 out.data[i,j,k] = tsbeta['beta'][k]
                 out.error[i,j,k] = tsbeta['error'][k]
                 out.mask[i,j,k] = False
             # Fit significance terms
             # Degrees of Freedom
@@ -268,165 +325,157 @@
             FS['R2Adj'].data[i,j] = tsbeta['R2Adj']
 
     # list of output files
     output_files = []
     # Output spatial files
     for i in range(0,ncomp):
         # output spatial file name
-        f1 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
+        f1 = (FILE_PREFIX, units, LMAX, order_str,
             gw_str, ds_str, coef_str[i], '', START, END, suffix)
-        f2 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-            gw_str, ds_str, coef_str[i], '_ERROR', START, END, suffix)
-        file1 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f1))
-        file2 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f2))
+        file1 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f1))
         # full attributes
-        UNITS_TITLE = f'{unit_list[UNITS-1]}{unit_suffix[i]}'
-        LONGNAME = unit_name[UNITS-1]
-        FILE_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{unit_longname[i]}'
+        UNITS_TITLE = f'{units_name}{unit_suffix[i]}'
+        LONGNAME = units_longname
+        FILE_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{fit_longname[i]}'
         # output regression fit to file
         output = out.index(i, date=False)
         output_data(output, FILENAME=file1, DATAFORM=DATAFORM,
             UNITS=UNITS_TITLE, LONGNAME=LONGNAME, TITLE=FILE_TITLE,
-            VERBOSE=VERBOSE, MODE=MODE)
-        output_data(output, FILENAME=file2, DATAFORM=DATAFORM,
-            UNITS=UNITS_TITLE, LONGNAME=LONGNAME, TITLE=FILE_TITLE,
-            KEY='error', VERBOSE=VERBOSE, MODE=MODE)
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
         # add output files to list object
         output_files.append(file1)
-        output_files.append(file2)
 
     # if fitting coefficients with cyclical components
-    if (ncycles > 0):
-        # output spatial titles for amplitudes
-        amp_title = {'ANN':'Annual Amplitude','SEMI':'Semi-Annual Amplitude',
-            'S2':'S2 Tidal Alias Amplitude'}
-        ph_title = {'ANN':'Annual Phase','SEMI':'Semi-Annual Phase',
-            'S2':'S2 Tidal Alias Phase'}
-
-        # output amplitude and phase of cyclical components
-        for i,flag in enumerate(amp_str):
-            # Indice pointing to the cyclical components
-            j = 1 + ORDER + 2*i
-            # Allocating memory for output amplitude and phase
-            amp = dinput.zeros_like()
-            ph = dinput.zeros_like()
-            # calculating amplitude and phase of spatial field
-            amp.data,ph.data = gravtk.time_series.amplitude(
-                out.data[:,:,j], out.data[:,:,j+1]
-            )
-            # convert phase from -180:180 to 0:360
-            ii,jj = np.nonzero(ph.data < 0)
-            ph.data[ii,jj] += 360.0
-            # Amplitude Error
-            comp1 = out.error[:,:,j]*out.data[:,:,j]/amp.data
-            comp2 = out.error[:,:,j+1]*out.data[:,:,j+1]/amp.data
-            amp.error = np.sqrt(comp1**2 + comp2**2)
-            # Phase Error (degrees)
-            comp1 = out.error[:,:,j]*out.data[:,:,j+1]/(amp.data**2)
-            comp2 = out.error[:,:,j+1]*out.data[:,:,j]/(amp.data**2)
-            ph.error = (180.0/np.pi)*np.sqrt(comp1**2 + comp2**2)
-
-            # output file names for amplitude, phase and errors
-            f3 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-                gw_str, ds_str, flag, '', START, END, suffix)
-            f4 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-                gw_str, ds_str, flag,'_PHASE', START, END, suffix)
-            file3 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f3))
-            file4 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f4))
-            # output spatial error file name
-            f5 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-                gw_str, ds_str, flag, '_ERROR', START, END, suffix)
-            f6 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-                gw_str, ds_str, flag, '_PHASE_ERROR', START, END, suffix)
-            file5 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f5))
-            file6 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f6))
-            # full attributes
-            AMP_UNITS = unit_list[UNITS-1]
-            PH_UNITS = 'degrees'
-            LONGNAME = unit_name[UNITS-1]
-            AMP_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{amp_title[flag]}'
-            PH_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{ph_title[flag]}'
-            # Output seasonal amplitude and phase to files
-            output_data(amp, FILENAME=file3, DATAFORM=DATAFORM,
-                UNITS=AMP_UNITS, LONGNAME=LONGNAME, TITLE=AMP_TITLE,
-                VERBOSE=VERBOSE, MODE=MODE)
-            output_data(ph, FILENAME=file4, DATAFORM=DATAFORM,
-                UNITS=PH_UNITS, LONGNAME='Phase', TITLE=PH_TITLE,
-                VERBOSE=VERBOSE, MODE=MODE)
-            # Output seasonal amplitude and phase error to files
-            output_data(amp, FILENAME=file5, DATAFORM=DATAFORM,
-                UNITS=AMP_UNITS, LONGNAME=LONGNAME, TITLE=AMP_TITLE,
-                KEY='error', VERBOSE=VERBOSE, MODE=MODE)
-            output_data(ph, FILENAME=file6, DATAFORM=DATAFORM,
-                UNITS=PH_UNITS, LONGNAME='Phase', TITLE=PH_TITLE,
-                KEY='error', VERBOSE=VERBOSE, MODE=MODE)
-            # add output files to list object
-            output_files.append(file3)
-            output_files.append(file4)
-            output_files.append(file5)
-            output_files.append(file6)
+    # output amplitude and phase of cyclical components
+    for i,flag in enumerate(amp_str):
+        # Indice pointing to the cyclical components
+        j = 1 + ORDER + 2*i
+        # Allocating memory for output amplitude and phase
+        amp = dinput.zeros_like()
+        ph = dinput.zeros_like()
+        # calculating amplitude and phase of spatial field
+        amp.data,ph.data = gravtk.time_series.amplitude(
+            out.data[:,:,j], out.data[:,:,j+1]
+        )
+        # convert phase from -180:180 to 0:360
+        ii,jj = np.nonzero(ph.data < 0)
+        ph.data[ii,jj] += 360.0
+        # Amplitude Error
+        comp1 = out.error[:,:,j]*out.data[:,:,j]/amp.data
+        comp2 = out.error[:,:,j+1]*out.data[:,:,j+1]/amp.data
+        amp.error = np.sqrt(comp1**2 + comp2**2)
+        # Phase Error (degrees)
+        comp1 = out.error[:,:,j]*out.data[:,:,j+1]/(amp.data**2)
+        comp2 = out.error[:,:,j+1]*out.data[:,:,j]/(amp.data**2)
+        ph.error = (180.0/np.pi)*np.sqrt(comp1**2 + comp2**2)
+
+        # output file names for amplitude, phase and errors
+        f2 = (FILE_PREFIX, units, LMAX, order_str,
+            gw_str, ds_str, flag, '_AMPL', START, END, suffix)
+        f3 = (FILE_PREFIX, units, LMAX, order_str,
+            gw_str, ds_str, flag,'_PHASE', START, END, suffix)
+        file2 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f2))
+        file3 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f3))
+        # full attributes
+        AMP_UNITS = units_name
+        PH_UNITS = 'degrees'
+        LONGNAME = units_longname
+        AMP_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{amp_title[flag]}'
+        PH_TITLE = f'GRACE/GRACE-FO_Spatial_Data_{ph_title[flag]}'
+        # Output seasonal amplitude and phase to files
+        output_data(amp, FILENAME=file2, DATAFORM=DATAFORM,
+            UNITS=AMP_UNITS, LONGNAME=LONGNAME, TITLE=AMP_TITLE,
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
+        output_data(ph, FILENAME=file3, DATAFORM=DATAFORM,
+            UNITS=PH_UNITS, LONGNAME='Phase', TITLE=PH_TITLE,
+            CONF=CONF, VERBOSE=VERBOSE, MODE=MODE)
+        # add output files to list object
+        output_files.append(file2)
+        output_files.append(file3)
 
     # Output fit significance
-    signif_longname = {'SSE':'Sum of Squares Error',
-        'AIC':'Akaike information criterion',
-        'BIC':'Bayesian information criterion',
-        'R2Adj':'Adjusted Coefficient of Determination'}
+    signif_longname = {}
+    signif_longname['SSE'] = 'Sum of Squares Error'
+    signif_longname['AIC'] = 'Akaike information criterion'
+    signif_longname['BIC'] = 'Bayesian information criterion'
+    signif_longname['R2Adj'] = 'Adjusted Coefficient of Determination'
     # for each fit significance term
     for key,fs in FS.items():
         # output file names for fit significance
         signif_str = f'{key}_'
-        f7 = (FILE_PREFIX, unit_list[UNITS-1], LMAX, order_str,
-            gw_str, ds_str, signif_str, coef_str[ORDER], START, END, suffix)
-        file7 = os.path.join(OUTPUT_DIRECTORY,output_format.format(*f7))
+        f4 = (FILE_PREFIX, units, LMAX, order_str, gw_str, ds_str,
+            signif_str, coef_str[ORDER], START, END, suffix)
+        file4 = OUTPUT_DIRECTORY.joinpath(output_format.format(*f4))
         # full attributes
         LONGNAME = signif_longname[key]
         # output fit significance to file
-        output_data(fs, FILENAME=file7, DATAFORM=DATAFORM,
+        output_data(fs, FILENAME=file4, DATAFORM=DATAFORM,
             UNITS=key, LONGNAME=LONGNAME, TITLE=nu,
             VERBOSE=VERBOSE, MODE=MODE)
         # add output files to list object
-        output_files.append(os.path.join(OUTPUT_DIRECTORY,f7))
+        output_files.append(file4)
 
     # return the list of output files
     return output_files
 
 # PURPOSE: wrapper function for outputting data to file
-def output_data(data, FILENAME=None, KEY='data', DATAFORM=None,
-    UNITS=None, LONGNAME=None, TITLE=None, VERBOSE=0, MODE=0o775):
-    output = data.copy()
-    setattr(output,'data',getattr(data, KEY))
-    # attributes for output files
-    attributes = {}
-    attributes['units'] = UNITS
-    attributes['longname'] = LONGNAME
-    attributes['title'] = TITLE
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+def output_data(data, FILENAME=None, DATAFORM=None, UNITS=None,
+    LONGNAME=None, TITLE=None, CONF=0, VERBOSE=0, MODE=0o775):
+    # field mapping for output regression data
+    field_mapping = {}
+    field_mapping['lat'] = 'lat'
+    field_mapping['lon'] = 'lon'
+    field_mapping['data'] = 'data'
+    # add data error to output field mapping
+    if hasattr(data, 'error'):
+        field_mapping['error'] = 'error'
+    # output attributes
+    attributes = dict(ROOT={})
+    attributes['lon'] = {}
+    attributes['lon']['long_name'] = 'longitude'
+    attributes['lon']['units'] = 'degrees_east'
+    attributes['lat'] = {}
+    attributes['lat']['long_name'] = 'latitude'
+    attributes['lat']['units'] = 'degrees_north'
+    attributes['data'] = {}
+    attributes['data']['description'] = 'Model_fit'
+    attributes['data']['long_name'] = LONGNAME
+    attributes['data']['units'] = UNITS
+    attributes['error'] = {}
+    attributes['error']['description'] = 'Uncertainty_in_model_fit'
+    attributes['error']['long_name'] = LONGNAME
+    attributes['error']['units'] = UNITS
+    attributes['error']['confidence'] = 100*CONF
+    # output global attributes
+    REFERENCE = f'Output from {pathlib.Path(sys.argv[0]).name}'
     # write to output file
     if (DATAFORM == 'ascii'):
         # ascii (.txt)
-        output.to_ascii(FILENAME, date=False, verbose=VERBOSE)
+        data.to_ascii(FILENAME, date=False, verbose=VERBOSE)
     elif (DATAFORM == 'netCDF4'):
         # netcdf (.nc)
-        output.to_netCDF4(FILENAME, date=False, verbose=VERBOSE,
-            **attributes)
+        data.to_netCDF4(FILENAME, date=False, verbose=VERBOSE,
+            field_mapping=field_mapping, attributes=attributes,
+            title=TITLE, reference=REFERENCE)
     elif (DATAFORM == 'HDF5'):
         # HDF5 (.H5)
-        output.to_HDF5(FILENAME, date=False, verbose=VERBOSE,
-            **attributes)
+        data.to_HDF5(FILENAME, date=False, verbose=VERBOSE,
+            field_mapping=field_mapping, attributes=attributes,
+            title=TITLE, reference=REFERENCE)
     # change the permissions mode of the output file
-    os.chmod(FILENAME, MODE)
+    FILENAME.chmod(mode=MODE)
 
 # PURPOSE: print a file log for the GRACE/GRACE-FO regression
 def output_log_file(input_arguments, output_files):
     # format: GRACE_processing_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_processing_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
@@ -437,16 +486,16 @@
 
 # PURPOSE: print a error file log for the GRACE/GRACE-FO regression
 def output_error_log_file(input_arguments):
     # format: GRACE_processing_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
     LOGFILE = 'GRACE_processing_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
@@ -461,16 +510,15 @@
             trends at each grid point following an input regression model
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for spatial files')
     parser.add_argument('--file-prefix','-P',
         type=str,
         help='Prefix string for input and output files')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
         type=int, default=60,
```

### Comparing `gravity-toolkit-1.2.0/scripts/run_grace_date.py` & `gravity-toolkit-1.2.1/scripts/run_grace_date.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 #!/usr/bin/env python
 u"""
 run_grace_date.py
-Written by Tyler Sutterley (12/2022)
+Written by Tyler Sutterley (05/2023)
 
 Wrapper program for running GRACE date and months programs
 
 Processing Centers: [CSR, GFZ, JPL]
 Data Releases CSR, GFZ, JPL: RL06
 Data Products: [GAA, GAB, GAC, GAD, GSM]
     CSR: (GAC, GAD, GSM) only
@@ -43,14 +43,15 @@
 
 PROGRAM DEPENDENCIES:
     grace_date.py: computes the dates of the GRACE/GRACE-FO datasets
     time.py: utilities for calculating time operations
     grace_months_index.py: creates a single file showing the GRACE dates
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 04/2022: use argparse descriptions within documentation
     Updated 12/2021: can use variable loglevels for verbose output
     Updated 10/2021: using python logging for handling verbose output
     Updated 09/2021: using verbose option to track program progress
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 10/2020: use argparse to set command line parameters
@@ -66,16 +67,16 @@
     Updated 12/2014: added main definition for running from the command line
     Updated 02/2014: minor update to if statements
     Written 07/2012
 """
 from __future__ import print_function
 
 import sys
-import os
 import logging
+import pathlib
 import argparse
 import gravity_toolkit as gravtk
 
 def run_grace_date(base_dir, PROC, DREL, VERBOSE=0, MODE=0o775):
     # create logger
     loglevels = [logging.CRITICAL, logging.INFO, logging.DEBUG]
     logging.basicConfig(level=loglevels[VERBOSE])
@@ -121,16 +122,15 @@
         description="""Wrapper program for running GRACE date and
             months programs
             """
     )
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, nargs='+',
         default=['CSR','GFZ','JPL'],
         choices=['CSR','GFZ','JPL'],
         help='GRACE/GRACE-FO Processing Center')
```

### Comparing `gravity-toolkit-1.2.0/scripts/run_sea_level_equation.py` & `gravity-toolkit-1.2.1/scripts/run_sea_level_equation.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 #!/usr/bin/env python
 u"""
-run_sea_level_equation.py (03/2023)
+run_sea_level_equation.py (05/2023)
 Solves the sea level equation with the option of including polar motion feedback
 Uses a Clenshaw summation to calculate the spherical harmonic summation
 
 CALLING SEQUENCE:
     python run_sea_level_equation.py --lmax 240 --body 0 --fluid 0 \
         --polar-feedback --reference CF --iterations 6 --format netCDF4 \
         --verbose --mode 0o775 input_file output_file
@@ -64,14 +64,15 @@
         the Recursive Computation of Very High Degree and Order Normalised
         Associated Legendre Functions", Journal of Geodesy (2002)
         http://dx.doi.org/10.1007/s00190-002-0216-2
     Tscherning and Poder, "Some Geodetic Applications of Clenshaw Summation",
         Bollettino di Geodesia e Scienze (1982)
 
 UPDATE HISTORY:
+    Updated 05/2023: use pathlib to define and operate on paths
     Updated 03/2023: add root attributes to output netCDF4 and HDF5 files
     Updated 02/2023: use love numbers class with additional attributes
     Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 04/2022: use wrapper function for reading load Love numbers
         use argparse descriptions within sphinx documentation
@@ -107,23 +108,24 @@
 """
 from __future__ import print_function
 
 import sys
 import os
 import re
 import logging
+import pathlib
 import argparse
 import traceback
 import numpy as np
 import collections
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: Computes Sea Level Fingerprints including polar motion feedback
@@ -136,30 +138,34 @@
     REFERENCE=None,
     ITERATIONS=0,
     POLAR=False,
     DATAFORM=None,
     DATE=False,
     MODE=0o775):
 
+    # set default paths
+    INPUT_FILE = pathlib.Path(INPUT_FILE).expanduser().absolute()
+    OUTPUT_FILE = pathlib.Path(OUTPUT_FILE).expanduser().absolute()
+    LANDMASK = pathlib.Path(LANDMASK).expanduser().absolute()
     # output attributes for spatial files
     attributes = collections.OrderedDict()
-    attributes['lineage'] = os.path.basename(INPUT_FILE)
-    attributes['land_sea_mask'] = os.path.basename(LANDMASK)
+    attributes['lineage'] = INPUT_FILE.name
+    attributes['land_sea_mask'] = LANDMASK.name
     attributes['title'] = 'Sea_Level_Fingerprint'
     attributes['max_degree'] = LMAX
     attributes['iterations'] = ITERATIONS
 
     # Land-Sea Mask with Antarctica from Rignot (2017) and Greenland from GEUS
     # 0=Ocean, 1=Land, 2=Lake, 3=Small Island, 4=Ice Shelf
     # Open the land-sea NetCDF file for reading
     landsea = gravtk.spatial().from_netCDF4(LANDMASK, date=False,
         varname='LSMASK')
     # create land function
     nth,nphi = landsea.shape
-    land_function = np.zeros((nth,nphi),dtype=np.float64)
+    land_function = np.zeros((nth, nphi), dtype=np.float64)
     # calculate colatitude in radians
     th = (90.0 - landsea.lat)*np.pi/180.0
     # extract land function from file
     # combine land and island levels for land function
     indx,indy = np.nonzero((landsea.data >= 1) & (landsea.data <= 3))
     land_function[indx,indy] = 1.0
 
@@ -218,15 +224,15 @@
     # copy dimensions
     sea_level.lon = np.copy(landsea.lon)
     sea_level.lat = np.copy(landsea.lat)
     sea_level.time = np.copy(load_Ylms.time) if DATE else None
     # remove singleton dimensions if necessary
     sea_level.squeeze()
     # add attributes to output spatial field
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
     sea_level.attributes['ROOT'] = attributes
 
     # attributes for output files
     kwargs = {}
     kwargs['units'] = 'centimeters'
     kwargs['longname'] = 'Equivalent_Water_Thickness'
     # save as output DATAFORM
@@ -239,37 +245,37 @@
     elif (DATAFORM == 'netCDF4'):
         # netCDF4 (.nc)
         sea_level.to_netCDF4(OUTPUT_FILE, date=DATE, **kwargs)
     elif (DATAFORM == 'HDF5'):
         # HDF5 (.H5)
         sea_level.to_HDF5(OUTPUT_FILE, date=DATE, **kwargs)
     # set the permissions mode of the output file
-    os.chmod(OUTPUT_FILE, MODE)
+    OUTPUT_FILE.chmod(mode=MODE)
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
         description="""Solves the sea level equation with the option of
             including polar motion feedback
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # input and output file
     parser.add_argument('infile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Input load file')
     parser.add_argument('outfile',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='?',
+        type=pathlib.Path, nargs='?',
         help='Output sea level fingerprints file')
     # land mask file
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for calculating sea level fingerprints')
     # maximum spherical harmonic degree and order
     parser.add_argument('--lmax','-l',
         type=int, default=240,
         help='Maximum spherical harmonic degree')
     # different treatments of the load Love numbers
     # 0: Han and Wahr (1995) values from PREM
```

### Comparing `gravity-toolkit-1.2.0/scripts/scale_grace_maps.py` & `gravity-toolkit-1.2.1/scripts/grace_spatial_maps.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 #!/usr/bin/env python
 u"""
-scale_grace_maps.py
-Written by Tyler Sutterley (03/2023)
+grace_spatial_maps.py
+Written by Tyler Sutterley (05/2023)
 
 Reads in GRACE/GRACE-FO spherical harmonic coefficients and exports
-    monthly scaled spatial fields, estimated scaling errors,
-    and estimated scaled delta errors
+    monthly spatial fields
 
 Will correct with the specified GIA model group, destripe/smooth/process,
-    and export the data in centimeters water equivalent (cm w.e.)
+    and export the data in specified units
+
+Spatial output units: cm w.e., mm geoid height, mm elastic uplift,
+    microgal gravity perturbation or surface pressure (mbar)
 
 COMMAND LINE OPTIONS:
     --help: list the command line options
     -D X, --directory X: Working data directory
     -O X, --output-directory X: output directory for spatial files
     -P X, --file-prefix X: prefix string for input and output files
     -c X, --center X: GRACE/GRACE-FO processing center
@@ -79,18 +81,26 @@
         GSFC: use values from GSFC (TN-14)
     --slr-c40 X: Replace C40 coefficients with SLR values
         CSR: use values from CSR (5x5 with 6,1)
         GSFC: use values from GSFC
     --slr-c50 X: Replace C50 coefficients with SLR values
         CSR: use values from CSR (5x5 with 6,1)
         GSFC: use values from GSFC
+    -U X, --units X: output units
+        1: cm of water thickness
+        2: mm of geoid height
+        3: mm of elastic crustal deformation [Davis 2004]
+        4: microGal gravitational perturbation
+        5: mbar equivalent surface pressure
     --spacing X: spatial resolution of output data (dlon,dlat)
     --interval X: output grid interval
         1: (0:360, 90:-90)
         2: (degree spacing/2)
+        3: non-global grid (set with defined bounds)
+    --bounds X: non-global grid bounding box (minlon,maxlon,minlat,maxlat)
     --mean-file X: GRACE/GRACE-FO mean file to remove from the harmonic data
     --mean-format X: Input data format for GRACE/GRACE-FO mean file
         ascii
         netCDF4
         HDF5
         gfc
     --mask X: Land-sea mask for redistributing land water flux
@@ -99,125 +109,122 @@
         ascii
         netCDF4
         HDF5
         index-ascii
         index-netCDF4
         index-HDF5
     --redistribute-removed: redistribute removed mass fields over the ocean
-    --scale-file X: scaling factor file
     --log: Output log of files created for each job
     -V, --verbose: verbose output of processing run
     -M X, --mode X: Permissions mode of the files created
 
 PYTHON DEPENDENCIES:
     numpy: Scientific Computing Tools For Python
         https://numpy.org
         https://numpy.org/doc/stable/user/numpy-for-matlab-users.html
     dateutil: powerful extensions to datetime
         https://dateutil.readthedocs.io/en/stable/
     netCDF4: Python interface to the netCDF C library
         https://unidata.github.io/netcdf4-python/netCDF4/index.html
     h5py: Pythonic interface to the HDF5 binary data format.
         https://www.h5py.org/
-    future: Compatibility layer between Python 2 and Python 3
-        https://python-future.org/
 
 PROGRAM DEPENDENCIES:
-    grace_input_months.py: Reads GRACE/GRACE-FO files for a specified date range
-        Includes degree 1 values (if specified)
+    grace_input_months.py: Reads GRACE/GRACE-FO files for a specified spherical
+            harmonic degree and order and for a specified date range
+        Includes degree 1 with with Swenson values (if specified)
         Replaces low-degree harmonics with SLR values (if specified)
-    read_GIA_model.py: reads spherical harmonics for glacial isostatic adjustment
+    read_GIA_model.py: reads harmonics for a glacial isostatic adjustment model
     read_love_numbers.py: reads Load Love Numbers from Han and Wahr (1995)
+    associated_legendre.py: Computes fully normalized associated
+        Legendre polynomials
     gauss_weights.py: Computes the Gaussian weights as a function of degree
-    ocean_stokes.py: reads a land-sea mask and converts to spherical harmonics
-    gen_stokes.py: converts a spatial field into spherical harmonic coefficients
+    ocean_stokes.py: converts a land-sea mask to a series of spherical harmonics
+    gen_stokes.py: converts a spatial field into a series of spherical harmonics
     geocenter.py: converts between spherical harmonics and geocenter variations
     harmonic_summation.py: calculates a spatial field from spherical harmonics
-    time_series.smooth.py: smoothes a time-series using a Loess-type algorithm
+    units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     harmonics.py: spherical harmonic data class for processing GRACE/GRACE-FO
     destripe_harmonics.py: calculates the decorrelation (destriping) filter
         and filters the GRACE/GRACE-FO coefficients for striping errors
     spatial.py: spatial data class for reading, writing and processing data
-    time.py: utilities for calculating time operations
-    units.py: class for converting GRACE/GRACE-FO Level-2 data to specific units
     utilities.py: download and management utilities for files
 
-REFERENCES:
-    C-W Hsu and I Velicogna, "Detection of Sea Level Fingerprints derived from
-        GRACE gravity data", Geophysical Research Letters, 44(17), (2017).
-        https://doi.org/10.1002/2017GL074070
-
-    F W Landerer and S C Swenson, "Accuracy of scaled GRACE terrestrial water
-        storage estimates", Water Resources Research, 48(4), W04531, (2012).
-        https://doi.org/10.1029/2011WR011453
-
-    J Wahr, S C Swenson, and I Velicogna, "Accuracy of GRACE mass estimates",
-        Geophysical Research Letters, 33(6), L06401, (2006).
-        https://doi.org/10.1029/2005GL025305
-
 UPDATE HISTORY:
-    Updated 03/2023: use new scaling_factors inheritance of spatial class
-        single input file with scaling factor variables
-    Updated 02/2023: use love numbers class with additional attributes
-    Updated 01/2023: refactored time series analysis functions
+    Updated 05/2023: use pathlib to define and operate on paths
+    Updated 03/2023: add root attributes to output netCDF4 and HDF5 files
+        use attributes from units class for writing to netCDF4/HDF5 files
+    Updated 02/2023: use get function to retrieve specific units
+        use love numbers class with additional attributes
+    Updated 01/2023: refactored associated legendre polynomials
     Updated 12/2022: single implicit import of gravity toolkit
     Updated 11/2022: use f-strings for formatting verbose or ascii output
     Updated 09/2022: add option to replace degree 4 zonal harmonics with SLR
+    Updated 07/2022: create mask for output gridded variables
     Updated 04/2022: use wrapper function for reading load Love numbers
         use argparse descriptions within sphinx documentation
     Updated 12/2021: can use variable loglevels for verbose output
         option to specify a specific geocenter correction file
         fix default file prefix to include center and release information
     Updated 11/2021: add GSFC low-degree harmonics
     Updated 10/2021: using python logging for handling verbose output
         add more choices for setting input format of the removed files
-    Updated 07/2021: switch from parameter files to argparse arguments
-        simplified file imports using wrappers in harmonics
+    Updated 07/2021: simplified file imports using wrappers in harmonics
         added path to default land-sea mask for mass redistribution
         remove choices for argparse processing centers
+    Updated 06/2021: switch from parameter files to argparse arguments
     Updated 05/2021: define int/float precision to prevent deprecation warning
     Updated 04/2021: include parameters for replacing C21/S21 and C22/S22
     Updated 02/2021: changed remove index to files with specified formats
-    Updated 02/2021: for public release
+    Updated 01/2021: harmonics object output from gen_stokes.py/ocean_stokes.py
+    Updated 12/2020: added more love number options and from gfc for mean files
+    Updated 10/2020: use argparse to set command line parameters
+    Updated 08/2020: use utilities to define path to load love numbers file
+    Updated 06/2020: using spatial data class for output operations
+    Updated 05/2020: for public release
 """
-from __future__ import print_function, division
+from __future__ import print_function
 
 import sys
 import os
+import re
 import copy
 import time
 import logging
-import argparse
+import pathlib
 import numpy as np
+import argparse
 import traceback
+import collections
 import gravity_toolkit as gravtk
 
 # PURPOSE: keep track of threads
 def info(args):
-    logging.info(os.path.basename(sys.argv[0]))
+    logging.info(pathlib.Path(sys.argv[0]).name)
     logging.info(args)
     logging.info(f'module name: {__name__}')
     if hasattr(os, 'getppid'):
         logging.info(f'parent process: {os.getppid():d}')
     logging.info(f'process id: {os.getpid():d}')
 
 # PURPOSE: import GRACE/GRACE-FO files for a given months range
-# Calculates monthly scaled spatial maps from GRACE/GRACE-FO
-# spherical harmonic coefficients
-def scale_grace_maps(base_dir, PROC, DREL, DSET, LMAX, RAD,
+# Converts the GRACE/GRACE-FO harmonics applying the specified procedures
+def grace_spatial_maps(base_dir, PROC, DREL, DSET, LMAX, RAD,
     START=None,
     END=None,
     MISSING=None,
     LMIN=None,
     MMAX=None,
     LOVE_NUMBERS=0,
     REFERENCE=None,
     DESTRIPE=False,
+    UNITS=None,
     DDEG=None,
     INTERVAL=None,
+    BOUNDS=None,
     GIA=None,
     GIA_FILE=None,
     ATM=False,
     POLE_TIDE=False,
     DEG1=None,
     DEG1_FILE=None,
     MODEL_DEG1=False,
@@ -229,134 +236,126 @@
     SLR_C50=None,
     DATAFORM=None,
     MEAN_FILE=None,
     MEANFORM=None,
     REMOVE_FILES=None,
     REMOVE_FORMAT=None,
     REDISTRIBUTE_REMOVED=False,
-    SCALE_FILE=None,
     LANDMASK=None,
     OUTPUT_DIRECTORY=None,
     FILE_PREFIX=None,
     VERBOSE=0,
     MODE=0o775):
 
-    # recursively create output Directory if not currently existing
-    if not os.access(OUTPUT_DIRECTORY, os.F_OK):
-        os.makedirs(OUTPUT_DIRECTORY, mode=MODE, exist_ok=True)
-
+    # recursively create output directory if not currently existing
+    OUTPUT_DIRECTORY = pathlib.Path(OUTPUT_DIRECTORY).expanduser().absolute()
+    OUTPUT_DIRECTORY.mkdir(mode=MODE, parents=True, exist_ok=True)
+
+    # output attributes for spatial files
+    attributes = collections.OrderedDict()
+    attributes['generating_institute'] = PROC
+    attributes['product_release'] = DREL
+    attributes['product_name'] = DSET
+    attributes['product_type'] = 'gravity_field'
+    attributes['title'] = 'GRACE/GRACE-FO Spatial Data'
     # list object of output files for file logs (full path)
     output_files = []
 
     # file information
     suffix = dict(ascii='txt', netCDF4='nc', HDF5='H5')
-    # output file format
-    file_format = '{0}{1}{2}_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
 
     # read arrays of kl, hl, and ll Love Numbers
     LOVE = gravtk.load_love_numbers(LMAX, LOVE_NUMBERS=LOVE_NUMBERS,
         REFERENCE=REFERENCE, FORMAT='class')
-
-    # atmospheric ECMWF "jump" flag (if ATM)
-    atm_str = '_wATM' if ATM else ''
-    # output string for both LMAX==MMAX and LMAX != MMAX cases
-    MMAX = np.copy(LMAX) if not MMAX else MMAX
-    order_str = f'M{MMAX:d}' if (MMAX != LMAX) else ''
-    # output spatial units
-    unit_str = 'cmwe'
-    unit_name = 'Equivalent_Water_Thickness'
-    # invalid value
-    fill_value = -9999.0
+    # add attributes for earth model and love numbers
+    attributes['earth_model'] = LOVE.model
+    attributes['earth_love_numbers'] = LOVE.citation
+    attributes['reference_frame'] = LOVE.reference
 
     # Calculating the Gaussian smoothing for radius RAD
     if (RAD != 0):
         wt = 2.0*np.pi*gravtk.gauss_weights(RAD,LMAX)
         gw_str = f'_r{RAD:0.0f}km'
+        attributes['smoothing_radius'] = f'{RAD:0.0f} km'
     else:
         # else = 1
         wt = np.ones((LMAX+1))
         gw_str = ''
 
-    # Read Ocean function and convert to Ylms for redistribution
-    if REDISTRIBUTE_REMOVED:
-        # read Land-Sea Mask and convert to spherical harmonics
-        ocean_Ylms = gravtk.ocean_stokes(LANDMASK, LMAX, MMAX=MMAX,
-            LOVE=LOVE)
-
-    # Grid spacing
-    dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
-    # Grid dimensions
-    if (INTERVAL == 1):# (0:360, 90:-90)
-        nlon = np.int64((360.0/dlon)+1.0)
-        nlat = np.int64((180.0/dlat)+1.0)
-    elif (INTERVAL == 2):# degree spacing/2
-        nlon = np.int64((360.0/dlon))
-        nlat = np.int64((180.0/dlat))
-
-    # field mapping for input spatial variables
-    field_mapping = dict(lon='lon', lat='lat', data='kfactor',
-        error='error', magnitude='power')
-    # read data for input scale files (ascii, netCDF4, HDF5)
-    if (DATAFORM == 'ascii'):
-        kfactor = gravtk.scaling_factors(spacing=[dlon,dlat], nlat=nlat,
-            nlon=nlon).from_ascii(SCALE_FILE)
-    elif (DATAFORM == 'netCDF4'):
-        kfactor = gravtk.scaling_factors().from_netCDF4(SCALE_FILE,
-            date=False, field_mapping=field_mapping)
-    elif (DATAFORM == 'HDF5'):
-        kfactor = gravtk.scaling_factors().from_HDF5(SCALE_FILE,
-            date=False, field_mapping=field_mapping)
-    # input data shape
-    nlat,nlon = kfactor.shape
+    # flag for spherical harmonic order
+    MMAX = np.copy(LMAX) if not MMAX else MMAX
+    order_str = f'M{MMAX:d}' if (MMAX != LMAX) else ''
+    # add attributes for LMAX and MMAX
+    attributes['max_degree'] = LMAX
+    attributes['max_order'] = MMAX
 
-    # input GRACE/GRACE-FO spherical harmonic datafiles for date range
+    # reading GRACE months for input date range
     # replacing low-degree harmonics with SLR values if specified
     # include degree 1 (geocenter) harmonics if specified
     # correcting for Pole-Tide and Atmospheric Jumps if specified
     Ylms = gravtk.grace_input_months(base_dir, PROC, DREL, DSET, LMAX,
         START, END, MISSING, SLR_C20, DEG1, MMAX=MMAX, SLR_21=SLR_21,
         SLR_22=SLR_22, SLR_C30=SLR_C30, SLR_C40=SLR_C40, SLR_C50=SLR_C50,
         DEG1_FILE=DEG1_FILE, MODEL_DEG1=MODEL_DEG1, ATM=ATM,
         POLE_TIDE=POLE_TIDE)
-    # create harmonics object from GRACE/GRACE-FO data
+    # convert to harmonics object and remove mean if specified
     GRACE_Ylms = gravtk.harmonics().from_dict(Ylms)
+    # add attributes for input GRACE/GRACE-FO spherical harmonics
+    for att_name, att_val in Ylms['attributes'].items():
+        attributes[att_name] = att_val
     # use a mean file for the static field to remove
     if MEAN_FILE:
         # read data form for input mean file (ascii, netCDF4, HDF5, gfc)
+        MEAN_FILE = pathlib.Path(MEAN_FILE).expanduser().absolute()
         mean_Ylms = gravtk.harmonics().from_file(MEAN_FILE,
             format=MEANFORM, date=False)
         # remove the input mean
         GRACE_Ylms.subtract(mean_Ylms)
+        attributes['lineage'].append(MEAN_FILE.name)
     else:
         GRACE_Ylms.mean(apply=True)
-    # date information of GRACE/GRACE-FO coefficients
-    nfiles = len(GRACE_Ylms.time)
 
     # filter GRACE/GRACE-FO coefficients
     if DESTRIPE:
         # destriping GRACE/GRACE-FO coefficients
         ds_str = '_FL'
         GRACE_Ylms = GRACE_Ylms.destripe()
+        attributes['filtering'] = 'Destriped'
     else:
         # using standard GRACE/GRACE-FO harmonics
         ds_str = ''
 
     # input GIA spherical harmonic datafiles
+    GIA_FILE = pathlib.Path(GIA_FILE).expanduser().absolute() if GIA else None
     GIA_Ylms_rate = gravtk.gia(lmax=LMAX).from_GIA(GIA_FILE, GIA=GIA, mmax=MMAX)
-    gia_str = f'_{GIA_Ylms_rate.title}' if GIA else ''
+    # output GIA string for filename
+    if GIA:
+        gia_str = f'_{GIA_Ylms_rate.title}'
+        attributes['GIA'] = (str(GIA_Ylms_rate.citation), GIA_FILE.name)
+    else:
+        gia_str = ''
     # monthly GIA calculated by gia_rate*time elapsed
     # finding change in GIA each month
     GIA_Ylms = GIA_Ylms_rate.drift(GRACE_Ylms.time, epoch=2003.3)
     GIA_Ylms.month[:] = np.copy(GRACE_Ylms.month)
 
     # default file prefix
     if not FILE_PREFIX:
         fargs = (PROC,DREL,DSET,Ylms['title'],gia_str)
         FILE_PREFIX = '{0}_{1}_{2}{3}{4}_'.format(*fargs)
 
+    # Read Ocean function and convert to Ylms for redistribution
+    if REDISTRIBUTE_REMOVED:
+        # read Land-Sea Mask and convert to spherical harmonics
+        ocean_Ylms = gravtk.ocean_stokes(LANDMASK, LMAX,
+            MMAX=MMAX, LOVE=LOVE)
+        ocean_str = '_OCN'
+    else:
+        ocean_str = ''
+
     # input spherical harmonic datafiles to be removed from the GRACE data
     # Remove sets of Ylms from the GRACE data before returning
     remove_Ylms = GRACE_Ylms.zeros_like()
     remove_Ylms.time[:] = np.copy(GRACE_Ylms.time)
     remove_Ylms.month[:] = np.copy(GRACE_Ylms.month)
     if REMOVE_FILES:
         # extend list if a single format was entered for all files
@@ -366,20 +365,22 @@
         for REMOVE_FILE,REMOVEFORM in zip(REMOVE_FILES,REMOVE_FORMAT):
             if REMOVEFORM in ('ascii','netCDF4','HDF5'):
                 # ascii (.txt)
                 # netCDF4 (.nc)
                 # HDF5 (.H5)
                 Ylms = gravtk.harmonics().from_file(REMOVE_FILE,
                     format=REMOVEFORM)
+                attributes['lineage'].append(Ylms.name)
             elif REMOVEFORM in ('index-ascii','index-netCDF4','index-HDF5'):
                 # read from index file
                 _,removeform = REMOVEFORM.split('-')
                 # index containing files in data format
                 Ylms = gravtk.harmonics().from_index(REMOVE_FILE,
                     format=removeform)
+                attributes['lineage'].extend([f.name for f in Ylms.filename])
             # reduce to GRACE/GRACE-FO months and truncate to degree and order
             Ylms = Ylms.subset(GRACE_Ylms.month).truncate(lmax=LMAX,mmax=MMAX)
             # distribute removed Ylms uniformly over the ocean
             if REDISTRIBUTE_REMOVED:
                 # calculate ratio between total removed mass and
                 # a uniformly distributed cm of water over the ocean
                 ratio = Ylms.clm[0,0,:]/ocean_Ylms.clm[0,0]
@@ -394,298 +395,155 @@
             if DESTRIPE:
                 Ylms = Ylms.destripe()
             # add data for month t and INDEX_FILE to the total
             # remove_clm and remove_slm matrices
             # redistributing the mass over the ocean if specified
             remove_Ylms.add(Ylms)
 
-    # calculating GRACE/GRACE-FO error (Wahr et al. 2006)
-    # output GRACE error file (for both LMAX==MMAX and LMAX != MMAX cases)
-    fargs = (PROC,DREL,DSET,LMAX,order_str,ds_str,atm_str,GRACE_Ylms.month[0],
-        GRACE_Ylms.month[-1], suffix[DATAFORM])
-    delta_format = '{0}_{1}_{2}_DELTA_CLM_L{3:d}{4}{5}{6}_{7:03d}-{8:03d}.{9}'
-    GRACE_Ylms.directory = Ylms['directory']
-    DELTA_FILE = os.path.join(GRACE_Ylms.directory,delta_format.format(*fargs))
-    # check full path of the GRACE directory for delta file
-    # if file was previously calculated: will read file
-    # else: will calculate the GRACE/GRACE-FO error
-    if not os.access(DELTA_FILE, os.F_OK):
-        # add output delta file to list object
-        output_files.append(DELTA_FILE)
-
-        # Delta coefficients of GRACE time series (Error components)
-        delta_Ylms = gravtk.harmonics(lmax=LMAX,mmax=MMAX)
-        delta_Ylms.clm = np.zeros((LMAX+1,MMAX+1))
-        delta_Ylms.slm = np.zeros((LMAX+1,MMAX+1))
-        # Smoothing Half-Width (CNES is a 10-day solution)
-        # All other solutions are monthly solutions (HFWTH for annual = 6)
-        if ((PROC == 'CNES') and (DREL in ('RL01','RL02'))):
-            HFWTH = 19
-        else:
-            HFWTH = 6
-        # Equal to the noise of the smoothed time-series
-        # for each spherical harmonic order
-        for m in range(0,MMAX+1):# MMAX+1 to include MMAX
-            # for each spherical harmonic degree
-            for l in range(m,LMAX+1):# LMAX+1 to include LMAX
-                # Delta coefficients of GRACE time series
-                for cs,csharm in enumerate(['clm','slm']):
-                    # calculate GRACE Error (Noise of smoothed time-series)
-                    # With Annual and Semi-Annual Terms
-                    val1 = getattr(GRACE_Ylms, csharm)
-                    smth = gravtk.time_series.smooth(GRACE_Ylms.time,
-                        val1[l,m,:], HFWTH=HFWTH)
-                    # number of smoothed points
-                    nsmth = len(smth['data'])
-                    tsmth = np.mean(smth['time'])
-                    # GRACE/GRACE-FO delta Ylms
-                    # variance of data-(smoothed+annual+semi)
-                    val2 = getattr(delta_Ylms, csharm)
-                    val2[l,m] = np.sqrt(np.sum(smth['noise']**2)/nsmth)
-
-        # attributes for output files
-        attributes = {}
-        attributes['title'] = 'GRACE/GRACE-FO Spherical Harmonic Errors'
-        attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
-        # save GRACE/GRACE-FO delta harmonics to file
-        delta_Ylms.time = np.copy(tsmth)
-        delta_Ylms.month = np.int64(nsmth)
-        delta_Ylms.to_file(DELTA_FILE, format=DATAFORM, **attributes)
-        # set the permissions mode of the output harmonics file
-        os.chmod(DELTA_FILE, MODE)
-        # append delta harmonics file to output files list
-        output_files.append(DELTA_FILE)
-    else:
-        # read GRACE/GRACE-FO delta harmonics from file
-        delta_Ylms = gravtk.harmonics().from_file(DELTA_FILE, format=DATAFORM)
-        # copy time and number of smoothed fields
-        tsmth = np.squeeze(delta_Ylms.time)
-        nsmth = np.int64(delta_Ylms.month)
-
     # Output spatial data object
     grid = gravtk.spatial()
-    grid.lon = np.copy(kfactor.lon)
-    grid.lat = np.copy(kfactor.lat)
-    grid.time = np.zeros((nfiles))
-    grid.month = np.zeros((nfiles),dtype=np.int64)
-    grid.data = np.zeros((nlat,nlon,nfiles))
-    grid.mask = np.zeros((nlat,nlon,nfiles),dtype=bool)
+    # Output Degree Spacing
+    dlon,dlat = (DDEG[0],DDEG[0]) if (len(DDEG) == 1) else (DDEG[0],DDEG[1])
+    # Output Degree Interval
+    if (INTERVAL == 1):
+        # (-180:180,90:-90)
+        nlon = np.int64((360.0/dlon)+1.0)
+        nlat = np.int64((180.0/dlat)+1.0)
+        grid.lon = -180 + dlon*np.arange(0,nlon)
+        grid.lat = 90.0 - dlat*np.arange(0,nlat)
+    elif (INTERVAL == 2):
+        # (Degree spacing)/2
+        grid.lon = np.arange(-180+dlon/2.0,180+dlon/2.0,dlon)
+        grid.lat = np.arange(90.0-dlat/2.0,-90.0-dlat/2.0,-dlat)
+        nlon = len(grid.lon)
+        nlat = len(grid.lat)
+    elif (INTERVAL == 3):
+        # non-global grid set with BOUNDS parameter
+        minlon,maxlon,minlat,maxlat = BOUNDS.copy()
+        grid.lon = np.arange(minlon+dlon/2.0, maxlon+dlon/2.0, dlon)
+        grid.lat = np.arange(maxlat-dlat/2.0, minlat-dlat/2.0, -dlat)
+        nlon = len(grid.lon)
+        nlat = len(grid.lat)
 
     # Computing plms for converting to spatial domain
-    phi = grid.lon[np.newaxis,:]*np.pi/180.0
     theta = (90.0-grid.lat)*np.pi/180.0
     PLM, dPLM = gravtk.plm_holmes(LMAX, np.cos(theta))
-    # square of legendre polynomials truncated to order MMAX
-    mm = np.arange(0,MMAX+1)
-    PLM2 = PLM[:,mm,:]**2
-
-    # dfactor is the degree dependent coefficients
-    # for converting to centimeters water equivalent (cmwe)
-    dfactor = gravtk.units(lmax=LMAX).harmonic(*LOVE).cmwe
 
+    # output spatial units
+    # Setting units factor for output
+    # dfactor computes the degree dependent coefficients
+    factors = gravtk.units(lmax=LMAX).harmonic(*LOVE)
+    # 1: cmwe, centimeters water equivalent
+    # 2: mmGH, millimeters geoid height
+    # 3: mmCU, millimeters elastic crustal deformation
+    # 4: micGal, microGal gravity perturbations
+    # 5: mbar, millibars equivalent surface pressure
+    units = gravtk.units.bycode(UNITS)
+    dfactor = factors.get(units)
+    # output spatial units and descriptive units longname
+    units_name, units_longname = gravtk.units.get_attributes(units)
+    # add attributes for earth parameters
+    attributes['earth_radius'] = f'{factors.rad_e:0.3f} cm'
+    attributes['earth_density'] = f'{factors.rho_e:0.3f} g/cm'
+    attributes['earth_gravity_constant'] = f'{factors.GM:0.3f} cm^3/s^2'
+    # add attributes to output spatial object
+    attributes['reference'] = f'Output from {pathlib.Path(sys.argv[0]).name}'
+    grid.attributes['ROOT'] = attributes
+
+    # output file format
+    file_format = '{0}{1}_L{2:d}{3}{4}{5}_{6:03d}.{7}'
     # converting harmonics to truncated, smoothed coefficients in units
     # combining harmonics to calculate output spatial fields
-    for i,gm in enumerate(GRACE_Ylms.month):
+    for i,grace_month in enumerate(GRACE_Ylms.month):
         # GRACE/GRACE-FO harmonics for time t
         Ylms = GRACE_Ylms.index(i)
         # Remove GIA rate for time
         Ylms.subtract(GIA_Ylms.index(i))
         # Remove monthly files to be removed
         Ylms.subtract(remove_Ylms.index(i))
         # smooth harmonics and convert to output units
         Ylms.convolve(dfactor*wt)
         # convert spherical harmonics to output spatial grid
-        grid.data[:,:,i] = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
-            grid.lon, grid.lat, LMAX=LMAX, MMAX=MMAX, PLM=PLM).T
+        grid.data = gravtk.harmonic_summation(Ylms.clm, Ylms.slm,
+            grid.lon, grid.lat, LMIN=LMIN, LMAX=LMAX,
+            MMAX=MMAX, PLM=PLM).T
+        grid.mask = np.zeros_like(grid.data, dtype=bool)
         # copy time variables for month
-        grid.time[i] = np.copy(Ylms.time)
-        grid.month[i] = np.copy(Ylms.month)
-    # update spacing and dimensions
-    grid.update_spacing()
-    grid.update_extents()
-    grid.update_dimensions()
-
-    # scale output data with kfactor
-    grid = grid.scale(kfactor.data)
-    grid.replace_invalid(fill_value, mask=kfactor.mask)
-
-    # output monthly files to ascii, netCDF4 or HDF5
-    fargs = (FILE_PREFIX, '', unit_str, LMAX, order_str, gw_str,
-        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
-    FILE = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
-    # attributes for output files
-    attributes = {}
-    attributes['units'] = copy.copy(unit_str)
-    attributes['longname'] = copy.copy(unit_name)
-    attributes['title'] = 'GRACE/GRACE-FO Spatial Data'
-    attributes['reference'] = f'Output from {os.path.basename(sys.argv[0])}'
-    if (DATAFORM == 'ascii'):
-        # ascii (.txt)
-        grid.to_ascii(FILE, date=True, verbose=VERBOSE)
-    elif (DATAFORM == 'netCDF4'):
-        # netCDF4
-        grid.to_netCDF4(FILE, date=True, verbose=VERBOSE, **attributes)
-    elif (DATAFORM == 'HDF5'):
-        # HDF5
-        grid.to_HDF5(FILE, date=True, verbose=VERBOSE, **attributes)
-    # set the permissions mode of the output files
-    os.chmod(FILE, MODE)
-    # add file to list
-    output_files.append(FILE)
-
-    # calculate power of scaled GRACE/GRACE-FO data
-    scaled_power = grid.sum(power=2.0).power(0.5)
-    # calculate residual leakage errors
-    # scaled by ratio of GRACE and synthetic power
-    ratio = scaled_power.scale(np.power(kfactor.magnitude,-1))
-    # replace invalid values with 0
-    ratio = np.nan_to_num(ratio.data, nan=0.0, posinf=0.0, neginf=0.0)
-    error = grid.copy()
-    error.data = kfactor.error*ratio
-    error.mask = np.copy(kfactor.mask)
-    error.update_mask()
-
-    # output monthly error files to ascii, netCDF4 or HDF5
-    fargs = (FILE_PREFIX, 'ERROR_', unit_str, LMAX, order_str, gw_str,
-        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
-    FILE = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
-    # attributes for output files
-    attributes['title'] = 'GRACE/GRACE-FO Scaling Error'
-    if (DATAFORM == 'ascii'):
-        # ascii (.txt)
-        error.to_ascii(FILE, date=False, verbose=VERBOSE)
-    elif (DATAFORM == 'netCDF4'):
-        # netCDF4
-        error.to_netCDF4(FILE, date=False, verbose=VERBOSE, **attributes)
-    elif (DATAFORM == 'HDF5'):
-        # HDF5
-        error.to_HDF5(FILE, date=False, verbose=VERBOSE, **attributes)
-    # set the permissions mode of the output files
-    os.chmod(FILE, MODE)
-    # add file to list
-    output_files.append(FILE)
+        grid.time = np.copy(Ylms.time)
+        grid.month = np.copy(Ylms.month)
 
-    # Output spatial data object
-    delta = gravtk.spatial()
-    delta.lon = np.copy(kfactor.lon)
-    delta.lat = np.copy(kfactor.lat)
-    delta.time = np.copy(tsmth)
-    delta.month = np.copy(nsmth)
-    delta.data = np.zeros((nlat,nlon))
-    delta.mask = np.zeros((nlat,nlon),dtype=bool)
-    # calculate scaled spatial error
-    # Calculating cos(m*phi)^2 and sin(m*phi)^2
-    m = delta_Ylms.m[:,np.newaxis]
-    ccos = np.cos(np.dot(m,phi))**2
-    ssin = np.sin(np.dot(m,phi))**2
-
-    # truncate delta harmonics to spherical harmonic range
-    Ylms = delta_Ylms.truncate(LMAX,lmin=LMIN,mmax=MMAX)
-    # convolve delta harmonics with degree dependent factors
-    # smooth harmonics and convert to output units
-    Ylms = Ylms.convolve(dfactor*wt).power(2.0).scale(1.0/nsmth)
-    # Calculate fourier coefficients
-    d_cos = np.zeros((MMAX+1,nlat))# [m,th]
-    d_sin = np.zeros((MMAX+1,nlat))# [m,th]
-    # Calculating delta spatial values
-    for k in range(0,nlat):
-        # summation over all spherical harmonic degrees
-        d_cos[:,k] = np.sum(PLM2[:,:,k]*Ylms.clm, axis=0)
-        d_sin[:,k] = np.sum(PLM2[:,:,k]*Ylms.slm, axis=0)
-    # Multiplying by c/s(phi#m) to get spatial error map
-    delta.data[:] = np.sqrt(np.dot(ccos.T,d_cos) + np.dot(ssin.T,d_sin)).T
-    # update spacing and dimensions
-    delta.update_spacing()
-    delta.update_extents()
-    delta.update_dimensions()
-
-    # scale output harmonic errors with kfactor
-    delta = delta.scale(kfactor.data)
-    delta.replace_invalid(fill_value, mask=kfactor.mask)
-
-    # output monthly files to ascii, netCDF4 or HDF5
-    fargs = (FILE_PREFIX, 'DELTA_', unit_str, LMAX, order_str, gw_str,
-        ds_str, grid.month[0], grid.month[-1], suffix[DATAFORM])
-    FILE = os.path.join(OUTPUT_DIRECTORY,file_format.format(*fargs))
-    # attributes for output files
-    attributes['title'] = 'GRACE/GRACE-FO Spatial Error'
-    if (DATAFORM == 'ascii'):
-        # ascii (.txt)
-        delta.to_ascii(FILE, date=True, verbose=VERBOSE)
-    elif (DATAFORM == 'netCDF4'):
-        # netCDF4
-        delta.to_netCDF4(FILE, date=True, verbose=VERBOSE, **attributes)
-    elif (DATAFORM == 'HDF5'):
-        # HDF5
-        delta.to_HDF5(FILE, date=True, verbose=VERBOSE, **attributes)
-    # set the permissions mode of the output files
-    os.chmod(FILE, MODE)
-    # add file to list
-    output_files.append(FILE)
+        # output monthly files to ascii, netCDF4 or HDF5
+        fargs = (FILE_PREFIX,units,LMAX,order_str,gw_str,
+            ds_str,grace_month,suffix[DATAFORM])
+        OUTPUT_FILE = OUTPUT_DIRECTORY.joinpath(file_format.format(*fargs))
+        grid.to_file(OUTPUT_FILE, format=DATAFORM, date=True,
+            verbose=VERBOSE, units=units_name, longname=units_longname)
+        # set the permissions mode of the output files
+        OUTPUT_FILE.chmod(mode=MODE)
+        # add file to list
+        output_files.append(OUTPUT_FILE)
 
     # return the list of output files
     return output_files
 
-# PURPOSE: print a file log for the GRACE/GRACE-FO analysis
+# PURPOSE: print a file log for the GRACE analysis
 def output_log_file(input_arguments, output_files):
-    # format: scale_GRACE_maps_run_2002-04-01_PID-70335.log
+    # format: GRACE_processing_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
-    LOGFILE = 'scale_GRACE_maps_run_{0}_PID-{1:d}.log'.format(*args)
+    LOGFILE = 'GRACE_processing_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print output files
     logging.info('\n\nOUTPUT FILES:')
     for f in output_files:
         logging.info(f)
     # close the log file
     fid.close()
 
-# PURPOSE: print a error file log for the GRACE/GRACE-FO analysis
+# PURPOSE: print a error file log for the GRACE analysis
 def output_error_log_file(input_arguments):
-    # format: scale_GRACE_maps_failed_run_2002-04-01_PID-70335.log
+    # format: GRACE_processing_failed_run_2002-04-01_PID-70335.log
     args = (time.strftime('%Y-%m-%d',time.localtime()), os.getpid())
-    LOGFILE = 'scale_GRACE_maps_failed_run_{0}_PID-{1:d}.log'.format(*args)
+    LOGFILE = 'GRACE_processing_failed_run_{0}_PID-{1:d}.log'.format(*args)
     # create a unique log and open the log file
-    DIRECTORY = os.path.expanduser(input_arguments.output_directory)
-    fid = gravtk.utilities.create_unique_file(os.path.join(DIRECTORY,LOGFILE))
+    DIRECTORY = pathlib.Path(input_arguments.output_directory)
+    fid = gravtk.utilities.create_unique_file(DIRECTORY.joinpath(LOGFILE))
     logging.basicConfig(stream=fid, level=logging.INFO)
     # print argument values sorted alphabetically
     logging.info('ARGUMENTS:')
     for arg, value in sorted(vars(input_arguments).items()):
         logging.info(f'{arg}: {value}')
     # print traceback error
     logging.info('\n\nTRACEBACK ERROR:')
     traceback.print_exc(file=fid)
     # close the log file
     fid.close()
 
 # PURPOSE: create argument parser
 def arguments():
     parser = argparse.ArgumentParser(
-        description="""Calculates scaled spatial maps from
-            GRACE/GRACE-FO spherical harmonic coefficients
+        description="""Calculates monthly spatial maps from GRACE/GRACE-FO
+            spherical harmonic coefficients
             """,
         fromfile_prefix_chars="@"
     )
     parser.convert_arg_line_to_args = gravtk.utilities.convert_arg_line_to_args
     # command line parameters
     # working data directory
     parser.add_argument('--directory','-D',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Working data directory')
     parser.add_argument('--output-directory','-O',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        default=os.getcwd(),
+        type=pathlib.Path, default=pathlib.Path.cwd(),
         help='Output directory for spatial files')
     parser.add_argument('--file-prefix','-P',
         type=str,
         help='Prefix string for input and output files')
     # Data processing center or satellite mission
     parser.add_argument('--center','-c',
         metavar='PROC', type=str, required=True,
@@ -739,21 +597,29 @@
     parser.add_argument('--radius','-R',
         type=float, default=0,
         help='Gaussian smoothing radius (km)')
     # Use a decorrelation (destriping) filter
     parser.add_argument('--destripe','-d',
         default=False, action='store_true',
         help='Use decorrelation (destriping) filter')
+    # output units
+    parser.add_argument('--units','-U',
+        type=int, default=1, choices=[1,2,3,4,5],
+        help='Output units')
     # output grid parameters
     parser.add_argument('--spacing',
         type=float, nargs='+', default=[0.5,0.5], metavar=('dlon','dlat'),
         help='Spatial resolution of output data')
     parser.add_argument('--interval',
-        type=int, default=2, choices=[1,2],
-        help=('Output grid interval (1: global, 2: centered global)'))
+        type=int, default=2, choices=[1,2,3],
+        help=('Output grid interval '
+            '(1: global, 2: centered global, 3: non-global)'))
+    parser.add_argument('--bounds',
+        type=float, nargs=4, metavar=('lon_min','lon_max','lat_min','lat_max'),
+        help='Bounding box for non-global grid')
     # GIA model type list
     models = {}
     models['IJ05-R2'] = 'Ivins R2 GIA Models'
     models['W12a'] = 'Whitehouse GIA Models'
     models['SM09'] = 'Simpson/Milne GIA Models'
     models['ICE6G'] = 'ICE-6G GIA Models'
     models['Wu10'] = 'Wu (2010) GIA Correction'
@@ -766,15 +632,15 @@
     models['HDF5'] = 'reformatted GIA in HDF5 format'
     # GIA model type
     parser.add_argument('--gia','-G',
         type=str, metavar='GIA', choices=models.keys(),
         help='GIA model type to read')
     # full path to GIA file
     parser.add_argument('--gia-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GIA file to read')
     # use atmospheric jump corrections from Fagiolini et al. (2015)
     parser.add_argument('--atm-correction',
         default=False, action='store_true',
         help='Apply atmospheric jump correction coefficients')
     # correct for pole tide drift follow Wahr et al. (2015)
     parser.add_argument('--pole-tide',
@@ -792,15 +658,15 @@
     # GFZ: GRACE/GRACE-FO coefficients from GFZ GravIS
     #     http://gravis.gfz-potsdam.de/corrections
     parser.add_argument('--geocenter',
         metavar='DEG1', type=str,
         choices=['Tellus','SLR','SLF','UCI','Swenson','GFZ'],
         help='Update Degree 1 coefficients with SLR or derived values')
     parser.add_argument('--geocenter-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='Specific geocenter file if not default')
     parser.add_argument('--interpolate-geocenter',
         default=False, action='store_true',
         help='Least-squares model missing Degree 1 coefficients')
     # replace low degree harmonics with values from Satellite Laser Ranging
     parser.add_argument('--slr-c20',
         type=str, default=None, choices=['CSR','GFZ','GSFC'],
@@ -822,52 +688,48 @@
         help='Replace C50 coefficients with SLR values')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--format','-F',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5'],
         help='Input/output data format')
     # mean file to remove
     parser.add_argument('--mean-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
+        type=pathlib.Path,
         help='GRACE/GRACE-FO mean file to remove from the harmonic data')
     # input data format (ascii, netCDF4, HDF5)
     parser.add_argument('--mean-format',
         type=str, default='netCDF4', choices=['ascii','netCDF4','HDF5','gfc'],
         help='Input data format for GRACE/GRACE-FO mean file')
     # monthly files to be removed from the GRACE/GRACE-FO data
     parser.add_argument('--remove-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), nargs='+',
+        type=pathlib.Path, nargs='+',
         help='Monthly files to be removed from the GRACE/GRACE-FO data')
     choices = []
     choices.extend(['ascii','netCDF4','HDF5'])
     choices.extend(['index-ascii','index-netCDF4','index-HDF5'])
     parser.add_argument('--remove-format',
         type=str, nargs='+', choices=choices,
         help='Input data format for files to be removed')
     parser.add_argument('--redistribute-removed',
         default=False, action='store_true',
         help='Redistribute removed mass fields over the ocean')
-    # scaling factor file
-    parser.add_argument('--scale-file',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)),
-        required=True, help='Scaling factor file')
     # land-sea mask for redistributing fluxes
     lsmask = gravtk.utilities.get_data_path(['data','landsea_hd.nc'])
     parser.add_argument('--mask',
-        type=lambda p: os.path.abspath(os.path.expanduser(p)), default=lsmask,
+        type=pathlib.Path, default=lsmask,
         help='Land-sea mask for redistributing land water flux')
     # Output log file for each job in forms
-    # scale_GRACE_maps_run_2002-04-01_PID-00000.log
-    # scale_GRACE_maps_failed_run_2002-04-01_PID-00000.log
+    # GRACE_processing_run_2002-04-01_PID-00000.log
+    # GRACE_processing_failed_run_2002-04-01_PID-00000.log
     parser.add_argument('--log',
         default=False, action='store_true',
         help='Output log file for each job')
-    # print information about processing run
+    # print information about each input and output file
     parser.add_argument('--verbose','-V',
         action='count', default=0,
-        help='Verbose output of processing run')
+        help='Verbose output of run')
     # permissions mode of the local directories and files (number in octal)
     parser.add_argument('--mode','-M',
         type=lambda x: int(x,base=8), default=0o775,
         help='Permissions mode of output files')
     # return the parser
     return parser
 
@@ -880,32 +742,34 @@
     # create logger
     loglevels = [logging.CRITICAL, logging.INFO, logging.DEBUG]
     logging.basicConfig(level=loglevels[args.verbose])
 
     # try to run the analysis with listed parameters
     try:
         info(args)
-        # run scale_grace_maps algorithm with parameters
-        output_files = scale_grace_maps(
+        # run grace_spatial_maps algorithm with parameters
+        output_files = grace_spatial_maps(
             args.directory,
             args.center,
             args.release,
             args.product,
             args.lmax,
             args.radius,
             START=args.start,
             END=args.end,
             MISSING=args.missing,
             LMIN=args.lmin,
             MMAX=args.mmax,
             LOVE_NUMBERS=args.love,
             REFERENCE=args.reference,
             DESTRIPE=args.destripe,
+            UNITS=args.units,
             DDEG=args.spacing,
             INTERVAL=args.interval,
+            BOUNDS=args.bounds,
             GIA=args.gia,
             GIA_FILE=args.gia_file,
             ATM=args.atm_correction,
             POLE_TIDE=args.pole_tide,
             DEG1=args.geocenter,
             DEG1_FILE=args.geocenter_file,
             MODEL_DEG1=args.interpolate_geocenter,
@@ -917,15 +781,14 @@
             SLR_C50=args.slr_c50,
             DATAFORM=args.format,
             MEAN_FILE=args.mean_file,
             MEANFORM=args.mean_format,
             REMOVE_FILES=args.remove_file,
             REMOVE_FORMAT=args.remove_format,
             REDISTRIBUTE_REMOVED=args.redistribute_removed,
-            SCALE_FILE=args.scale_file,
             LANDMASK=args.mask,
             OUTPUT_DIRECTORY=args.output_directory,
             FILE_PREFIX=args.file_prefix,
             VERBOSE=args.verbose,
             MODE=args.mode)
     except Exception as exc:
         # if there has been an error exception
```

### Comparing `gravity-toolkit-1.2.0/setup.py` & `gravity-toolkit-1.2.1/setup.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/test/conftest.py` & `gravity-toolkit-1.2.1/test/conftest.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/test/test_download_and_read.py` & `gravity-toolkit-1.2.1/test/test_download_and_read.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,34 +1,56 @@
 #!/usr/bin/env python
 u"""
 test_download_and_read.py (11/2021)
 Tests the read program to verify that coefficients are being extracted
 """
-import os
 import pytest
 import shutil
 import inspect
+import pathlib
 import posixpath
 import gravity_toolkit as gravtk
 from read_GRACE_geocenter.read_GRACE_geocenter import read_GRACE_geocenter
 
 # PURPOSE: Download a GRACE file from PO.DAAC and check that read program runs
+@pytest.mark.skip(reason='PO.DAAC Drive retired on April 24, 2023')
 def test_podaac_download_and_read(username,webdav):
     HOST=['https://podaac-tools.jpl.nasa.gov','drive','files','allData','grace',
         'L2','CSR','RL06','GSM-2_2002095-2002120_GRAC_UTCSR_BA01_0600.gz']
     # download and read as virtual file object
     FILE = gravtk.utilities.from_drive(HOST,username=username,
         password=webdav,verbose=True)
     Ylms = gravtk.read_GRACE_harmonics(FILE, 60)
     keys = ['time', 'start', 'end', 'clm', 'slm', 'eclm', 'eslm', 'header']
     test = dict(start=2452369.5, end=2452394.5)
     assert all((key in Ylms.keys()) for key in keys)
     assert all((Ylms[key] == val) for key,val in test.items())
     assert (Ylms['clm'][2,0] == -0.484169355584e-03)
 
+# PURPOSE: Download a GRACE file from PO.DAAC Cumulus and check that read program runs
+def test_podaac_cumulus_download_and_read(username,password):
+    # find the path to the data files
+    ids, urls, mtimes = gravtk.utilities.cmr(
+        mission='grace', center='CSR', release='RL06', level='L2',
+        product='GSM', start_date='2002-04-01', end_date='2002-04-30',
+        provider='POCLOUD', endpoint='data')
+    # build opener for data client access
+    URS = 'urs.earthdata.nasa.gov'
+    opener = gravtk.utilities.attempt_login(URS,
+        username=username, password=password,
+        authorization_header=False, verbose=True)
+    # download and read as virtual file object
+    FILE = gravtk.utilities.from_http(urls[0], context=None, verbose=True)
+    Ylms = gravtk.read_GRACE_harmonics(FILE, 60)
+    keys = ['time', 'start', 'end', 'clm', 'slm', 'eclm', 'eslm', 'header']
+    test = dict(start=2452369.5, end=2452394.5)
+    assert all((key in Ylms.keys()) for key in keys)
+    assert all((Ylms[key] == val) for key,val in test.items())
+    assert (Ylms['clm'][2,0] == -0.484169355584e-03)
+
 # PURPOSE: Download a GRACE file from GFZ and check that read program runs
 def test_gfz_ftp_download_and_read():
     HOST=['isdcftp.gfz-potsdam.de','grace','Level-2','CSR','RL06',
         'GSM-2_2002095-2002120_GRAC_UTCSR_BA01_0600.gz']
     # download and read as virtual file object
     FILE = gravtk.utilities.from_ftp(HOST,verbose=True)
     Ylms = gravtk.read_GRACE_harmonics(FILE, 60)
@@ -69,64 +91,66 @@
     swarm_file='SW_OPER_EGF_SHA_2__20131201T000000_20131231T235959_0101.ZIP'
     parameters = gravtk.utilities.urlencode({'file':
         posixpath.join('swarm','Level2longterm','EGF',swarm_file)})
     remote_file = [HOST,'?do=download&{0}'.format(parameters)]
     # download and read as virtual file object
     gravtk.utilities.from_http(remote_file,
         local=swarm_file,verbose=True)
+    swarm_file = pathlib.Path(swarm_file).absolute()
     Ylms = gravtk.read_gfc_harmonics(swarm_file)
     keys = ['time', 'start', 'end', 'clm', 'slm', 'eclm', 'eslm']
     test = dict(start=2456627.5, end=2456658.499988426)
     assert all((key in Ylms.keys()) for key in keys)
     assert all((Ylms[key] == val) for key,val in test.items())
     assert (Ylms['clm'][2,0] == -0.48416530506600003e-03)
     # clean up
-    os.remove(swarm_file)
+    swarm_file.unlink()
 
 # PURPOSE: Download a GRACE ITSG GRAZ file and check that read program runs
 def test_itsg_graz_download_and_read():
     HOST=['http://ftp.tugraz.at','outgoing','ITSG','GRACE',
         'ITSG-Grace_operational','monthly','monthly_n60',
         'ITSG-Grace_operational_n60_2018-06.gfc']
     # download and read as virtual file object
-    gravtk.utilities.from_http(HOST,local=HOST[-1],verbose=True)
-    Ylms = gravtk.read_gfc_harmonics(HOST[-1])
+    gravtk.utilities.from_http(HOST, local=HOST[-1], verbose=True)
+    itsg_file = pathlib.Path(HOST[-1]).absolute()
+    Ylms = gravtk.read_gfc_harmonics(itsg_file)
     keys = ['time', 'start', 'end', 'clm', 'slm', 'eclm', 'eslm']
     test = dict(start=2458270.5, end=2458300.499988426)
     assert all((key in Ylms.keys()) for key in keys)
     assert all((Ylms[key] == val) for key,val in test.items())
     assert (Ylms['clm'][2,0] == -0.4841694727612e-03)
     # clean up
-    os.remove(HOST[-1])
+    itsg_file.unlink()
 
 # PURPOSE: Download Sutterley and Velicogna (2019) geocenter files
 @pytest.fixture(scope="module", autouse=True)
 def download_geocenter():
     # download geocenter files to filepath
     filename = inspect.getframeinfo(inspect.currentframe()).filename
-    filepath = os.path.dirname(os.path.abspath(filename))
+    filepath = pathlib.Path(filename).absolute().parent
     gravtk.utilities.from_figshare(filepath,verbose=True)
     # run tests
     yield
     # clean up
-    shutil.rmtree(os.path.join(filepath,'geocenter'))
+    shutil.rmtree(filepath.joinpath('geocenter'))
 
 # parameterize processing center and data release
 @pytest.mark.parametrize("PROC", ['CSR','GFZ','JPL'])
 @pytest.mark.parametrize("DREL", ['RL06'])
 # PURPOSE: read Sutterley and Velicogna (2019) geocenter files
 def test_geocenter_read(PROC, DREL):
     filename = inspect.getframeinfo(inspect.currentframe()).filename
-    filepath = os.path.dirname(os.path.abspath(filename))
+    filepath = pathlib.Path(filename).absolute().parent
     MODEL = dict(RL04='OMCT', RL05='OMCT', RL06='MPIOM')
     args = (PROC,DREL,MODEL[DREL],'SLF_iter')
     FILE = '{0}_{1}_{2}_{3}.txt'.format(*args)
     # assert that file exists
-    geocenter_file = os.path.join(filepath,'geocenter',FILE)
-    assert os.access(geocenter_file, os.F_OK)
+    geocenter_file = filepath.joinpath('geocenter', FILE)
+    assert geocenter_file.exists()
     # test geocenter read program
     DEG1 = read_GRACE_geocenter(geocenter_file)
     keys = ['time', 'JD', 'month', 'C10', 'C11', 'S11','header']
     assert all((key in DEG1.keys()) for key in keys)
     # test geocenter class
     DATA = gravtk.geocenter().from_UCI(geocenter_file)
     for key in ['time', 'month', 'C10', 'C11', 'S11']:
```

### Comparing `gravity-toolkit-1.2.0/test/test_gia.py` & `gravity-toolkit-1.2.1/test/test_gia.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,36 +1,36 @@
 #!/usr/bin/env python
 u"""
 test_gia.py (12/2022)
 Tests the that GIA model readers are equivalent
 """
-import os
 import gzip
 import time
 import pytest
 import shutil
+import pathlib
 import numpy as np
 import gravity_toolkit as gravtk
 
 # PURPOSE: Download ICE-6G GIA model
 @pytest.fixture(scope="module", autouse=True)
 def download_GIA_model():
     # output GIA file
-    GIA_FILE = 'Stokes_trend_High_Res.txt'
+    GIA_FILE = pathlib.Path('Stokes_trend_High_Res.txt')
     # download GIA model
     HOST = ['https://www.atmosp.physics.utoronto.ca','~peltier','datasets',
         'Ice6G_C_VM5a','ICE-6G_High_Res_Stokes_trend.txt.gz']
     fid = gravtk.utilities.from_http(HOST, verbose=True)
     # decompress GIA model from virtual BytesIO object
     with gzip.open(fid, 'rb') as f_in, open(GIA_FILE, 'wb') as f_out:
         shutil.copyfileobj(f_in, f_out)
     # run tests
     yield
     # clean up
-    os.remove(GIA_FILE)
+    GIA_FILE.unlink()
 
 # PURPOSE: read ICE-6G GIA test outputs
 def test_GIA_model_read():
     # output GIA file and type
     GIA_FILE = 'Stokes_trend_High_Res.txt'
     GIA = 'ICE6G-D'
     # read GIA model
```

### Comparing `gravity-toolkit-1.2.0/test/test_harmonics.py` & `gravity-toolkit-1.2.1/test/test_harmonics.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,37 +4,37 @@
 Tests harmonic programs using the Velicogna and Wahr (2013) Greenland synthetic
     1. Converts synthetic spatial distribution to spherical harmonics
     2. Compares output spherical harmonics with validation dataset
     3. Combines harmonics to calculate a truncated and smoothed spatial dataset
     4. Compares output smoothed spatial distribution with validation dataset
 Tests harmonic objects flatten, expansion and iteration routines
 """
-import os
 import pytest
 import inspect
+import pathlib
 import numpy as np
 import gravity_toolkit as gravtk
 
 # path to test files
 filename = inspect.getframeinfo(inspect.currentframe()).filename
-filepath = os.path.dirname(os.path.abspath(filename))
+filepath = pathlib.Path(filename).absolute().parent
 
 # PURPOSE: test harmonic conversion programs
 def test_harmonics():
     # path to load Love numbers file
     love_numbers_file = gravtk.utilities.get_data_path(
         ['data','love_numbers'])
     # read load Love numbers
     LOVE = gravtk.read_love_numbers(love_numbers_file, FORMAT='class')
 
     # read input spatial distribution file
-    distribution_file = 'out.green_ice.grid.0.5.2008.cmh20.gz'
-    input_distribution = gravtk.spatial(spacing=[0.5,0.5], nlat=361,
-        nlon=721, extent=[0,360.0,-90,90]).from_ascii(
-        os.path.join(filepath,distribution_file),date=False,compression='gzip')
+    distribution_file = filepath.joinpath('out.green_ice.grid.0.5.2008.cmh20.gz')
+    input_distribution = gravtk.spatial().from_ascii(distribution_file,
+        date=False, spacing=[0.5,0.5], nlat=361, nlon=721,
+        extent=[0,360.0,-90,90], compression='gzip')
 
     # spherical harmonic parameters
     # maximum spherical harmonic degree
     LMAX = 60
     # gaussian smoothing radius (km)
     RAD = 250.0
 
@@ -48,17 +48,17 @@
         method='mohlenkamp')
     # convert to spherical harmonics
     test_Ylms = gravtk.gen_stokes(input_distribution.data,
         input_distribution.lon, input_distribution.lat, UNITS=1, LMAX=LMAX,
         PLM=PLM, LOVE=LOVE)
 
     # read harmonics from file
-    harmonics_file = 'out.geoid.green_ice.0.5.2008.60.gz'
+    harmonics_file = filepath.joinpath('out.geoid.green_ice.0.5.2008.60.gz')
     valid_Ylms = gravtk.harmonics(lmax=LMAX, mmax=LMAX).from_ascii(
-        os.path.join(filepath,harmonics_file),date=False,compression='gzip')
+        harmonics_file, date=False, compression='gzip')
 
     # check that harmonic data is equal to machine precision
     difference_Ylms = test_Ylms.copy()
     difference_Ylms.subtract(valid_Ylms)
     harmonic_eps = np.finfo(np.float32).eps
     assert np.all(np.abs(difference_Ylms.clm) < harmonic_eps)
     assert np.all(np.abs(difference_Ylms.slm) < harmonic_eps)
@@ -78,18 +78,18 @@
         LMAX=LMAX, PLM=PLM).T
     # convert harmonics back to spatial domain using wrapper function
     test_combine = gravtk.stokes_summation(test_Ylms.clm,
         test_Ylms.slm, input_distribution.lon, input_distribution.lat,
         LMAX=LMAX, PLM=PLM, UNITS=1, RAD=RAD, LOVE=LOVE).T
 
     # read input and output spatial distribution files
-    distribution_file = 'out.combine.green_ice.0.5.2008.60.gz'
-    output_distribution = gravtk.spatial(spacing=[0.5,0.5], nlat=361,
-        nlon=721, extent=[0,360.0,-90,90]).from_ascii(
-        os.path.join(filepath,distribution_file),date=False,compression='gzip')
+    spatial_file = filepath.joinpath('out.combine.green_ice.0.5.2008.60.gz')
+    output_distribution = gravtk.spatial().from_ascii(spatial_file,
+        date=False, spacing=[0.5,0.5], nlat=361, nlon=721,
+        extent=[0,360.0,-90,90], compression='gzip')
 
     # check that data is equal to machine precision
     difference_distribution = test_distribution - output_distribution.data
     difference_transform = test_transform - output_distribution.data
     distribution_eps = np.finfo(np.float16).eps
     assert np.all(np.abs(difference_distribution) < distribution_eps)
     assert np.all(np.abs(difference_transform) < distribution_eps)
```

### Comparing `gravity-toolkit-1.2.0/test/test_legendre.py` & `gravity-toolkit-1.2.1/test/test_legendre.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/test/test_love_numbers.py` & `gravity-toolkit-1.2.1/test/test_love_numbers.py`

 * *Files identical despite different names*

### Comparing `gravity-toolkit-1.2.0/test/test_point_masses.py` & `gravity-toolkit-1.2.1/test/test_point_masses.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,23 +13,23 @@
 @pytest.mark.parametrize("NPTS", np.random.randint(2,2000,size=1))
 def test_point_masses(NPTS):
     # create spatial grid
     dlon,dlat = (1.0,1.0)
     lat = np.arange(90.0 - dlat/2.0, -90.0 - dlat/2.0, -dlat)
     lon = np.arange(-180.0 + dlon/2.0, 180.0 + dlon/2.0, dlon)
     gridlon,gridlat = np.meshgrid(lon,lat)
-    nlat,nlon = np.shape(gridlon)
+    nlat, nlon = np.shape(gridlon)
 
     # parameterize point masses
     LAT = lat[0]-dlat*np.random.randint(0,nlat,size=NPTS)
     LON  = lon[0]+dlon*np.random.randint(0,nlon,size=NPTS)
     MASS = 100.0 - 200.0*np.random.randn(NPTS)
 
     # create test gridded field
-    data = np.zeros((nlat,nlon))
+    data = np.zeros((nlat, nlon))
     for i in range(NPTS):
         indy,indx = np.nonzero((gridlat == LAT[i]) & (gridlon == LON[i]))
         data[indy,indx] += MASS[i]
 
     # path to load Love numbers file
     love_numbers_file = gravtk.utilities.get_data_path(
         ['data','love_numbers'])
```

