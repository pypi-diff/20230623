# Comparing `tmp/snowflake_ml_python-1.0.1-py3-none-any.whl.zip` & `tmp/snowflake_ml_python-1.0.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,248 +1,248 @@
-Zip file size: 1824135 bytes, number of entries: 246
--rw-r--r--  2.0 unx      161 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/env.py
--rw-r--r--  2.0 unx    14126 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/env_utils.py
--rw-r--r--  2.0 unx     7535 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/file_utils.py
--rw-r--r--  2.0 unx     2696 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/init_utils.py
--rw-r--r--  2.0 unx    20238 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/telemetry.py
--rw-r--r--  2.0 unx     2168 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/type_utils.py
--rw-r--r--  2.0 unx     3678 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/formatting.py
--rw-r--r--  2.0 unx     5358 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/identifier.py
--rw-r--r--  2.0 unx     2068 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/import_utils.py
--rw-r--r--  2.0 unx     4550 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/parallelize.py
--rw-r--r--  2.0 unx     3722 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/pkg_version_utils.py
--rw-r--r--  2.0 unx    12205 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/query_result_checker.py
--rw-r--r--  2.0 unx     1400 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/temp_file_utils.py
--rw-r--r--  2.0 unx     1841 b- defN 23-Jun-16 19:38 snowflake/ml/_internal/utils/uri.py
--rw-r--r--  2.0 unx    26746 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/fileset.py
--rw-r--r--  2.0 unx     1040 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/fileset_errors.py
--rw-r--r--  2.0 unx     5915 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/parquet_parser.py
--rw-r--r--  2.0 unx    11536 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/sfcfs.py
--rw-r--r--  2.0 unx    14859 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/stage_fs.py
--rw-r--r--  2.0 unx     3462 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/tf_dataset.py
--rw-r--r--  2.0 unx     2386 b- defN 23-Jun-16 19:38 snowflake/ml/fileset/torch_datapipe.py
--r-xr-xr-x  2.0 unx      223 b- defN 23-Jun-16 19:41 snowflake/ml/model/_core_requirements.py
--rw-r--r--  2.0 unx     8959 b- defN 23-Jun-16 19:38 snowflake/ml/model/_deploy_client/warehouse/deploy.py
--rw-r--r--  2.0 unx     2292 b- defN 23-Jun-16 19:38 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
--rw-r--r--  2.0 unx     9548 b- defN 23-Jun-16 19:38 snowflake/ml/model/_deployer.py
--rw-r--r--  2.0 unx     4488 b- defN 23-Jun-16 19:38 snowflake/ml/model/_env.py
--rw-r--r--  2.0 unx     2189 b- defN 23-Jun-16 19:38 snowflake/ml/model/_handlers/_base.py
--rw-r--r--  2.0 unx     6084 b- defN 23-Jun-16 19:38 snowflake/ml/model/_handlers/custom.py
--rw-r--r--  2.0 unx     7484 b- defN 23-Jun-16 19:38 snowflake/ml/model/_handlers/sklearn.py
--rw-r--r--  2.0 unx     7711 b- defN 23-Jun-16 19:38 snowflake/ml/model/_handlers/snowmlmodel.py
--rw-r--r--  2.0 unx     7170 b- defN 23-Jun-16 19:38 snowflake/ml/model/_handlers/xgboost.py
--rw-r--r--  2.0 unx    26376 b- defN 23-Jun-16 19:38 snowflake/ml/model/_model.py
--rw-r--r--  2.0 unx     2101 b- defN 23-Jun-16 19:38 snowflake/ml/model/_model_handler.py
--rw-r--r--  2.0 unx    16992 b- defN 23-Jun-16 19:38 snowflake/ml/model/_model_meta.py
--rw-r--r--  2.0 unx     8016 b- defN 23-Jun-16 19:38 snowflake/ml/model/custom_model.py
--rw-r--r--  2.0 unx    43624 b- defN 23-Jun-16 19:38 snowflake/ml/model/model_signature.py
--rw-r--r--  2.0 unx     4698 b- defN 23-Jun-16 19:38 snowflake/ml/model/type_hints.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/calibration/__init__.py
--r-xr-xr-x  2.0 unx    53589 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/__init__.py
--r-xr-xr-x  2.0 unx    51516 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/affinity_propagation.py
--r-xr-xr-x  2.0 unx    53529 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/agglomerative_clustering.py
--r-xr-xr-x  2.0 unx    51354 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/birch.py
--r-xr-xr-x  2.0 unx    53736 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/bisecting_k_means.py
--r-xr-xr-x  2.0 unx    51695 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/dbscan.py
--r-xr-xr-x  2.0 unx    54069 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/feature_agglomeration.py
--r-xr-xr-x  2.0 unx    53323 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/k_means.py
--r-xr-xr-x  2.0 unx    51897 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/mean_shift.py
--r-xr-xr-x  2.0 unx    54598 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/mini_batch_k_means.py
--r-xr-xr-x  2.0 unx    55029 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/optics.py
--r-xr-xr-x  2.0 unx    52087 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/spectral_biclustering.py
--r-xr-xr-x  2.0 unx    55025 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/spectral_clustering.py
--r-xr-xr-x  2.0 unx    51217 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/cluster/spectral_coclustering.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/compose/__init__.py
--r-xr-xr-x  2.0 unx    53800 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/compose/column_transformer.py
--r-xr-xr-x  2.0 unx    51385 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/compose/transformed_target_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/__init__.py
--r-xr-xr-x  2.0 unx    51357 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/elliptic_envelope.py
--r-xr-xr-x  2.0 unx    49633 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/empirical_covariance.py
--r-xr-xr-x  2.0 unx    50907 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/graphical_lasso.py
--r-xr-xr-x  2.0 unx    52371 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
--r-xr-xr-x  2.0 unx    49835 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/ledoit_wolf.py
--r-xr-xr-x  2.0 unx    50598 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/min_cov_det.py
--r-xr-xr-x  2.0 unx    49524 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/oas.py
--r-xr-xr-x  2.0 unx    49810 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/covariance/shrunk_covariance.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/decomposition/__init__.py
--r-xr-xr-x  2.0 unx    54625 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/decomposition/dictionary_learning.py
--r-xr-xr-x  2.0 unx    51997 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/factor_analysis.py
--r-xr-xr-x  2.0 unx    52459 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/fast_ica.py
--r-xr-xr-x  2.0 unx    50794 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/incremental_pca.py
--r-xr-xr-x  2.0 unx    54825 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/kernel_pca.py
--r-xr-xr-x  2.0 unx    55808 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
--r-xr-xr-x  2.0 unx    53125 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
--r-xr-xr-x  2.0 unx    53669 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/pca.py
--r-xr-xr-x  2.0 unx    51990 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/sparse_pca.py
--r-xr-xr-x  2.0 unx    51562 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/decomposition/truncated_svd.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/discriminant_analysis/__init__.py
--r-xr-xr-x  2.0 unx    53810 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
--r-xr-xr-x  2.0 unx    51875 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/__init__.py
--r-xr-xr-x  2.0 unx    52828 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
--r-xr-xr-x  2.0 unx    51727 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
--r-xr-xr-x  2.0 unx    53752 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/bagging_classifier.py
--r-xr-xr-x  2.0 unx    52996 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/bagging_regressor.py
--r-xr-xr-x  2.0 unx    58538 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
--r-xr-xr-x  2.0 unx    57149 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
--r-xr-xr-x  2.0 unx    60147 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    59731 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    59797 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    58119 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    52773 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/isolation_forest.py
--r-xr-xr-x  2.0 unx    58493 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/random_forest_classifier.py
--r-xr-xr-x  2.0 unx    57092 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/random_forest_regressor.py
--r-xr-xr-x  2.0 unx    52680 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/stacking_regressor.py
--r-xr-xr-x  2.0 unx    52255 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/voting_classifier.py
--r-xr-xr-x  2.0 unx    50790 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/ensemble/voting_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/__init__.py
--r-xr-xr-x  2.0 unx    50303 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
--r-xr-xr-x  2.0 unx    50001 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/select_fdr.py
--r-xr-xr-x  2.0 unx    49995 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/select_fpr.py
--r-xr-xr-x  2.0 unx    50003 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/select_fwe.py
--r-xr-xr-x  2.0 unx    50080 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/select_k_best.py
--r-xr-xr-x  2.0 unx    50100 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/select_percentile.py
--r-xr-xr-x  2.0 unx    52753 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
--r-xr-xr-x  2.0 unx    49732 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/feature_selection/variance_threshold.py
--rw-r--r--  2.0 unx     9110 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/framework/_utils.py
--rw-r--r--  2.0 unx    21900 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/framework/base.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/gaussian_process/__init__.py
--r-xr-xr-x  2.0 unx    55298 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
--r-xr-xr-x  2.0 unx    53990 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/impute/__init__.py
--r-xr-xr-x  2.0 unx    55853 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/impute/iterative_imputer.py
--r-xr-xr-x  2.0 unx    52075 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/impute/knn_imputer.py
--r-xr-xr-x  2.0 unx    50872 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/impute/missing_indicator.py
--rw-r--r--  2.0 unx    18118 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/impute/simple_imputer.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/__init__.py
--r-xr-xr-x  2.0 unx    49816 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
--r-xr-xr-x  2.0 unx    51689 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/nystroem.py
--r-xr-xr-x  2.0 unx    50843 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
--r-xr-xr-x  2.0 unx    50272 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
--r-xr-xr-x  2.0 unx    50271 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_ridge/__init__.py
--r-xr-xr-x  2.0 unx    51789 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/lightgbm/__init__.py
--r-xr-xr-x  2.0 unx    51311 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
--r-xr-xr-x  2.0 unx    50822 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/__init__.py
--r-xr-xr-x  2.0 unx    51537 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ard_regression.py
--r-xr-xr-x  2.0 unx    51850 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/bayesian_ridge.py
--r-xr-xr-x  2.0 unx    52734 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/elastic_net.py
--r-xr-xr-x  2.0 unx    53992 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/elastic_net_cv.py
--r-xr-xr-x  2.0 unx    51790 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/gamma_regressor.py
--r-xr-xr-x  2.0 unx    50978 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/huber_regressor.py
--r-xr-xr-x  2.0 unx    52275 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lars.py
--r-xr-xr-x  2.0 unx    52482 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lars_cv.py
--r-xr-xr-x  2.0 unx    52374 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lasso.py
--r-xr-xr-x  2.0 unx    53149 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lasso_cv.py
--r-xr-xr-x  2.0 unx    53378 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lasso_lars.py
--r-xr-xr-x  2.0 unx    53324 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
--r-xr-xr-x  2.0 unx    52669 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
--r-xr-xr-x  2.0 unx    50504 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/linear_regression.py
--r-xr-xr-x  2.0 unx    56755 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/logistic_regression.py
--r-xr-xr-x  2.0 unx    57775 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
--r-xr-xr-x  2.0 unx    51960 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
--r-xr-xr-x  2.0 unx    53588 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
--r-xr-xr-x  2.0 unx    51542 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/multi_task_lasso.py
--r-xr-xr-x  2.0 unx    52794 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
--r-xr-xr-x  2.0 unx    51069 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
--r-xr-xr-x  2.0 unx    54420 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
--r-xr-xr-x  2.0 unx    53495 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
--r-xr-xr-x  2.0 unx    53925 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/perceptron.py
--r-xr-xr-x  2.0 unx    51821 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/poisson_regressor.py
--r-xr-xr-x  2.0 unx    55295 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ransac_regressor.py
--r-xr-xr-x  2.0 unx    53355 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ridge.py
--r-xr-xr-x  2.0 unx    53673 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ridge_classifier.py
--r-xr-xr-x  2.0 unx    52212 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
--r-xr-xr-x  2.0 unx    52988 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/ridge_cv.py
--r-xr-xr-x  2.0 unx    59341 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/sgd_classifier.py
--r-xr-xr-x  2.0 unx    53955 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
--r-xr-xr-x  2.0 unx    56810 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/sgd_regressor.py
--r-xr-xr-x  2.0 unx    52243 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
--r-xr-xr-x  2.0 unx    53214 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/linear_model/tweedie_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/manifold/__init__.py
--r-xr-xr-x  2.0 unx    52609 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/manifold/isomap.py
--r-xr-xr-x  2.0 unx    51827 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/manifold/mds.py
--r-xr-xr-x  2.0 unx    52598 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/manifold/spectral_embedding.py
--r-xr-xr-x  2.0 unx    55870 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/manifold/tsne.py
--rw-r--r--  2.0 unx      304 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/__init__.py
--rw-r--r--  2.0 unx    39902 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/classification.py
--rw-r--r--  2.0 unx     4921 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/correlation.py
--rw-r--r--  2.0 unx     4757 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/covariance.py
--rw-r--r--  2.0 unx    12037 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/metrics_utils.py
--rw-r--r--  2.0 unx    15397 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/ranking.py
--rw-r--r--  2.0 unx     1880 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/metrics/regression.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/mixture/__init__.py
--r-xr-xr-x  2.0 unx    56514 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
--r-xr-xr-x  2.0 unx    54516 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/mixture/gaussian_mixture.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/model_selection/__init__.py
--r-xr-xr-x  2.0 unx    57065 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/model_selection/grid_search_cv.py
--r-xr-xr-x  2.0 unx    57909 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/model_selection/randomized_search_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/multiclass/__init__.py
--r-xr-xr-x  2.0 unx    50497 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
--r-xr-xr-x  2.0 unx    51425 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
--r-xr-xr-x  2.0 unx    50755 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/multiclass/output_code_classifier.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/__init__.py
--r-xr-xr-x  2.0 unx    51082 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
--r-xr-xr-x  2.0 unx    51403 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/categorical_nb.py
--r-xr-xr-x  2.0 unx    51090 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/complement_nb.py
--r-xr-xr-x  2.0 unx    50230 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
--r-xr-xr-x  2.0 unx    50847 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/__init__.py
--r-xr-xr-x  2.0 unx    53634 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    53116 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
--r-xr-xr-x  2.0 unx    51593 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/kernel_density.py
--r-xr-xr-x  2.0 unx    53874 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/local_outlier_factor.py
--r-xr-xr-x  2.0 unx    50397 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/nearest_centroid.py
--r-xr-xr-x  2.0 unx    52306 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/nearest_neighbors.py
--r-xr-xr-x  2.0 unx    53782 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
--r-xr-xr-x  2.0 unx    54263 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    53149 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/neural_network/__init__.py
--r-xr-xr-x  2.0 unx    50798 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
--r-xr-xr-x  2.0 unx    58296 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/neural_network/mlp_classifier.py
--r-xr-xr-x  2.0 unx    57573 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/neural_network/mlp_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/pipeline/__init__.py
--rw-r--r--  2.0 unx    22173 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/pipeline/pipeline.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/__init__.py
--rw-r--r--  2.0 unx     6092 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/binarizer.py
--rw-r--r--  2.0 unx    20422 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
--rw-r--r--  2.0 unx     6285 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/label_encoder.py
--rw-r--r--  2.0 unx     8491 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
--rw-r--r--  2.0 unx    10716 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/min_max_scaler.py
--rw-r--r--  2.0 unx     5951 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/normalizer.py
--rw-r--r--  2.0 unx    67083 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
--rw-r--r--  2.0 unx    27848 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
--r-xr-xr-x  2.0 unx    50932 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/preprocessing/polynomial_features.py
--rw-r--r--  2.0 unx    11981 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/robust_scaler.py
--rw-r--r--  2.0 unx    10672 b- defN 23-Jun-16 19:38 snowflake/ml/modeling/preprocessing/standard_scaler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/semi_supervised/__init__.py
--r-xr-xr-x  2.0 unx    51269 b- defN 23-Jun-16 19:42 snowflake/ml/modeling/semi_supervised/label_propagation.py
--r-xr-xr-x  2.0 unx    51633 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/semi_supervised/label_spreading.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/__init__.py
--r-xr-xr-x  2.0 unx    53811 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/linear_svc.py
--r-xr-xr-x  2.0 unx    52226 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/linear_svr.py
--r-xr-xr-x  2.0 unx    54525 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/nu_svc.py
--r-xr-xr-x  2.0 unx    51601 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/nu_svr.py
--r-xr-xr-x  2.0 unx    54688 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/svc.py
--r-xr-xr-x  2.0 unx    51804 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/svm/svr.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/tree/__init__.py
--r-xr-xr-x  2.0 unx    56887 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/tree/decision_tree_classifier.py
--r-xr-xr-x  2.0 unx    55583 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/tree/decision_tree_regressor.py
--r-xr-xr-x  2.0 unx    56250 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/tree/extra_tree_classifier.py
--r-xr-xr-x  2.0 unx    54955 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/tree/extra_tree_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/xgboost/__init__.py
--r-xr-xr-x  2.0 unx    60657 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/xgboost/xgb_classifier.py
--r-xr-xr-x  2.0 unx    60163 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/xgboost/xgb_regressor.py
--r-xr-xr-x  2.0 unx    60821 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
--r-xr-xr-x  2.0 unx    60354 b- defN 23-Jun-16 19:41 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
--rw-r--r--  2.0 unx     1381 b- defN 23-Jun-16 19:38 snowflake/ml/registry/_schema.py
--rw-r--r--  2.0 unx    85339 b- defN 23-Jun-16 19:38 snowflake/ml/registry/model_registry.py
--rw-r--r--  2.0 unx     6138 b- defN 23-Jun-16 19:38 snowflake/ml/utils/connection_params.py
--rw-r--r--  2.0 unx     3893 b- defN 23-Jun-16 19:38 snowflake/ml/utils/sparse.py
--r-xr-xr-x  2.0 unx       16 b- defN 23-Jun-16 19:41 snowflake/ml/version.py
-?rw-------  2.0 unx       91 b- defN 23-Jun-16 19:42 snowflake_ml_python-1.0.1.dist-info/WHEEL
-?rw-------  2.0 unx    10425 b- defN 23-Jun-16 19:42 snowflake_ml_python-1.0.1.dist-info/METADATA
-?rw-------  2.0 unx    26136 b- defN 23-Jun-16 19:42 snowflake_ml_python-1.0.1.dist-info/RECORD
-246 files, 8920727 bytes uncompressed, 1781365 bytes compressed:  80.0%
+Zip file size: 1855114 bytes, number of entries: 246
+-rw-r--r--  2.0 unx      161 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/env.py
+-rw-r--r--  2.0 unx    14126 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/env_utils.py
+-rw-r--r--  2.0 unx     6353 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/file_utils.py
+-rw-r--r--  2.0 unx     2696 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/init_utils.py
+-rw-r--r--  2.0 unx    20238 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/telemetry.py
+-rw-r--r--  2.0 unx     2168 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/type_utils.py
+-rw-r--r--  2.0 unx     3678 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/formatting.py
+-rw-r--r--  2.0 unx     7703 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/identifier.py
+-rw-r--r--  2.0 unx     2068 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/import_utils.py
+-rw-r--r--  2.0 unx     4550 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/parallelize.py
+-rw-r--r--  2.0 unx     3722 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/pkg_version_utils.py
+-rw-r--r--  2.0 unx    12205 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/query_result_checker.py
+-rw-r--r--  2.0 unx     1400 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/temp_file_utils.py
+-rw-r--r--  2.0 unx     1841 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/uri.py
+-rw-r--r--  2.0 unx    26746 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/fileset.py
+-rw-r--r--  2.0 unx     1040 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/fileset_errors.py
+-rw-r--r--  2.0 unx     5915 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/parquet_parser.py
+-rw-r--r--  2.0 unx    11536 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/sfcfs.py
+-rw-r--r--  2.0 unx    14859 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/stage_fs.py
+-rw-r--r--  2.0 unx     3462 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/tf_dataset.py
+-rw-r--r--  2.0 unx     2386 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/torch_datapipe.py
+-r-xr-xr-x  2.0 unx      197 b- defN 23-Jun-23 01:58 snowflake/ml/model/_core_requirements.py
+-rw-r--r--  2.0 unx     7962 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deploy_client/warehouse/deploy.py
+-rw-r--r--  2.0 unx     2369 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
+-rw-r--r--  2.0 unx     9548 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deployer.py
+-rw-r--r--  2.0 unx     4488 b- defN 23-Jun-23 01:55 snowflake/ml/model/_env.py
+-rw-r--r--  2.0 unx     2299 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/_base.py
+-rw-r--r--  2.0 unx     6084 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/custom.py
+-rw-r--r--  2.0 unx     7558 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/sklearn.py
+-rw-r--r--  2.0 unx     7711 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/snowmlmodel.py
+-rw-r--r--  2.0 unx     7204 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/xgboost.py
+-rw-r--r--  2.0 unx    26448 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model.py
+-rw-r--r--  2.0 unx     2101 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model_handler.py
+-rw-r--r--  2.0 unx    17684 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model_meta.py
+-rw-r--r--  2.0 unx     8016 b- defN 23-Jun-23 01:55 snowflake/ml/model/custom_model.py
+-rw-r--r--  2.0 unx    43624 b- defN 23-Jun-23 01:55 snowflake/ml/model/model_signature.py
+-rw-r--r--  2.0 unx     4405 b- defN 23-Jun-23 01:55 snowflake/ml/model/type_hints.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/calibration/__init__.py
+-r-xr-xr-x  2.0 unx    54140 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/__init__.py
+-r-xr-xr-x  2.0 unx    52067 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/affinity_propagation.py
+-r-xr-xr-x  2.0 unx    54080 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/agglomerative_clustering.py
+-r-xr-xr-x  2.0 unx    51905 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/birch.py
+-r-xr-xr-x  2.0 unx    54287 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/bisecting_k_means.py
+-r-xr-xr-x  2.0 unx    52246 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/dbscan.py
+-r-xr-xr-x  2.0 unx    54620 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/feature_agglomeration.py
+-r-xr-xr-x  2.0 unx    53874 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/k_means.py
+-r-xr-xr-x  2.0 unx    52448 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/mean_shift.py
+-r-xr-xr-x  2.0 unx    55149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/mini_batch_k_means.py
+-r-xr-xr-x  2.0 unx    55580 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/optics.py
+-r-xr-xr-x  2.0 unx    52638 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_biclustering.py
+-r-xr-xr-x  2.0 unx    55576 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_clustering.py
+-r-xr-xr-x  2.0 unx    51768 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_coclustering.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/__init__.py
+-r-xr-xr-x  2.0 unx    54351 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/column_transformer.py
+-r-xr-xr-x  2.0 unx    51936 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/transformed_target_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/__init__.py
+-r-xr-xr-x  2.0 unx    51908 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/elliptic_envelope.py
+-r-xr-xr-x  2.0 unx    50184 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/empirical_covariance.py
+-r-xr-xr-x  2.0 unx    51458 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/graphical_lasso.py
+-r-xr-xr-x  2.0 unx    52922 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
+-r-xr-xr-x  2.0 unx    50386 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/ledoit_wolf.py
+-r-xr-xr-x  2.0 unx    51149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/min_cov_det.py
+-r-xr-xr-x  2.0 unx    50075 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/oas.py
+-r-xr-xr-x  2.0 unx    50361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/shrunk_covariance.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/__init__.py
+-r-xr-xr-x  2.0 unx    55176 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/dictionary_learning.py
+-r-xr-xr-x  2.0 unx    52548 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/factor_analysis.py
+-r-xr-xr-x  2.0 unx    53010 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/fast_ica.py
+-r-xr-xr-x  2.0 unx    51345 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/incremental_pca.py
+-r-xr-xr-x  2.0 unx    55376 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/kernel_pca.py
+-r-xr-xr-x  2.0 unx    56359 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
+-r-xr-xr-x  2.0 unx    53676 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
+-r-xr-xr-x  2.0 unx    54220 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/pca.py
+-r-xr-xr-x  2.0 unx    52541 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/sparse_pca.py
+-r-xr-xr-x  2.0 unx    52113 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/truncated_svd.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/__init__.py
+-r-xr-xr-x  2.0 unx    54361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx    52426 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/__init__.py
+-r-xr-xr-x  2.0 unx    53379 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
+-r-xr-xr-x  2.0 unx    52278 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
+-r-xr-xr-x  2.0 unx    54303 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/bagging_classifier.py
+-r-xr-xr-x  2.0 unx    53547 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/bagging_regressor.py
+-r-xr-xr-x  2.0 unx    59089 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
+-r-xr-xr-x  2.0 unx    57700 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
+-r-xr-xr-x  2.0 unx    60698 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    60282 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    60348 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    58670 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    53324 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/isolation_forest.py
+-r-xr-xr-x  2.0 unx    59044 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/random_forest_classifier.py
+-r-xr-xr-x  2.0 unx    57643 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/random_forest_regressor.py
+-r-xr-xr-x  2.0 unx    53231 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/stacking_regressor.py
+-r-xr-xr-x  2.0 unx    52806 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/voting_classifier.py
+-r-xr-xr-x  2.0 unx    51341 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/ensemble/voting_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/__init__.py
+-r-xr-xr-x  2.0 unx    50854 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
+-r-xr-xr-x  2.0 unx    50552 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fdr.py
+-r-xr-xr-x  2.0 unx    50546 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fpr.py
+-r-xr-xr-x  2.0 unx    50554 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fwe.py
+-r-xr-xr-x  2.0 unx    50631 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_k_best.py
+-r-xr-xr-x  2.0 unx    50651 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_percentile.py
+-r-xr-xr-x  2.0 unx    53304 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
+-r-xr-xr-x  2.0 unx    50283 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/variance_threshold.py
+-rw-r--r--  2.0 unx     9110 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/framework/_utils.py
+-rw-r--r--  2.0 unx    21900 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/framework/base.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/__init__.py
+-r-xr-xr-x  2.0 unx    55849 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
+-r-xr-xr-x  2.0 unx    54541 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/impute/__init__.py
+-r-xr-xr-x  2.0 unx    56404 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/iterative_imputer.py
+-r-xr-xr-x  2.0 unx    52626 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/knn_imputer.py
+-r-xr-xr-x  2.0 unx    51423 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/missing_indicator.py
+-rw-r--r--  2.0 unx    18118 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/impute/simple_imputer.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/__init__.py
+-r-xr-xr-x  2.0 unx    50367 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
+-r-xr-xr-x  2.0 unx    52240 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/nystroem.py
+-r-xr-xr-x  2.0 unx    51394 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
+-r-xr-xr-x  2.0 unx    50823 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
+-r-xr-xr-x  2.0 unx    50822 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_ridge/__init__.py
+-r-xr-xr-x  2.0 unx    52340 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/__init__.py
+-r-xr-xr-x  2.0 unx    51862 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
+-r-xr-xr-x  2.0 unx    51373 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/__init__.py
+-r-xr-xr-x  2.0 unx    52088 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/ard_regression.py
+-r-xr-xr-x  2.0 unx    52401 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/bayesian_ridge.py
+-r-xr-xr-x  2.0 unx    53285 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/elastic_net.py
+-r-xr-xr-x  2.0 unx    54543 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    52341 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/gamma_regressor.py
+-r-xr-xr-x  2.0 unx    51529 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/huber_regressor.py
+-r-xr-xr-x  2.0 unx    52826 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lars.py
+-r-xr-xr-x  2.0 unx    53033 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lars_cv.py
+-r-xr-xr-x  2.0 unx    52925 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso.py
+-r-xr-xr-x  2.0 unx    53700 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_cv.py
+-r-xr-xr-x  2.0 unx    53929 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars.py
+-r-xr-xr-x  2.0 unx    53875 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
+-r-xr-xr-x  2.0 unx    53220 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
+-r-xr-xr-x  2.0 unx    51055 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/linear_regression.py
+-r-xr-xr-x  2.0 unx    57306 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/logistic_regression.py
+-r-xr-xr-x  2.0 unx    58326 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
+-r-xr-xr-x  2.0 unx    52511 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
+-r-xr-xr-x  2.0 unx    54139 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    52093 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_lasso.py
+-r-xr-xr-x  2.0 unx    53345 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
+-r-xr-xr-x  2.0 unx    51620 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
+-r-xr-xr-x  2.0 unx    54971 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
+-r-xr-xr-x  2.0 unx    54046 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
+-r-xr-xr-x  2.0 unx    54476 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/perceptron.py
+-r-xr-xr-x  2.0 unx    52372 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/poisson_regressor.py
+-r-xr-xr-x  2.0 unx    55846 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ransac_regressor.py
+-r-xr-xr-x  2.0 unx    53906 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge.py
+-r-xr-xr-x  2.0 unx    54224 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_classifier.py
+-r-xr-xr-x  2.0 unx    52763 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
+-r-xr-xr-x  2.0 unx    53539 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_cv.py
+-r-xr-xr-x  2.0 unx    59892 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_classifier.py
+-r-xr-xr-x  2.0 unx    54506 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
+-r-xr-xr-x  2.0 unx    57361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_regressor.py
+-r-xr-xr-x  2.0 unx    52794 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
+-r-xr-xr-x  2.0 unx    53765 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/tweedie_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/__init__.py
+-r-xr-xr-x  2.0 unx    53160 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/isomap.py
+-r-xr-xr-x  2.0 unx    52378 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/mds.py
+-r-xr-xr-x  2.0 unx    53149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/spectral_embedding.py
+-r-xr-xr-x  2.0 unx    56421 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/tsne.py
+-rw-r--r--  2.0 unx      304 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/__init__.py
+-rw-r--r--  2.0 unx    40077 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/classification.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/correlation.py
+-rw-r--r--  2.0 unx     4757 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/covariance.py
+-rw-r--r--  2.0 unx    12037 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/metrics_utils.py
+-rw-r--r--  2.0 unx    15397 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/ranking.py
+-rw-r--r--  2.0 unx    23144 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/regression.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/__init__.py
+-r-xr-xr-x  2.0 unx    57065 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
+-r-xr-xr-x  2.0 unx    55067 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/gaussian_mixture.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/__init__.py
+-r-xr-xr-x  2.0 unx    57616 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/grid_search_cv.py
+-r-xr-xr-x  2.0 unx    58460 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/randomized_search_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/__init__.py
+-r-xr-xr-x  2.0 unx    51048 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
+-r-xr-xr-x  2.0 unx    51976 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
+-r-xr-xr-x  2.0 unx    51306 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/output_code_classifier.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/__init__.py
+-r-xr-xr-x  2.0 unx    51633 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
+-r-xr-xr-x  2.0 unx    51954 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/categorical_nb.py
+-r-xr-xr-x  2.0 unx    51641 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/complement_nb.py
+-r-xr-xr-x  2.0 unx    50781 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
+-r-xr-xr-x  2.0 unx    51398 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/__init__.py
+-r-xr-xr-x  2.0 unx    54185 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    53667 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx    52144 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/kernel_density.py
+-r-xr-xr-x  2.0 unx    54425 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/local_outlier_factor.py
+-r-xr-xr-x  2.0 unx    50948 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/nearest_centroid.py
+-r-xr-xr-x  2.0 unx    52857 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/nearest_neighbors.py
+-r-xr-xr-x  2.0 unx    54333 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
+-r-xr-xr-x  2.0 unx    54814 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    53700 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/__init__.py
+-r-xr-xr-x  2.0 unx    51349 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
+-r-xr-xr-x  2.0 unx    58847 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/mlp_classifier.py
+-r-xr-xr-x  2.0 unx    58124 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/mlp_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/pipeline/__init__.py
+-rw-r--r--  2.0 unx    23381 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/pipeline/pipeline.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     6092 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/binarizer.py
+-rw-r--r--  2.0 unx    20422 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
+-rw-r--r--  2.0 unx     6285 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/label_encoder.py
+-rw-r--r--  2.0 unx     8491 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
+-rw-r--r--  2.0 unx    10716 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/min_max_scaler.py
+-rw-r--r--  2.0 unx     5951 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/normalizer.py
+-rw-r--r--  2.0 unx    66998 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
+-rw-r--r--  2.0 unx    27848 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
+-r-xr-xr-x  2.0 unx    51483 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/preprocessing/polynomial_features.py
+-rw-r--r--  2.0 unx    11981 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/robust_scaler.py
+-rw-r--r--  2.0 unx    10672 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/standard_scaler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/semi_supervised/__init__.py
+-r-xr-xr-x  2.0 unx    51820 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/semi_supervised/label_propagation.py
+-r-xr-xr-x  2.0 unx    52184 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/semi_supervised/label_spreading.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/__init__.py
+-r-xr-xr-x  2.0 unx    54362 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/linear_svc.py
+-r-xr-xr-x  2.0 unx    52777 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/linear_svr.py
+-r-xr-xr-x  2.0 unx    55076 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/nu_svc.py
+-r-xr-xr-x  2.0 unx    52152 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/nu_svr.py
+-r-xr-xr-x  2.0 unx    55239 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/svc.py
+-r-xr-xr-x  2.0 unx    52355 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/svr.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/__init__.py
+-r-xr-xr-x  2.0 unx    57438 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/decision_tree_classifier.py
+-r-xr-xr-x  2.0 unx    56134 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/decision_tree_regressor.py
+-r-xr-xr-x  2.0 unx    56801 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/extra_tree_classifier.py
+-r-xr-xr-x  2.0 unx    55506 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/extra_tree_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/__init__.py
+-r-xr-xr-x  2.0 unx    61208 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/xgb_classifier.py
+-r-xr-xr-x  2.0 unx    60714 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/xgboost/xgb_regressor.py
+-r-xr-xr-x  2.0 unx    61372 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
+-r-xr-xr-x  2.0 unx    60905 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
+-rw-r--r--  2.0 unx     1381 b- defN 23-Jun-23 01:55 snowflake/ml/registry/_schema.py
+-rw-r--r--  2.0 unx    84697 b- defN 23-Jun-23 01:55 snowflake/ml/registry/model_registry.py
+-rw-r--r--  2.0 unx     6138 b- defN 23-Jun-23 01:55 snowflake/ml/utils/connection_params.py
+-rw-r--r--  2.0 unx     3893 b- defN 23-Jun-23 01:55 snowflake/ml/utils/sparse.py
+-r-xr-xr-x  2.0 unx       16 b- defN 23-Jun-23 01:58 snowflake/ml/version.py
+?rw-------  2.0 unx       91 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/WHEEL
+?rw-------  2.0 unx    11756 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/METADATA
+?rw-------  2.0 unx    26137 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/RECORD
+246 files, 9029188 bytes uncompressed, 1812344 bytes compressed:  79.9%
```

## zipnote {}

```diff
@@ -723,17 +723,17 @@
 
 Filename: snowflake/ml/utils/sparse.py
 Comment: 
 
 Filename: snowflake/ml/version.py
 Comment: 
 
-Filename: snowflake_ml_python-1.0.1.dist-info/WHEEL
+Filename: snowflake_ml_python-1.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: snowflake_ml_python-1.0.1.dist-info/METADATA
+Filename: snowflake_ml_python-1.0.2.dist-info/METADATA
 Comment: 
 
-Filename: snowflake_ml_python-1.0.1.dist-info/RECORD
+Filename: snowflake_ml_python-1.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## snowflake/ml/_internal/file_utils.py

```diff
@@ -1,19 +1,17 @@
 import contextlib
 import hashlib
-import importlib
 import io
 import os
 import pathlib
+import pkgutil
 import shutil
 import tempfile
 import zipfile
-from typing import IO, Generator, Optional, Tuple, Union
-
-from snowflake.snowpark import session as snowpark_session
+from typing import IO, Generator, List, Optional, Union
 
 GENERATED_PY_FILE_EXT = (".pyc", ".pyo", ".pyd", ".pyi")
 
 
 def copy_file_or_tree(src: str, dst_dir: str) -> None:
     """Copy file or directory into target directory.
 
@@ -112,27 +110,14 @@
     """
     with tempfile.TemporaryDirectory(dir=temp_root) as tempdir:
         with zipfile.ZipFile(stream, mode="r", compression=zipfile.ZIP_DEFLATED) as zf:
             zf.extractall(path=tempdir)
         yield tempdir
 
 
-@contextlib.contextmanager
-def zip_snowml() -> Generator[Tuple[io.BytesIO, str], None, None]:
-    """Zip the snowflake-ml source code as a zip-file for import.
-
-    Yields:
-        A bytes IO stream containing the zip file.
-    """
-    snowml_path = list(importlib.import_module("snowflake.ml").__path__)[0]
-    root_path = os.path.normpath(os.path.join(snowml_path, os.pardir, os.pardir))
-    with zip_file_or_directory_to_stream(snowml_path, root_path) as stream:
-        yield stream, hash_directory(snowml_path)
-
-
 def hash_directory(directory: Union[str, pathlib.Path]) -> str:
     """Hash the **content** of a folder recursively using SHA-1.
 
     Args:
         directory: The path to the directory to be hashed.
 
     Returns:
@@ -150,25 +135,13 @@
             elif path.is_dir():
                 hash = _update_hash_from_dir(path, hash)
         return hash
 
     return _update_hash_from_dir(directory, hashlib.sha1()).hexdigest()
 
 
-def upload_snowml(session: snowpark_session.Session, stage_location: Optional[str] = None) -> str:
-    """Upload the SnowML local code into a stage if provided, or a session stage.
-    It will label the file name using the SHA-1 of the snowflake.ml folder, so that if the source code does not change,
-    it won't reupload. Any changes will, however, result a new zip file.
-
-    Args:
-        session: Snowpark connection session.
-        stage_location: The path to the stage location where the uploaded SnowML should be. Defaults to None.
-
-    Returns:
-        The path to the uploaded SnowML zip file.
-    """
-    with zip_snowml() as (stream, hash_str):
-        if stage_location is None:
-            stage_location = session.get_session_stage()
-        file_location = os.path.join(stage_location, f"snowml_{hash_str}.zip")
-        session.file.put_stream(stream, stage_location=file_location, auto_compress=False, overwrite=False)
-    return file_location
+def get_all_modules(dirname: str, prefix: str = "") -> List[pkgutil.ModuleInfo]:
+    subdirs = [f.path for f in os.scandir(dirname) if f.is_dir()]
+    modules = list(pkgutil.iter_modules(subdirs, prefix=prefix))
+    for dirname in subdirs:
+        modules.extend(get_all_modules(dirname, prefix=f"{prefix}.{dirname}" if prefix else dirname))
+    return modules
```

## snowflake/ml/_internal/utils/identifier.py

```diff
@@ -1,21 +1,26 @@
 import re
 from typing import Any, List, Optional, Tuple, Union, overload
 
 from snowflake.snowpark._internal.analyzer import analyzer_utils
 
 # Snowflake Identifier Regex. See https://docs.snowflake.com/en/sql-reference/identifiers-syntax.html.
-_SF_UNQUOTED_IDENTIFIER = "[A-Za-z_][A-Za-z0-9_$]*"
+_SF_UNQUOTED_CASE_INSENSITIVE_IDENTIFIER = "[A-Za-z_][A-Za-z0-9_$]*"
+_SF_UNQUOTED_CASE_SENSITIVE_IDENTIFIER = "[A-Z_][A-Z0-9_$]*"
 SF_QUOTED_IDENTIFIER = '"(?:[^"]|"")*"'
-_SF_IDENTIFIER = f"({_SF_UNQUOTED_IDENTIFIER}|{SF_QUOTED_IDENTIFIER})"
+_SF_IDENTIFIER = f"({_SF_UNQUOTED_CASE_INSENSITIVE_IDENTIFIER}|{SF_QUOTED_IDENTIFIER})"
 _SF_SCHEMA_LEVEL_OBJECT = rf"{_SF_IDENTIFIER}\.{_SF_IDENTIFIER}\.{_SF_IDENTIFIER}(.*)"
 _SF_SCHEMA_LEVEL_OBJECT_RE = re.compile(_SF_SCHEMA_LEVEL_OBJECT)
 
-UNQUOTED_CASE_INSENSITIVE_RE = re.compile(f"^({_SF_UNQUOTED_IDENTIFIER})$")
+UNQUOTED_CASE_INSENSITIVE_RE = re.compile(f"^({_SF_UNQUOTED_CASE_INSENSITIVE_IDENTIFIER})$")
+UNQUOTED_CASE_SENSITIVE_RE = re.compile(f"^({_SF_UNQUOTED_CASE_SENSITIVE_IDENTIFIER})$")
 QUOTED_IDENTIFIER_RE = re.compile(f"^({SF_QUOTED_IDENTIFIER})$")
+DOUBLE_QUOTE = '"'
+
+quote_name_without_upper_casing = analyzer_utils.quote_name_without_upper_casing
 
 
 def _is_quoted(id: str) -> bool:
     """Checks if input is quoted.
 
     NOTE: Snowflake treats all identifiers as UPPERCASE by default. That is 'Hello' would become 'HELLO'. To preserve
     case, one needs to use quoted identifiers, e.g. "Hello" (note the double quote). Callers must take care of that
@@ -57,18 +62,55 @@
 
     Returns:
         String with quotes removed if quoted; original string otherwise.
     """
     if not _is_quoted(id):
         return id.upper()
     unquoted_id = id[1:-1]
-    return unquoted_id.replace('""', '"')
+    return unquoted_id.replace(DOUBLE_QUOTE + DOUBLE_QUOTE, DOUBLE_QUOTE)
 
 
-quote_name_without_upper_casing = analyzer_utils.quote_name_without_upper_casing
+def _get_escaped_name(id: str) -> str:
+    """Add double quotes to escape quotes.
+        Replace double quotes with double double quotes if there is existing double quotes
+
+    NOTE: See note in :meth:`_is_quoted`.
+
+    Args:
+        id: The string to be checked & treated.
+
+    Returns:
+        String with quotes would doubled; original string would add double quotes.
+    """
+    escape_quotes = id.replace(DOUBLE_QUOTE, DOUBLE_QUOTE + DOUBLE_QUOTE)
+    return DOUBLE_QUOTE + escape_quotes + DOUBLE_QUOTE
+
+
+def get_inferred_name(id: str) -> str:
+    """Double quote id when it is case-sensitive and can start with and
+    contain any valid characters; unquote otherwise.
+
+    Examples:
+        COL1 -> COL1
+        1COL -> "1COL"
+        Col -> "Col"
+        "COL" -> \"""COL""\"  (ignore '\')
+        COL 1 -> "COL 1"
+
+    Args:
+        id: The string to be checked & treated.
+
+    Returns:
+        Double quoted identifier if necessary; unquoted string otherwise.
+    """
+    if UNQUOTED_CASE_SENSITIVE_RE.match(id):
+        return id
+    escaped_id = get_escaped_names(id)
+    assert isinstance(escaped_id, str)
+    return escaped_id
 
 
 def concat_names(ids: List[str]) -> str:
     """Concatenates `ids` to form one valid id.
 
     NOTE: See note in :meth:`_is_quoted`.
 
@@ -85,15 +127,15 @@
             # If any part is quoted, the user cares about case.
             quotes_needed = True
             # Remove quotes before using it.
             id = _get_unescaped_name(id)
         parts.append(id)
     final_id = "".join(parts)
     if quotes_needed:
-        return quote_name_without_upper_casing(final_id)
+        return _get_escaped_name(final_id)
     return final_id
 
 
 def parse_schema_level_object_identifier(
     path: str,
 ) -> Tuple[Union[str, Any], Union[str, Any], Union[str, Any], Union[str, Any]]:
     """Parse a string which starts with schema level object.
@@ -131,15 +173,15 @@
 @overload
 def get_unescaped_names(ids: List[str]) -> List[str]:
     ...
 
 
 def get_unescaped_names(ids: Optional[Union[str, List[str]]]) -> Optional[Union[str, List[str]]]:
     """Given a user provided identifier(s), this method will compute the equivalent column name identifier(s) in the
-    response pandas dataframe(i.e., in the respones of snowpark_df.to_pandas()) using the rules defined here
+    response pandas dataframe(i.e., in the response of snowpark_df.to_pandas()) using the rules defined here
     https://docs.snowflake.com/en/sql-reference/identifiers-syntax.
 
     Args:
         ids: User provided column name identifier(s).
 
     Returns:
         Equivalent column name identifier(s) in the response pandas dataframe.
@@ -152,7 +194,32 @@
         return None
     elif type(ids) is list:
         return [_get_unescaped_name(id) for id in ids]
     elif type(ids) is str:
         return _get_unescaped_name(ids)
     else:
         raise ValueError("Unsupported type. Only string or list of string are supported for selecting columns.")
+
+
+def get_escaped_names(ids: Optional[Union[str, List[str]]]) -> Optional[Union[str, List[str]]]:
+    """Given a user provided identifier(s), this method will compute the equivalent column name identifier(s)
+    in case of column name contains special characters, and maintains case-sensitivity
+    https://docs.snowflake.com/en/sql-reference/identifiers-syntax.
+
+    Args:
+        ids: User provided column name identifier(s).
+
+    Returns:
+        Double-quoted Identifiers for column names, to make sure that column names are case sensitive
+
+    Raises:
+        ValueError: if input types is unsupported or column name identifiers are invalid.
+    """
+
+    if ids is None:
+        return None
+    elif type(ids) is list:
+        return [_get_escaped_name(id) for id in ids]
+    elif type(ids) is str:
+        return _get_escaped_name(ids)
+    else:
+        raise ValueError("Unsupported type. Only string or list of string are supported for selecting columns.")
```

## snowflake/ml/model/_core_requirements.py

```diff
@@ -1 +1 @@
-REQUIREMENTS=['anyio>=3.5.0,<4', 'cloudpickle', 'numpy>=1.23,<2', 'packaging>=20.9,<24', 'pandas>=1.0.0,<2', 'pyyaml>=6.0,<7', 'scikit-learn>=1.2.1,<2', 'snowflake-snowpark-python>=1.4.0,<2', 'typing-extensions>=4.1.0,<5']
+REQUIREMENTS=['anyio>=3.5.0,<4', 'cloudpickle', 'numpy>=1.23,<2', 'packaging>=20.9,<24', 'pandas>=1.0.0,<2', 'pyyaml>=6.0,<7', 'snowflake-snowpark-python>=1.4.0,<2', 'typing-extensions>=4.1.0,<5']
```

## snowflake/ml/model/_deploy_client/warehouse/deploy.py

```diff
@@ -2,15 +2,15 @@
 import tempfile
 import warnings
 from types import ModuleType
 from typing import IO, List, Optional, Tuple, TypedDict, Union
 
 from typing_extensions import Unpack
 
-from snowflake.ml._internal import env as snowml_env, env_utils, file_utils
+from snowflake.ml._internal import env_utils
 from snowflake.ml.model import (
     _env as model_env,
     _model,
     _model_meta,
     type_hints as model_types,
 )
 from snowflake.ml.model._deploy_client.warehouse import infer_template
@@ -58,37 +58,27 @@
         meta = _model.load_model(session=session, model_stage_file_path=model_stage_file_path, meta_only=True)
 
     relax_version = kwargs.get("relax_version", False)
 
     if target_method not in meta.signatures.keys():
         raise ValueError(f"Target method {target_method} does not exist in model.")
 
-    _use_local_snowml = kwargs.get("_use_local_snowml", False)
-
-    final_packages = _get_model_final_packages(
-        meta, session, relax_version=relax_version, _use_local_snowml=_use_local_snowml
-    )
+    final_packages = _get_model_final_packages(meta, session, relax_version=relax_version)
 
     stage_location = kwargs.get("permanent_udf_stage_location", None)
     if stage_location:
         stage_location = stage_location.strip().rstrip("/")
         if not stage_location.startswith("@"):
             raise ValueError(f"Invalid stage location {stage_location}.")
 
-    _snowml_wheel_path = None
-    if _use_local_snowml:
-        _snowml_wheel_path = file_utils.upload_snowml(session, stage_location=stage_location)
-
     with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
         _write_UDF_py_file(f.file, extract_model_code, target_method, **kwargs)
         print(f"Generated UDF file is persisted at: {f.name}")
-        imports = (
-            ([model_dir_path] if model_dir_path else [])
-            + ([model_stage_file_path] if model_stage_file_path else [])
-            + ([_snowml_wheel_path] if _snowml_wheel_path else [])
+        imports = ([model_dir_path] if model_dir_path else []) + (
+            [model_stage_file_path] if model_stage_file_path else []
         )
 
         class _UDFParams(TypedDict):
             file_path: str
             func_name: str
             name: str
             input_types: List[st.DataType]
@@ -135,33 +125,32 @@
     """
     keep_order = kwargs.get("keep_order", True)
 
     udf_code = infer_template._UDF_CODE_TEMPLATE.format(
         extract_model_code=extract_model_code,
         keep_order_code=infer_template._KEEP_ORDER_CODE_TEMPLATE if keep_order else "",
         target_method=target_method,
+        code_dir_name=_model_meta.ModelMetadata.MODEL_CODE_DIR,
     )
     f.write(udf_code)
     f.flush()
 
 
 def _get_model_final_packages(
     meta: _model_meta.ModelMetadata,
     session: snowpark_session.Session,
     relax_version: Optional[bool] = False,
-    _use_local_snowml: Optional[bool] = False,
 ) -> List[str]:
     """Generate final packages list of dependency of a model to be deployed to warehouse.
 
     Args:
         meta: Model metadata to get dependency information.
         session: Snowpark connection session.
         relax_version: Whether or not relax the version restriction when fail to resolve dependencies.
             Defaults to False.
-        _use_local_snowml: Flag to indicate if using local SnowML code as execution library
 
     Raises:
         RuntimeError: Raised when PIP requirements and dependencies from non-Snowflake anaconda channel found.
         RuntimeError: Raised when not all packages are available in snowflake conda channel.
 
     Returns:
         List of final packages string that is accepted by Snowpark register UDF call.
@@ -170,24 +159,14 @@
     if (
         any(channel.lower() not in ["", "snowflake"] for channel in meta._conda_dependencies.keys())
         or meta.pip_requirements
     ):
         raise RuntimeError("PIP requirements and dependencies from non-Snowflake anaconda channel is not supported.")
 
     deps = meta._conda_dependencies[""]
-    if _use_local_snowml:
-        local_snowml_version = snowml_env.VERSION
-        snowml_dept = next((dep for dep in deps if dep.name == env_utils._SNOWML_PKG_NAME), None)
-        if snowml_dept:
-            if not snowml_dept.specifier.contains(local_snowml_version) and not relax_version:
-                raise RuntimeError(
-                    "Incompatible snowflake-ml-python-version is found. "
-                    + f"Require {snowml_dept.specifier}, got {local_snowml_version}."
-                )
-            deps.remove(snowml_dept)
 
     try:
         final_packages = env_utils.resolve_conda_environment(
             deps, [model_env._SNOWFLAKE_CONDA_CHANNEL_URL], python_version=meta.python_version
         )
         if final_packages is None and relax_version:
             final_packages = env_utils.resolve_conda_environment(
```

## snowflake/ml/model/_deploy_client/warehouse/infer_template.py

```diff
@@ -44,18 +44,18 @@
    def __exit__(self, type, value, traceback):
       self._fd.close()
       self._lock.release()
 
 IMPORT_DIRECTORY_NAME = "snowflake_import_directory"
 import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]
 
-from snowflake.ml.model import _model
-
 {extract_model_code}
 
+sys.path.insert(0, os.path.join(extracted_model_dir_path, "{code_dir_name}"))
+from snowflake.ml.model import _model
 model, meta = _model._load_model_for_deploy(extracted_model_dir_path)
 
 # TODO(halu): Wire `max_batch_size`.
 # TODO(halu): Avoid per batch async detection branching.
 @vectorized(input=pd.DataFrame, max_batch_size=10)
 def infer(df):
     input_cols = [spec.name for spec in meta.signatures["{target_method}"].inputs]
```

## snowflake/ml/model/_handlers/_base.py

```diff
@@ -1,11 +1,11 @@
 from abc import ABC, abstractmethod
 from typing import Generic, Optional
 
-from typing_extensions import TypeGuard
+from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml.model import _model_meta, type_hints as model_types
 
 
 class _ModelHandler(ABC, Generic[model_types._ModelType]):
     """Provides handling for a given type of model defined by `type` class property."""
 
@@ -39,24 +39,26 @@
     def _save_model(
         name: str,
         model: model_types._ModelType,
         model_meta: _model_meta.ModelMetadata,
         model_blobs_dir_path: str,
         sample_input: Optional[model_types.SupportedDataType] = None,
         is_sub_model: Optional[bool] = False,
+        **kwargs: Unpack[model_types.ModelSaveOption],
     ) -> None:
         """Save the model.
 
         Args:
             name: Name of the model.
             model: The model object.
             model_meta: The model metadata.
             model_blobs_dir_path: Directory path to the model.
             sample_input: Sample input to infer the signatures from.
             is_sub_model: Flag to show if it is a sub model, a sub model does not need signature.
+            kwargs: Additional saving options.
         """
         ...
 
     @staticmethod
     @abstractmethod
     def _load_model(
         name: str, model_meta: _model_meta.ModelMetadata, model_blobs_dir_path: str
```

## snowflake/ml/model/_handlers/sklearn.py

```diff
@@ -97,14 +97,15 @@
         os.makedirs(model_blob_path, exist_ok=True)
         with open(os.path.join(model_blob_path, _SKLModelHandler.MODEL_BLOB_FILE), "wb") as f:
             cloudpickle.dump(model, f)
         base_meta = model_meta_api._ModelBlobMetadata(
             name=name, model_type=_SKLModelHandler.handler_type, path=_SKLModelHandler.MODEL_BLOB_FILE
         )
         model_meta.models[name] = base_meta
+        model_meta._include_if_absent([("scikit-learn", "scikit-learn")])
 
     @staticmethod
     def _load_model(
         name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str
     ) -> Union["sklearn.base.BaseEstimator", "sklearn.pipeline.Pipeline"]:
         model_blob_path = os.path.join(model_blobs_dir_path, name)
         if not hasattr(model_meta, "models"):
```

## snowflake/ml/model/_handlers/xgboost.py

```diff
@@ -91,15 +91,15 @@
         base_meta = model_meta_api._ModelBlobMetadata(
             name=name,
             model_type=_XGBModelHandler.handler_type,
             path=_XGBModelHandler.MODEL_BLOB_FILE,
             options={"xgb_estimator_type": model.__class__.__name__},
         )
         model_meta.models[name] = base_meta
-        model_meta._include_if_absent([("xgboost", "xgboost")])
+        model_meta._include_if_absent([("scikit-learn", "scikit-learn"), ("xgboost", "xgboost")])
 
     @staticmethod
     def _load_model(
         name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str
     ) -> Union["xgboost.Booster", "xgboost.XGBModel"]:
         import xgboost
```

## snowflake/ml/model/_model.py

```diff
@@ -1,33 +1,35 @@
 import os
 import tempfile
 import warnings
 from types import ModuleType
-from typing import Dict, List, Literal, Optional, Tuple, Union, overload
+from typing import TYPE_CHECKING, Dict, List, Literal, Optional, Tuple, Union, overload
 
-from snowflake.ml._internal import file_utils
+from snowflake.ml._internal import file_utils, type_utils
 from snowflake.ml.model import (
     _env,
     _model_handler,
     _model_meta,
     custom_model,
     model_signature,
     type_hints as model_types,
 )
-from snowflake.ml.modeling.framework import base
 from snowflake.snowpark import FileOperation, Session
 
+if TYPE_CHECKING:
+    from snowflake.ml.modeling.framework import base
+
 MODEL_BLOBS_DIR = "models"
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: base.BaseEstimator,
+    model: "base.BaseEstimator",
     model_dir_path: str,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
     code_paths: Optional[List[str]] = None,
@@ -131,15 +133,15 @@
     ...
 
 
 @overload
 def save_model(
     *,
     name: str,
-    model: base.BaseEstimator,
+    model: "base.BaseEstimator",
     session: Session,
     model_stage_file_path: str,
     metadata: Optional[Dict[str, str]] = None,
     conda_dependencies: Optional[List[str]] = None,
     pip_requirements: Optional[List[str]] = None,
     python_version: Optional[str] = None,
     ext_modules: Optional[List[ModuleType]] = None,
@@ -318,17 +320,19 @@
 
     if not ((model_stage_file_path is None) ^ (model_dir_path is None)):
         raise ValueError(
             "model_dir_path and model_stage_file_path both cannot be "
             + f"{'None' if model_stage_file_path is None else 'specified'} at the same time."
         )
 
-    if ((signatures is None) and (sample_input is None) and not isinstance(model, base.BaseEstimator)) or (
-        (signatures is not None) and (sample_input is not None)
-    ):
+    if (
+        (signatures is None)
+        and (sample_input is None)
+        and not type_utils.LazyType("snowflake.ml.modeling.framework.base.BaseEstimator").isinstance(model)
+    ) or ((signatures is not None) and (sample_input is not None)):
         raise ValueError(
             "Signatures and sample_input both cannot be "
             + f"{'None for local model' if signatures is None else 'specified'} at the same time."
         )
 
     if not options:
         options = {}
@@ -357,15 +361,15 @@
             ext_modules=ext_modules,
             code_paths=code_paths,
             options=options,
         )
 
     assert session and model_stage_file_path
     if os.path.splitext(model_stage_file_path)[1] != ".zip":
-        raise ValueError("Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
+        raise ValueError(f"Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
 
     with tempfile.TemporaryDirectory() as temp_local_model_dir_path:
         meta = _save(
             name=name,
             model=model,
             local_dir_path=temp_local_model_dir_path,
             signatures=signatures,
@@ -393,23 +397,23 @@
 
 
 def _save(
     *,
     name: str,
     model: model_types.SupportedModelType,
     local_dir_path: str,
-    signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
-    sample_input: Optional[model_types.SupportedDataType] = None,
-    metadata: Optional[Dict[str, str]] = None,
-    conda_dependencies: Optional[List[str]] = None,
-    pip_requirements: Optional[List[str]] = None,
-    python_version: Optional[str] = None,
-    ext_modules: Optional[List[ModuleType]] = None,
-    code_paths: Optional[List[str]] = None,
-    options: Optional[model_types.ModelSaveOption] = None,
+    signatures: Optional[Dict[str, model_signature.ModelSignature]],
+    sample_input: Optional[model_types.SupportedDataType],
+    metadata: Optional[Dict[str, str]],
+    conda_dependencies: Optional[List[str]],
+    pip_requirements: Optional[List[str]],
+    python_version: Optional[str],
+    ext_modules: Optional[List[ModuleType]],
+    code_paths: Optional[List[str]],
+    options: model_types.ModelSaveOption,
 ) -> _model_meta.ModelMetadata:
     local_dir_path = os.path.normpath(local_dir_path)
 
     handler = _model_handler._find_handler(model)
     if handler is None:
         raise TypeError(f"{type(model)} is not supported.")
     with _model_meta._create_model_metadata(
@@ -419,14 +423,15 @@
         metadata=metadata,
         code_paths=code_paths,
         signatures=signatures,
         ext_modules=ext_modules,
         conda_dependencies=conda_dependencies,
         pip_requirements=pip_requirements,
         python_version=python_version,
+        **options,
     ) as meta:
         model_blobs_path = os.path.join(local_dir_path, MODEL_BLOBS_DIR)
         os.makedirs(model_blobs_path, exist_ok=True)
         model = handler.cast_model(model)
         handler._save_model(
             name=name,
             model=model,
@@ -535,15 +540,15 @@
         if not os.path.isdir(model_dir_path):
             raise ValueError(f"Provided model directory {model_dir_path} is not a directory.")
 
         return _load(local_dir_path=model_dir_path, meta_only=meta_only)
 
     assert session and model_stage_file_path
     if os.path.splitext(model_stage_file_path)[1] != ".zip":
-        raise ValueError("Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
+        raise ValueError(f"Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
 
     fo = FileOperation(session=session)
     zf = fo.get_stream(model_stage_file_path)
     with file_utils.unzip_stream_in_temp_dir(stream=zf) as temp_local_model_dir_path:
         return _load(local_dir_path=temp_local_model_dir_path, meta_only=meta_only)
```

## snowflake/ml/model/_model_meta.py

```diff
@@ -1,14 +1,14 @@
 import dataclasses
+import importlib
 import os
 import sys
 import warnings
 from contextlib import contextmanager
 from datetime import datetime
-from pathlib import Path
 from types import ModuleType
 from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Tuple, cast
 
 import cloudpickle
 import yaml
 from packaging import version
 
@@ -20,16 +20,14 @@
     type_hints as model_types,
 )
 from snowflake.snowpark import DataFrame as SnowparkDataFrame
 
 MODEL_METADATA_VERSION = 1
 _BASIC_DEPENDENCIES = _core_requirements.REQUIREMENTS
 
-_BASIC_DEPENDENCIES.append(env_utils._SNOWML_PKG_NAME)
-
 
 @dataclasses.dataclass
 class _ModelBlobMetadata:
     """Dataclass to store metadata of an individual model blob (sub-model) in the packed model.
 
     Attributes:
         name: The name to refer the sub-model.
@@ -80,14 +78,18 @@
             current version would be captured. Defaults to None.
         **kwargs: Dict of attributes and values of the metadata. Used when loading from file.
 
     Yields:
         A model metadata object.
     """
     model_dir_path = os.path.normpath(model_dir_path)
+    embed_local_ml_library = kwargs.pop("embed_local_ml_library", False)
+    if embed_local_ml_library:
+        snowml_path = list(importlib.import_module("snowflake.ml").__path__)[0]
+        kwargs["local_ml_library_version"] = f"{snowml_env.VERSION}+{file_utils.hash_directory(snowml_path)}"
 
     model_meta = ModelMetadata(
         name=name,
         metadata=metadata,
         model_type=model_type,
         conda_dependencies=conda_dependencies,
         pip_requirements=pip_requirements,
@@ -96,14 +98,22 @@
         **kwargs,
     )
     if code_paths:
         code_dir_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
         os.makedirs(code_dir_path, exist_ok=True)
         for code_path in code_paths:
             file_utils.copy_file_or_tree(code_path, code_dir_path)
+
+    if embed_local_ml_library:
+        code_dir_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
+        snowml_path = list(importlib.import_module("snowflake.ml").__path__)[0]
+        snowml_path_in_code = os.path.join(code_dir_path, "snowflake")
+        os.makedirs(snowml_path_in_code, exist_ok=True)
+        file_utils.copy_file_or_tree(snowml_path, snowml_path_in_code)
+
     try:
         imported_modules = []
         if ext_modules:
             registered_modules = cloudpickle.list_registry_pickle_by_value()
             for mod in ext_modules:
                 if mod.__name__ not in registered_modules:
                     cloudpickle.register_pickle_by_value(mod)
@@ -113,36 +123,33 @@
     finally:
         for mod in imported_modules:
             cloudpickle.unregister_pickle_by_value(mod)
 
 
 def _load_model_metadata(model_dir_path: str) -> "ModelMetadata":
     """Load models for a directory. Model is initially loaded normally. If additional codes are included when packed,
-        the code path is added to system path to be imported and overwrites those modules with the same name that has
-        been imported.
+        the code path is added to system path to be imported with highest priority.
 
     Args:
         model_dir_path: Path to the directory containing the model to be loaded.
 
     Returns:
         A model metadata object.
     """
     model_dir_path = os.path.normpath(model_dir_path)
 
     meta = ModelMetadata.load_model_metadata(model_dir_path)
     code_path = os.path.join(model_dir_path, ModelMetadata.MODEL_CODE_DIR)
     if os.path.exists(code_path):
-        sys.path = [code_path] + sys.path
-        modules = [
-            p.stem
-            for p in Path(code_path).rglob("*.py")
-            if p.is_file() and p.name != "__init__.py" and p.name != "__main__.py"
-        ]
+        if code_path in sys.path:
+            sys.path.remove(code_path)
+        sys.path.insert(0, code_path)
+        modules = file_utils.get_all_modules(code_path)
         for module in modules:
-            sys.modules.pop(module, None)
+            sys.modules.pop(module.name, None)
     return meta
 
 
 class ModelMetadata:
     """Model metadata for Snowflake native model packaged model.
 
     Attributes:
@@ -202,16 +209,18 @@
 
         self._conda_dependencies = env_utils.validate_conda_dependency_string_list(
             conda_dependencies if conda_dependencies else []
         )
         self._pip_requirements = env_utils.validate_pip_requirement_string_list(
             pip_requirements if pip_requirements else []
         )
-
-        self._include_if_absent([(dep, dep) for dep in _BASIC_DEPENDENCIES])
+        if "local_ml_library_version" in kwargs:
+            self._include_if_absent([(dep, dep) for dep in _BASIC_DEPENDENCIES])
+        else:
+            self._include_if_absent([(dep, dep) for dep in _BASIC_DEPENDENCIES + [env_utils._SNOWML_PKG_NAME]])
 
         self.__dict__.update(kwargs)
 
     @property
     def pip_requirements(self) -> List[str]:
         """List of pip Python packages requirements for running the model."""
         return list(sorted(map(str, self._pip_requirements)))
@@ -340,15 +349,15 @@
         Returns:
             Loaded model metadata object.
         """
         model_yaml_path = os.path.join(path, ModelMetadata.MODEL_METADATA_FILE)
         with open(model_yaml_path) as f:
             loaded_mata = yaml.safe_load(f.read())
 
-        loaded_mata_version = loaded_mata.get("version", None)
+        loaded_mata_version = loaded_mata.pop("version", None)
         if not loaded_mata_version or loaded_mata_version != MODEL_METADATA_VERSION:
             raise NotImplementedError("Unknown or unsupported model metadata file found.")
 
         meta = ModelMetadata.from_dict(loaded_mata)
         env_dir_path = os.path.join(path, ModelMetadata.ENV_DIR)
         meta._conda_dependencies, python_version = _env.load_conda_env_file(
             os.path.join(env_dir_path, _env._CONDA_ENV_FILE_NAME)
```

## snowflake/ml/model/type_hints.py

```diff
@@ -1,24 +1,23 @@
 # mypy: disable-error-code="import"
 from typing import TYPE_CHECKING, Sequence, TypedDict, TypeVar, Union
 
 import numpy.typing as npt
 from typing_extensions import NotRequired, TypeAlias
 
-from snowflake.ml.modeling.framework import base
-
 if TYPE_CHECKING:
     import numpy as np
     import pandas as pd
     import sklearn.base
     import sklearn.pipeline
     import xgboost
 
     import snowflake.ml.model.custom_model
     import snowflake.snowpark
+    from snowflake.ml.modeling.framework import base  # noqa: F401
 
 
 _SupportedBuiltins = Union[int, float, bool, str, bytes, "_SupportedBuiltinsList"]
 _SupportedNumpyDtype = Union[
     "np.int8",
     "np.int16",
     "np.int32",
@@ -50,15 +49,15 @@
     "snowflake.ml.model.custom_model.CustomModel",
     "sklearn.base.BaseEstimator",
     "sklearn.pipeline.Pipeline",
     "xgboost.XGBModel",
     "xgboost.Booster",
 ]
 
-SupportedSnowMLModelType: TypeAlias = base.BaseEstimator
+SupportedSnowMLModelType: TypeAlias = "base.BaseEstimator"
 
 SupportedModelType = Union[
     SupportedLocalModelType,
     SupportedSnowMLModelType,
 ]
 """This is defined as the type that Snowflake native model packaging could accept.
 Here is all acceptable types of Snowflake native model packaging and its handler file in _handlers/ folder.
@@ -80,23 +79,16 @@
 class DeployOptions(TypedDict):
     """Common Options for deploying to Snowflake.
 
     output_with_input_features: Whether or not preserve the input columns in the output when predicting.
         Defaults to False.
     keep_order: Whether or not preserve the row order when predicting. Only available for dataframe has fewer than 2**64
         rows. Defaults to True.
-
-    Internal-only options
-    _use_local_snowml: Use local SnowML when as the execution library of the deployment. If set to True, local SnowML
-        would be packed and uploaded to 1) session stage, if it is a temporary deployment, or 2) the provided stage path
-        if it is a permanent deployment. It should be set to True before SnowML available in Snowflake Anaconda Channel.
-        Default to False.
     """
 
-    _use_local_snowml: NotRequired[bool]
     output_with_input_features: NotRequired[bool]
     keep_order: NotRequired[bool]
 
 
 class WarehouseDeployOptions(DeployOptions):
     """Options for deploying to the Snowflake Warehouse.
 
@@ -111,22 +103,24 @@
     relax_version: NotRequired[bool]
     replace_udf: NotRequired[bool]
 
 
 class ModelSaveOption(TypedDict):
     """Options for saving the model.
 
+    embed_local_ml_library: Embedding local SnowML into the code directory of the folder.
     allow_overwritten_stage_file: Flag to indicate when saving the model as a stage file, whether overwriting existed
         file is allowed. Default to False.
     """
 
+    embed_local_ml_library: NotRequired[bool]
     allow_overwritten_stage_file: NotRequired[bool]
 
 
-class CustomModelSaveOption(TypedDict):
+class CustomModelSaveOption(ModelSaveOption):
     ...
 
 
 class SKLModelSaveOptions(ModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
```

## snowflake/ml/modeling/calibration/calibrated_classifier_cv.py

```diff
@@ -678,34 +678,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/affinity_propagation.py

```diff
@@ -655,34 +655,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/agglomerative_clustering.py

```diff
@@ -688,34 +688,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/birch.py

```diff
@@ -646,34 +646,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/bisecting_k_means.py

```diff
@@ -695,34 +695,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/dbscan.py

```diff
@@ -663,34 +663,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/feature_agglomeration.py

```diff
@@ -695,34 +695,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/k_means.py

```diff
@@ -690,34 +690,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/mean_shift.py

```diff
@@ -666,34 +666,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/mini_batch_k_means.py

```diff
@@ -716,34 +716,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/optics.py

```diff
@@ -736,34 +736,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/spectral_biclustering.py

```diff
@@ -674,34 +674,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/spectral_clustering.py

```diff
@@ -732,34 +732,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/cluster/spectral_coclustering.py

```diff
@@ -653,34 +653,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/compose/column_transformer.py

```diff
@@ -683,34 +683,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/compose/transformed_target_regressor.py

```diff
@@ -642,34 +642,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/elliptic_envelope.py

```diff
@@ -639,34 +639,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/empirical_covariance.py

```diff
@@ -615,34 +615,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/graphical_lasso.py

```diff
@@ -649,34 +649,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/graphical_lasso_cv.py

```diff
@@ -682,34 +682,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/ledoit_wolf.py

```diff
@@ -622,34 +622,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/min_cov_det.py

```diff
@@ -634,34 +634,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/oas.py

```diff
@@ -615,34 +615,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/covariance/shrunk_covariance.py

```diff
@@ -621,34 +621,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/dictionary_learning.py

```diff
@@ -722,34 +722,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/factor_analysis.py

```diff
@@ -664,34 +664,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/fast_ica.py

```diff
@@ -682,34 +682,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/incremental_pca.py

```diff
@@ -634,34 +634,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/kernel_pca.py

```diff
@@ -730,34 +730,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py

```diff
@@ -752,34 +752,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py

```diff
@@ -697,34 +697,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/pca.py

```diff
@@ -699,34 +699,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/sparse_pca.py

```diff
@@ -672,34 +672,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/decomposition/truncated_svd.py

```diff
@@ -653,34 +653,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py

```diff
@@ -668,34 +668,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py

```diff
@@ -630,34 +630,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/ada_boost_classifier.py

```diff
@@ -655,34 +655,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/ada_boost_regressor.py

```diff
@@ -652,34 +652,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/bagging_classifier.py

```diff
@@ -687,34 +687,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/bagging_regressor.py

```diff
@@ -687,34 +687,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/extra_trees_classifier.py

```diff
@@ -789,34 +789,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/extra_trees_regressor.py

```diff
@@ -768,34 +768,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py

```diff
@@ -803,34 +803,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py

```diff
@@ -812,34 +812,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py

```diff
@@ -781,34 +781,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py

```diff
@@ -771,34 +771,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/isolation_forest.py

```diff
@@ -676,34 +676,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/random_forest_classifier.py

```diff
@@ -785,34 +785,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/random_forest_regressor.py

```diff
@@ -764,34 +764,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/stacking_regressor.py

```diff
@@ -666,34 +666,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/voting_classifier.py

```diff
@@ -648,34 +648,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/ensemble/voting_regressor.py

```diff
@@ -630,34 +630,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/generic_univariate_select.py

```diff
@@ -620,34 +620,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/select_fdr.py

```diff
@@ -616,34 +616,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/select_fpr.py

```diff
@@ -616,34 +616,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/select_fwe.py

```diff
@@ -616,34 +616,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/select_k_best.py

```diff
@@ -617,34 +617,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/select_percentile.py

```diff
@@ -616,34 +616,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/sequential_feature_selector.py

```diff
@@ -676,34 +676,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/feature_selection/variance_threshold.py

```diff
@@ -609,34 +609,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py

```diff
@@ -702,34 +702,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py

```diff
@@ -685,34 +685,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/impute/iterative_imputer.py

```diff
@@ -728,34 +728,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/impute/knn_imputer.py

```diff
@@ -663,34 +663,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/impute/missing_indicator.py

```diff
@@ -637,34 +637,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py

```diff
@@ -612,34 +612,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_approximation/nystroem.py

```diff
@@ -660,34 +660,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py

```diff
@@ -636,34 +636,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_approximation/rbf_sampler.py

```diff
@@ -623,34 +623,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py

```diff
@@ -621,34 +621,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/kernel_ridge/kernel_ridge.py

```diff
@@ -655,34 +655,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/lightgbm/lgbm_classifier.py

```diff
@@ -644,34 +644,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/lightgbm/lgbm_regressor.py

```diff
@@ -644,34 +644,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ard_regression.py

```diff
@@ -664,34 +664,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/bayesian_ridge.py

```diff
@@ -673,34 +673,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/elastic_net.py

```diff
@@ -679,34 +679,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/elastic_net_cv.py

```diff
@@ -715,34 +715,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/gamma_regressor.py

```diff
@@ -660,34 +660,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/huber_regressor.py

```diff
@@ -643,34 +643,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lars.py

```diff
@@ -672,34 +672,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lars_cv.py

```diff
@@ -680,34 +680,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lasso.py

```diff
@@ -673,34 +673,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lasso_cv.py

```diff
@@ -701,34 +701,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lasso_lars.py

```diff
@@ -693,34 +693,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lasso_lars_cv.py

```diff
@@ -694,34 +694,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/lasso_lars_ic.py

```diff
@@ -677,34 +677,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/linear_regression.py

```diff
@@ -630,34 +630,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/logistic_regression.py

```diff
@@ -744,34 +744,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/logistic_regression_cv.py

```diff
@@ -765,34 +765,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net.py

```diff
@@ -663,34 +663,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py

```diff
@@ -704,34 +704,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/multi_task_lasso.py

```diff
@@ -655,34 +655,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py

```diff
@@ -690,34 +690,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py

```diff
@@ -638,34 +638,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py

```diff
@@ -712,34 +712,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py

```diff
@@ -699,34 +699,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/perceptron.py

```diff
@@ -712,34 +712,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/poisson_regressor.py

```diff
@@ -660,34 +660,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ransac_regressor.py

```diff
@@ -723,34 +723,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ridge.py

```diff
@@ -693,34 +693,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ridge_classifier.py

```diff
@@ -693,34 +693,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ridge_classifier_cv.py

```diff
@@ -659,34 +659,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/ridge_cv.py

```diff
@@ -680,34 +680,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/sgd_classifier.py

```diff
@@ -799,34 +799,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/sgd_one_class_svm.py

```diff
@@ -699,34 +699,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/sgd_regressor.py

```diff
@@ -765,34 +765,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/theil_sen_regressor.py

```diff
@@ -667,34 +667,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/linear_model/tweedie_regressor.py

```diff
@@ -693,34 +693,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/manifold/isomap.py

```diff
@@ -691,34 +691,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/manifold/mds.py

```diff
@@ -674,34 +674,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/manifold/spectral_embedding.py

```diff
@@ -676,34 +676,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/manifold/tsne.py

```diff
@@ -741,34 +741,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/metrics/classification.py

```diff
@@ -50,15 +50,20 @@
 
         The best performance is 1 with ``normalize == True`` and the number
         of samples with ``normalize == False``.
     """
     metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
 
     if isinstance(y_true_col_names, str) or (len(y_true_col_names) == 1):
-        score_column = F.iff(df[y_true_col_names] == df[y_pred_col_names], 1, 0)  # type: ignore[arg-type]
+        y_true, y_pred = (
+            (y_true_col_names, y_pred_col_names)
+            if isinstance(y_true_col_names, str)
+            else (y_true_col_names[0], y_pred_col_names[0])
+        )
+        score_column = F.iff(df[y_true] == df[y_pred], 1, 0)  # type: ignore[arg-type]
     # multilabel
     else:
         expr = " and ".join([f"({y_true_col_names[i]} = {y_pred_col_names[i]})" for i in range(len(y_true_col_names))])
         score_column = F.iff(expr, 1, 0)  # type: ignore[arg-type]
     return metrics_utils.weighted_sum(
         df=df,
         sample_score_column=score_column,
```

## snowflake/ml/modeling/metrics/regression.py

```diff
@@ -1,39 +1,547 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 
 import inspect
+from typing import List, Optional, Union
 
+import cloudpickle
+import numpy as np
+import numpy.typing as npt
+import sklearn
+from packaging import version
+from sklearn import metrics
+
+from snowflake import snowpark
 from snowflake.ml._internal import telemetry
-from snowflake.snowpark import DataFrame, functions as F
+from snowflake.ml.modeling.metrics import metrics_utils
+from snowflake.snowpark import functions as F
+from snowflake.snowpark._internal import utils as snowpark_utils
 
 _PROJECT = "ModelDevelopment"
 _SUBPROJECT = "Metrics"
 
 
-@telemetry.send_api_usage_telemetry(
-    project=_PROJECT,
-    subproject=_SUBPROJECT,
-)
-def r2_score(*, df: DataFrame, y_true_col_name: str, y_pred_col_name: str) -> float:
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def d2_absolute_error_score(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    :math:`D^2` regression score function, \
+    fraction of absolute error explained.
+
+    Best possible score is 1.0 and it can be negative (because the model can be
+    arbitrarily worse). A model that always uses the empirical median of `y_true`
+    as constant prediction, disregarding the input features,
+    gets a :math:`D^2` score of 0.0.
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        multioutput: {'raw_values', 'uniform_average'}  or array-like of shape \
+            (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of errors in case of multioutput input.
+            'uniform_average':
+                Errors of all outputs are averaged with uniform weight.
+
+    Returns:
+        score: float or ndarray of floats
+            The :math:`D^2` score with an absolute error deviance
+            or ndarray of scores if 'multioutput' is 'raw_values'.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"d2_absolute_error_score_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def d2_absolute_error_score_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        score = metrics.d2_absolute_error_score(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            multioutput=multioutput,
+        )
+
+        return cloudpickle.dumps(score)  # type: ignore[no-any-return]
+
+    score: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return score
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def d2_pinball_score(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    alpha: float = 0.5,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    :math:`D^2` regression score function, fraction of pinball loss explained.
+
+    Best possible score is 1.0 and it can be negative (because the model can be
+    arbitrarily worse). A model that always uses the empirical alpha-quantile of
+    `y_true` as constant prediction, disregarding the input features,
+    gets a :math:`D^2` score of 0.0.
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        alpha: Slope of the pinball deviance. It determines the quantile level
+            alpha for which the pinball deviance and also D2 are optimal.
+            The default `alpha=0.5` is equivalent to `d2_absolute_error_score`.
+        multioutput: {'raw_values', 'uniform_average'}  or array-like of shape \
+            (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of errors in case of multioutput input.
+            'uniform_average':
+                Scores of all outputs are averaged with uniform weight.
+
+    Returns:
+        score: float or ndarray of floats
+            The :math:`D^2` score with a pinball deviance
+            or ndarray of scores if `multioutput='raw_values'`.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"d2_pinball_score_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def d2_pinball_score_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        score = metrics.d2_pinball_score(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            alpha=alpha,
+            multioutput=multioutput,
+        )
+
+        return cloudpickle.dumps(score)  # type: ignore[no-any-return]
+
+    score: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return score
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def explained_variance_score(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+    force_finite: bool = True,
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    Explained variance regression score function.
+
+    Best possible score is 1.0, lower values are worse.
+
+    In the particular case when ``y_true`` is constant, the explained variance
+    score is not finite: it is either ``NaN`` (perfect predictions) or
+    ``-Inf`` (imperfect predictions). To prevent such non-finite numbers to
+    pollute higher-level experiments such as a grid search cross-validation,
+    by default these cases are replaced with 1.0 (perfect predictions) or 0.0
+    (imperfect predictions) respectively. If ``force_finite``
+    is set to ``False``, this score falls back on the original :math:`R^2`
+    definition.
+
+    Note:
+       The Explained Variance score is similar to the
+       :func:`R^2 score <r2_score>`, with the notable difference that it
+       does not account for systematic offsets in the prediction. Most often
+       the :func:`R^2 score <r2_score>` should be preferred.
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        multioutput: {'raw_values', 'uniform_average', 'variance_weighted'} or \
+            array-like of shape (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of scores in case of multioutput input.
+            'uniform_average':
+                Scores of all outputs are averaged with uniform weight.
+            'variance_weighted':
+                Scores of all outputs are averaged, weighted by the variances
+                of each individual output.
+        force_finite: Flag indicating if ``NaN`` and ``-Inf`` scores resulting
+            from constant data should be replaced with real numbers (``1.0`` if
+            prediction is perfect, ``0.0`` otherwise). Default is ``True``, a
+            convenient setting for hyperparameters' search procedures (e.g. grid
+            search cross-validation).
+
+    Returns:
+        score: float or ndarray of floats
+            The explained variance or ndarray if 'multioutput' is 'raw_values'.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"explained_variance_score_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def explained_variance_score_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        score = metrics.explained_variance_score(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            multioutput=multioutput,
+            force_finite=force_finite,
+        )
+
+        return cloudpickle.dumps(score)  # type: ignore[no-any-return]
+
+    score: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return score
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def mean_absolute_error(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    Mean absolute error regression loss.
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        multioutput: {'raw_values', 'uniform_average'}  or array-like of shape \
+            (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of errors in case of multioutput input.
+            'uniform_average':
+                Errors of all outputs are averaged with uniform weight.
+
+    Returns:
+        loss: float or ndarray of floats
+            If multioutput is 'raw_values', then mean absolute error is returned
+            for each output separately.
+            If multioutput is 'uniform_average' or an ndarray of weights, then the
+            weighted average of all output errors is returned.
+
+            MAE output is non-negative floating point. The best value is 0.0.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"mean_absolute_error_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def mean_absolute_error_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        loss = metrics.mean_absolute_error(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            multioutput=multioutput,
+        )
+
+        return cloudpickle.dumps(loss)  # type: ignore[no-any-return]
+
+    loss: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return loss
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def mean_absolute_percentage_error(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    Mean absolute percentage error (MAPE) regression loss.
+
+    Note here that the output is not a percentage in the range [0, 100]
+    and a value of 100 does not mean 100% but 1e2. Furthermore, the output
+    can be arbitrarily high when `y_true` is small (which is specific to the
+    metric) or when `abs(y_true - y_pred)` is large (which is common for most
+    regression metrics).
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        multioutput: {'raw_values', 'uniform_average'}  or array-like of shape \
+            (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of errors in case of multioutput input.
+            'uniform_average':
+                Errors of all outputs are averaged with uniform weight.
+
+    Returns:
+        loss: float or ndarray of floats
+            If multioutput is 'raw_values', then mean absolute percentage error
+            is returned for each output separately.
+            If multioutput is 'uniform_average' or an ndarray of weights, then the
+            weighted average of all output errors is returned.
+
+            MAPE output is non-negative floating point. The best value is 0.0.
+            But note that bad predictions can lead to arbitrarily large
+            MAPE values, especially if some `y_true` values are very close to zero.
+            Note that we return a large value instead of `inf` when `y_true` is zero.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"mean_absolute_percentage_error_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def mean_absolute_percentage_error_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        loss = metrics.mean_absolute_percentage_error(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            multioutput=multioutput,
+        )
+
+        return cloudpickle.dumps(loss)  # type: ignore[no-any-return]
+
+    loss: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return loss
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def mean_squared_error(
+    *,
+    df: snowpark.DataFrame,
+    y_true_col_names: Union[str, List[str]],
+    y_pred_col_names: Union[str, List[str]],
+    sample_weight_col_name: Optional[str] = None,
+    multioutput: Union[str, npt.ArrayLike] = "uniform_average",
+    squared: bool = True,
+) -> Union[float, npt.NDArray[np.float_]]:
+    """
+    Mean squared error regression loss.
+
+    Args:
+        df: Input dataframe.
+        y_true_col_names: Column name(s) representing actual values.
+        y_pred_col_names: Column name(s) representing predicted values.
+        sample_weight_col_name: Column name representing sample weights.
+        multioutput: {'raw_values', 'uniform_average'}  or array-like of shape \
+            (n_outputs,), default='uniform_average'
+            Defines aggregating of multiple output values.
+            Array-like value defines weights used to average errors.
+            'raw_values':
+                Returns a full set of errors in case of multioutput input.
+            'uniform_average':
+                Errors of all outputs are averaged with uniform weight.
+        squared: If True returns MSE value, if False returns RMSE value.
+
+    Returns:
+        loss: float or ndarray of floats
+            A non-negative floating point value (the best value is 0.0), or an
+            array of floating point values, one for each individual target.
+    """
+    metrics_utils.check_label_columns(y_true_col_names, y_pred_col_names)
+
+    session = df._session
+    assert session is not None
+    sproc_name = f"mean_squared_error_{snowpark_utils.generate_random_alphanumeric()}"
+    sklearn_release = version.parse(sklearn.__version__).release
+    statement_params = telemetry.get_statement_params(_PROJECT, _SUBPROJECT)
+    cols = metrics_utils.flatten_cols([y_true_col_names, y_pred_col_names, sample_weight_col_name])
+    query = df[cols].queries["queries"][-1]
+
+    @F.sproc(  # type: ignore[misc]
+        session=session,
+        name=sproc_name,
+        replace=True,
+        packages=[
+            "cloudpickle",
+            f"scikit-learn=={sklearn_release[0]}.{sklearn_release[1]}.*",
+            "snowflake-snowpark-python",
+        ],
+        statement_params=statement_params,
+    )
+    def mean_squared_error_sproc(session: snowpark.Session) -> bytes:
+        df = session.sql(query).to_pandas(statement_params=statement_params)
+        y_true = df[y_true_col_names]
+        y_pred = df[y_pred_col_names]
+        sample_weight = df[sample_weight_col_name] if sample_weight_col_name else None
+
+        loss = metrics.mean_squared_error(
+            y_true,
+            y_pred,
+            sample_weight=sample_weight,
+            multioutput=multioutput,
+            squared=squared,
+        )
+
+        return cloudpickle.dumps(loss)  # type: ignore[no-any-return]
+
+    loss: Union[float, npt.NDArray[np.float_]] = cloudpickle.loads(
+        session.call(sproc_name, statement_params=statement_params)
+    )
+    return loss
+
+
+@telemetry.send_api_usage_telemetry(project=_PROJECT, subproject=_SUBPROJECT)
+def r2_score(*, df: snowpark.DataFrame, y_true_col_name: str, y_pred_col_name: str) -> float:
     """:math:`R^2` (coefficient of determination) regression score function.
     Returns R squared metric on 2 columns in the dataframe.
 
     Best possible score is 1.0 and it can be negative (because the
     model can be arbitrarily worse). In the general case when the true y is
     non-constant, a constant model that always predicts the average y
     disregarding the input features would get a :math:`R^2` score of 0.0.
 
     TODO(pdorairaj): Implement other params from sklearn - sample_weight, multi_output, force_finite.
 
     Args:
-        df (DataFrame): Input dataframe.
-        y_true_col_name (str): Column name representing actual values.
-        y_pred_col_name (str): Column name representing predicted values.
+        df: Input dataframe.
+        y_true_col_name: Column name representing actual values.
+        y_pred_col_name: Column name representing predicted values.
 
     Returns:
         R squared metric.
     """
 
     df_avg = df.select(F.avg(y_true_col_name).as_("avg_y_true"))  # type: ignore[arg-type]
     df_r_square = df.join(df_avg).select(
```

## snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py

```diff
@@ -738,34 +738,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/mixture/gaussian_mixture.py

```diff
@@ -711,34 +711,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/model_selection/grid_search_cv.py

```diff
@@ -747,34 +747,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/model_selection/randomized_search_cv.py

```diff
@@ -762,34 +762,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/multiclass/one_vs_one_classifier.py

```diff
@@ -621,34 +621,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py

```diff
@@ -630,34 +630,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/multiclass/output_code_classifier.py

```diff
@@ -633,34 +633,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/naive_bayes/bernoulli_nb.py

```diff
@@ -633,34 +633,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/naive_bayes/categorical_nb.py

```diff
@@ -639,34 +639,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/naive_bayes/complement_nb.py

```diff
@@ -633,34 +633,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/naive_bayes/gaussian_nb.py

```diff
@@ -614,34 +614,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/naive_bayes/multinomial_nb.py

```diff
@@ -627,34 +627,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/k_neighbors_classifier.py

```diff
@@ -684,34 +684,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/k_neighbors_regressor.py

```diff
@@ -686,34 +686,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/kernel_density.py

```diff
@@ -665,34 +665,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/local_outlier_factor.py

```diff
@@ -693,34 +693,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/nearest_centroid.py

```diff
@@ -624,34 +624,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/nearest_neighbors.py

```diff
@@ -676,34 +676,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py

```diff
@@ -695,34 +695,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py

```diff
@@ -696,34 +696,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py

```diff
@@ -686,34 +686,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neural_network/bernoulli_rbm.py

```diff
@@ -645,34 +645,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neural_network/mlp_classifier.py

```diff
@@ -798,34 +798,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/neural_network/mlp_regressor.py

```diff
@@ -794,34 +794,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/pipeline/pipeline.py

```diff
@@ -10,14 +10,15 @@
 from sklearn import __version__ as skversion, pipeline
 from sklearn.compose import ColumnTransformer
 from sklearn.preprocessing import FunctionTransformer
 from sklearn.utils import metaestimators
 
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
+from snowflake.ml.model.model_signature import ModelSignature, _infer_signature
 from snowflake.ml.modeling.framework import _utils, base
 
 _PROJECT = "ModelDevelopment"
 _SUBPROJECT = "Framework"
 
 
 def _final_step_has(attr: str) -> Callable[..., bool]:
@@ -99,14 +100,16 @@
         self._is_final_step_estimator = Pipeline._is_estimator(steps[-1][1])
         self._is_fitted = False
         self._feature_names_in: List[np.ndarray[Any, np.dtype[Any]]] = []
         self._n_features_in: List[int] = []
         self._transformers_to_input_indices: Dict[str, List[int]] = {}
         self._is_convertable_to_sklearn = True
 
+        self._model_signature_dict: Optional[Dict[str, ModelSignature]] = None
+
         deps: Set[str] = {f"pandas=={pd.__version__}", f"scikit-learn=={skversion}"}
         for _, obj in steps:
             if isinstance(obj, base.BaseTransformer):
                 deps = deps | set(obj._get_dependencies())
         self._deps = list(deps)
 
     @staticmethod
@@ -237,14 +240,15 @@
             all_cols = transformed_dataset.columns[:]
             estimator[1].fit(transformed_dataset)
 
             self._append_step_feature_consumption_info(
                 step_name=estimator[0], all_cols=all_cols, input_cols=estimator[1].get_input_cols()
             )
 
+        self._get_model_signatures(dataset=dataset)
         self._is_fitted = True
         return self
 
     @metaestimators.available_if(_final_step_has("transform"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
@@ -305,14 +309,15 @@
         if estimator:
             if has_callable_attr(estimator[1], "fit_transform"):
                 res: snowpark.DataFrame = estimator[1].fit_transform(transformed_dataset)
             else:
                 res = estimator[1].fit(transformed_dataset).transform(transformed_dataset)
             return res
 
+        self._get_model_signatures(dataset=dataset)
         self._is_fitted = True
         return transformed_dataset
 
     def _final_step_can_fit_predict(self) -> bool:
         return has_callable_attr(self.steps[-1][1], "fit_predict") or (
             has_callable_attr(self.steps[-1][1], "fit") and has_callable_attr(self.steps[-1][1], "predict")
         )
@@ -342,14 +347,15 @@
         estimator = self._get_estimator()
         if estimator:
             if has_callable_attr(estimator[1], "fit_predict"):
                 transformed_dataset = estimator[1].fit_predict(transformed_dataset)
             else:
                 transformed_dataset = estimator[1].fit(transformed_dataset).predict(transformed_dataset)
 
+        self._get_model_signatures(dataset=dataset)
         self._is_fitted = True
         return transformed_dataset
 
     @metaestimators.available_if(_final_step_has("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
@@ -555,7 +561,25 @@
             else:
                 sksteps.append(estimator_step)
 
         return pipeline.Pipeline(steps=sksteps)
 
     def _get_dependencies(self) -> List[str]:
         return self._deps
+
+    def _get_model_signatures(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> None:
+        self._model_signature_dict = dict()
+
+        input_columns = self._get_sanitized_list_of_columns(dataset.columns)
+        inputs_signature = _infer_signature(dataset[input_columns], "input")
+
+        estimator_step = self._get_estimator()
+        if estimator_step:
+            estimator_signatures = estimator_step[1].model_signatures
+            for method, signature in estimator_signatures.items():
+                self._model_signature_dict[method] = ModelSignature(inputs=inputs_signature, outputs=signature.outputs)
+
+    @property
+    def model_signatures(self) -> Dict[str, ModelSignature]:
+        if self._model_signature_dict is None:
+            raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
+        return self._model_signature_dict
```

## snowflake/ml/modeling/preprocessing/one_hot_encoder.py

```diff
@@ -796,15 +796,15 @@
             state_pandas = state_pandas.merge(split_pandas, on=[_COLUMN_NAME, _CATEGORY], how="left")
 
         # columns: COLUMN_NAME, CATEGORY, COUNT, FITTED_CATEGORY, ENCODING, N_FEATURES_OUT, ENCODED_VALUE, OUTPUT_CATs
         assert dataset._session is not None, "dataset._session cannot be None"
         state_df = dataset._session.create_dataframe(state_pandas)
 
         transformed_dataset = dataset
-        origional_dataset_columns = transformed_dataset.columns[:]
+        original_dataset_columns = transformed_dataset.columns[:]
         all_output_cols = []
         for input_col in self.input_cols:
             output_cols = [
                 identifier.quote_name_without_upper_casing(col) for col in self._dense_output_cols_mappings[input_col]
             ]
             all_output_cols += output_cols
             input_col_state_df = state_df.filter(F.col(_COLUMN_NAME) == input_col)[output_cols + [_CATEGORY]]
@@ -814,15 +814,15 @@
                 input_col_state_df,
                 on=transformed_dataset[input_col].equal_null(input_col_state_df[_CATEGORY]),
                 how="left",
             )[transformed_dataset.columns + output_cols]
 
         transformed_dataset = self._handle_unknown_in_transform(transformed_dataset)
         # Reorder columns. Passthrough columns are added at the right to the output of the transformers.
-        transformed_dataset = transformed_dataset[all_output_cols + origional_dataset_columns]
+        transformed_dataset = transformed_dataset[all_output_cols + original_dataset_columns]
         return transformed_dataset
 
     def _transform_snowpark_sparse_udf(self, dataset: snowpark.DataFrame) -> snowpark.DataFrame:
         """
         Transform Snowpark dataframe using one-hot encoding when
         `self.sparse=True`. Return the sparse representation where
         the transformed output is {column_index: 1.0} for each value
@@ -891,23 +891,22 @@
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
         """
         encoder_sklearn = self.to_sklearn()
-
         transformed_dataset = encoder_sklearn.transform(dataset[self.input_cols])
 
-        if not self.sparse:
-            dataset = dataset.copy()
-            dataset[self.get_output_cols()] = transformed_dataset
-            return dataset
+        if self.sparse:
+            return transformed_dataset
 
-        return transformed_dataset
+        dataset = dataset.copy()
+        dataset[self.get_output_cols()] = transformed_dataset
+        return dataset
 
     def _create_unfitted_sklearn_object(self) -> preprocessing.OneHotEncoder:
         sklearn_args = self.get_sklearn_args(
             default_sklearn_obj=preprocessing.OneHotEncoder(),
             sklearn_initial_keywords=_SKLEARN_INITIAL_KEYWORDS,
             sklearn_unused_keywords=_SKLEARN_UNUSED_KEYWORDS,
             snowml_only_keywords=_SNOWML_ONLY_KEYWORDS,
@@ -1327,25 +1326,25 @@
         """
         Output columns getter.
 
         Returns:
             Output columns.
         """
         if self.sparse:
-            output_cols = self.output_cols
-        else:
-            output_cols = (
-                [
-                    identifier.quote_name_without_upper_casing(col)
-                    for input_col in self.input_cols
-                    for col in self._dense_output_cols_mappings[input_col]
-                ]
-                if self._dense_output_cols_mappings
-                else []
-            )
+            return self.output_cols
+
+        output_cols = (
+            [
+                identifier.get_inferred_name(col)
+                for input_col in self.input_cols
+                for col in self._dense_output_cols_mappings[input_col]
+            ]
+            if self._dense_output_cols_mappings
+            else []
+        )
         return output_cols
 
     def _get_dense_output_cols_mappings(self) -> None:
         """
         Get input column to dense output columns mappings and assign them to
         `self._dense_output_cols_mappings`.
         """
```

## snowflake/ml/modeling/preprocessing/polynomial_features.py

```diff
@@ -635,34 +635,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/semi_supervised/label_propagation.py

```diff
@@ -639,34 +639,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/semi_supervised/label_spreading.py

```diff
@@ -648,34 +648,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/linear_svc.py

```diff
@@ -699,34 +699,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/linear_svr.py

```diff
@@ -672,34 +672,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/nu_svc.py

```diff
@@ -710,34 +710,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/nu_svr.py

```diff
@@ -671,34 +671,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/svc.py

```diff
@@ -713,34 +713,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/svm/svr.py

```diff
@@ -674,34 +674,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/tree/decision_tree_classifier.py

```diff
@@ -742,34 +742,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/tree/decision_tree_regressor.py

```diff
@@ -724,34 +724,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/tree/extra_tree_classifier.py

```diff
@@ -734,34 +734,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/tree/extra_tree_regressor.py

```diff
@@ -716,34 +716,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/xgboost/xgb_classifier.py

```diff
@@ -816,34 +816,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/xgboost/xgb_regressor.py

```diff
@@ -815,34 +815,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/xgboost/xgbrf_classifier.py

```diff
@@ -820,34 +820,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/modeling/xgboost/xgbrf_regressor.py

```diff
@@ -820,34 +820,45 @@
         output_cols = expected_output_cols_list.copy()
 
         # Model expects exact same columns names in the input df for predict call.
         # Given the scenario that user use snowpark DataFrame in fit call, but pandas DataFrame in predict call
         # input cols need to match unquoted / quoted
         input_cols = self.input_cols
         unquoted_input_cols = identifier.get_unescaped_names(self.input_cols)
+        quoted_input_cols = identifier.get_escaped_names(unquoted_input_cols)
 
         estimator = self._sklearn_object
 
-        input_df = dataset[input_cols] # Select input columns with quoted column names.
-        if hasattr(estimator, "feature_names_in_"):
-            missing_features = []
-            for i, f in enumerate(getattr(estimator, "feature_names_in_")):
-                if i >= len(input_cols) or (input_cols[i] != f and unquoted_input_cols[i] != f):
-                    missing_features.append(f)
-
-            if len(missing_features) > 0:
-                raise ValueError(
-                    "The feature names should match with those that were passed during fit.\n"
-                    f"Features seen during fit call but not present in the input: {missing_features}\n"
-                    f"Features in the input dataframe : {input_cols}\n"
-                )
-            input_df.columns = getattr(estimator, "feature_names_in_")
-        else:
-            # Just rename the column names to unquoted identifiers.
-            input_df.columns = unquoted_input_cols # Replace the quoted columns identifier with unquoted column ids.
+        features_required_by_estimator =  getattr(estimator, "feature_names_in_") if hasattr(estimator, "feature_names_in_") else unquoted_input_cols
+        missing_features = []
+        features_in_dataset = set(dataset.columns)
+        columns_to_select = []
+        for i, f in enumerate(features_required_by_estimator):
+            if (
+                    i >= len(input_cols)
+                or (input_cols[i] != f and unquoted_input_cols[i] != f and quoted_input_cols[i] != f)
+                or (input_cols[i] not in features_in_dataset and unquoted_input_cols[i] not in features_in_dataset 
+                    and quoted_input_cols[i] not in features_in_dataset)
+                ):
+                missing_features.append(f)
+            elif input_cols[i] in features_in_dataset:
+                columns_to_select.append(input_cols[i])
+            elif unquoted_input_cols[i] in features_in_dataset:
+                columns_to_select.append(unquoted_input_cols[i])
+            else:
+                columns_to_select.append(quoted_input_cols[i])
+
+        if len(missing_features) > 0:
+            raise ValueError(
+                "The feature names should match with those that were passed during fit.\n"
+                f"Features seen during fit call but not present in the input: {missing_features}\n"
+                f"Features in the input dataframe : {input_cols}\n"
+            )
+        input_df = dataset[columns_to_select]
+        input_df.columns = features_required_by_estimator
 
         transformed_numpy_array = getattr(estimator, inference_method)(
             input_df
         )
 
         if (
             isinstance(transformed_numpy_array, list)
```

## snowflake/ml/registry/model_registry.py

```diff
@@ -4,15 +4,14 @@
 import sys
 import tempfile
 import types
 import zipfile
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, cast
 from uuid import uuid1
 
-import cloudpickle as cp
 from absl import logging
 
 from snowflake import connector, snowpark
 from snowflake.ml._internal import file_utils, telemetry
 from snowflake.ml._internal.utils import (
     formatting,
     identifier,
@@ -1449,14 +1448,16 @@
         model: Any,
         description: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         conda_dependencies: Optional[List[str]] = None,
         pip_requirements: Optional[List[str]] = None,
         signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
         sample_input_data: Optional[Any] = None,
+        code_paths: Optional[List[str]] = None,
+        options: Optional[model_types.ModelSaveOption] = None,
     ) -> str:
         """Uploads and register a model to the Model Registry.
 
         Args:
             model_name: The given name for the model. The combination (name + version) must be unique for each model.
             model_version: Version string to be set for the model. The combination (name + version) must be unique for
                 each model.
@@ -1468,14 +1469,16 @@
                 not specified, defaults channel will be used. When deploying to Snowflake Warehouse, defaults channel
                 would be replaced with the Snowflake Anaconda channel.
             pip_requirements: List of PIP package specs. Model will not be able to deploy to the warehouse if there is
                 pip requirements.
             signatures: Signatures of the model, which is a mapping from target method name to signatures of input and
                 output, which could be inferred by calling `infer_signature` method with sample input data.
             sample_input_data: Sample of the input data for the model.
+            code_paths: Directory of code to import when loading and deploying the model.
+            options: Additional options when saving the model.
 
         Raises:
             TypeError: Raised when both signatures and sample_input_data is not presented. Will be captured locally.
             DataError: Raised when the given model exists.
 
         Returns:
             String of the auto-generate unique model identifier. None if failed.
@@ -1486,68 +1489,58 @@
         self._model_identifier_is_nonempty_or_raise(model_name, model_version)
 
         existing_model_nums = self._list_selected_models(model_name=model_name, model_version=model_version).count()
         if existing_model_nums:
             raise connector.DataError(f"Model {model_name}/{model_version} already exists. Unable to log the model.")
         with tempfile.TemporaryDirectory() as tmpdir:
             model = cast(model_types.SupportedModelType, model)
-            try:
-                if signatures:
-                    model_api.save_model(
-                        name=model_name,
-                        model_dir_path=tmpdir,
-                        model=model,
-                        signatures=signatures,
-                        metadata=tags,
-                        conda_dependencies=conda_dependencies,
-                        pip_requirements=pip_requirements,
-                    )
-                elif sample_input_data is not None:
-                    model_api.save_model(
-                        name=model_name,
-                        model_dir_path=tmpdir,
-                        model=model,
-                        metadata=tags,
-                        conda_dependencies=conda_dependencies,
-                        pip_requirements=pip_requirements,
-                        sample_input=sample_input_data,
-                    )
-                elif isinstance(model, base.BaseEstimator):
-                    model_api.save_model(
-                        name=model_name,
-                        model_dir_path=tmpdir,
-                        model=model,
-                        metadata=tags,
-                        conda_dependencies=conda_dependencies,
-                        pip_requirements=pip_requirements,
-                    )
-                else:
-                    raise TypeError("Either signature or sample input data should exist for native model packaging.")
-                return self._log_model_path(
-                    model_name=model_name,
-                    model_version=model_version,
-                    path=tmpdir,
-                    type="snowflake_native",
-                    description=description,
-                    tags=tags,  # TODO: Inherent model type enum.
+            if signatures:
+                model_metadata = model_api.save_model(
+                    name=model_name,
+                    model_dir_path=tmpdir,
+                    model=model,
+                    signatures=signatures,
+                    metadata=tags,
+                    conda_dependencies=conda_dependencies,
+                    pip_requirements=pip_requirements,
+                    code_paths=code_paths,
+                    options=options,
                 )
-            except (AttributeError, TypeError):
-                pass
-
-        with tempfile.NamedTemporaryFile(delete=True) as local_model_file:
-            cp.dump(model, local_model_file)
-            local_model_file.flush()
-
+            elif sample_input_data is not None:
+                model_metadata = model_api.save_model(
+                    name=model_name,
+                    model_dir_path=tmpdir,
+                    model=model,
+                    metadata=tags,
+                    conda_dependencies=conda_dependencies,
+                    pip_requirements=pip_requirements,
+                    sample_input=sample_input_data,
+                    code_paths=code_paths,
+                    options=options,
+                )
+            elif isinstance(model, base.BaseEstimator):
+                model_metadata = model_api.save_model(
+                    name=model_name,
+                    model_dir_path=tmpdir,
+                    model=model,
+                    metadata=tags,
+                    conda_dependencies=conda_dependencies,
+                    pip_requirements=pip_requirements,
+                    code_paths=code_paths,
+                    options=options,
+                )
+            else:
+                raise TypeError("Either signature or sample input data should exist for native model packaging.")
             return self._log_model_path(
                 model_name=model_name,
                 model_version=model_version,
-                path=local_model_file.name,
-                type=model.__class__.__name__,
+                path=tmpdir,
+                type=model_metadata.model_type,
                 description=description,
-                tags=tags,
+                tags=tags,  # TODO: Inherent model type enum.
             )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
@@ -1561,30 +1554,21 @@
         Returns:
             Restored model object.
         """
         remote_model_path = self._get_model_path(model_name=model_name, model_version=model_version)
         restored_model = None
         with tempfile.TemporaryDirectory() as local_model_directory:
             self._session.file.get(remote_model_path, local_model_directory)
-            is_native_model_format = False
             local_path = os.path.join(local_model_directory, os.path.basename(remote_model_path))
-            try:
-                if zipfile.is_zipfile(local_path):
-                    extracted_dir = os.path.join(local_model_directory, "extracted")
-                    with zipfile.ZipFile(local_path, "r") as myzip:
-                        if len(myzip.namelist()) > 1:
-                            myzip.extractall(extracted_dir)
-                            restored_model, _meta = model_api.load_model(model_dir_path=extracted_dir)
-                            is_native_model_format = True
-            except TypeError:
-                pass
-            if not is_native_model_format:
-                file_path = os.path.join(local_model_directory, os.path.basename(os.path.basename(remote_model_path)))
-                with open(file_path, mode="r+b") as model_file:
-                    restored_model = cp.load(model_file)
+            if zipfile.is_zipfile(local_path):
+                extracted_dir = os.path.join(local_model_directory, "extracted")
+                with zipfile.ZipFile(local_path, "r") as myzip:
+                    if len(myzip.namelist()) > 1:
+                        myzip.extractall(extracted_dir)
+                        restored_model, _ = model_api.load_model(model_dir_path=extracted_dir)
 
         return restored_model
 
     # Repository Operations
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
```

## snowflake/ml/version.py

```diff
@@ -1 +1 @@
-VERSION="1.0.1"
+VERSION="1.0.2"
```

## Comparing `snowflake_ml_python-1.0.1.dist-info/METADATA` & `snowflake_ml_python-1.0.2.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: snowflake-ml-python
-Version: 1.0.1
+Version: 1.0.2
 Description-Content-Type: text/markdown
 Author: Snowflake, Inc
 Author-email: support@snowflake.com
 Home-page: https://github.com/snowflakedb/snowflake-ml-python
 License: Apache License, Version 2.0
 Classifier: Development Status :: 3 - Alpha
 Classifier: Environment :: Console
@@ -92,14 +92,37 @@
 
 ### Install the library to the Python virtual environment
 ```
 pip install snowflake-ml-python
 ```
 # Release History
 
+## 1.0.2 (2023-06-22)
+
+### Behavior Changes
+- Model Registry: Prohibit non-snowflake-native models from being logged.
+- Model Registry: `_use_local_snowml` parameter in options of `deploy()` has been removed.
+- Model Registry: A default `False` `embed_local_ml_library` parameter has been added to the options of `log_model()`. With this set to `False` (default), the version of the local snowflake-ml-python library will be recorded and used when deploying the model. With this set to `True`, local snowflake-ml-python library will be embedded into the logged model, and will be used when you load or deploy the model.
+
+### New Features
+- Model Registry: A new optional argument named `code_paths` has been added to the arguments of `log_model()` for users to specify additional code paths to be imported when loading and deploying the model.
+- Model Registry: A new optional argument named `options` has been added to the arguments of `log_model()` to specify any additional options when saving the model.
+- Model Development: Added metrics:
+  - d2_absolute_error_score
+  - d2_pinball_score
+  - explained_variance_score
+  - mean_absolute_error
+  - mean_absolute_percentage_error
+  - mean_squared_error
+
+### Bug Fixes
+
+- Model Development: `accuracy_score()` now works when given label column names are lists of a single value.
+
+
 ## 1.0.1 (2023-06-16)
 ### Behavior Changes
 
 - Model Development: Changed Metrics APIs to imitate sklearn metrics modules:
   - `accuracy_score()`, `confusion_matrix()`, `precision_recall_fscore_support()`, `precision_score()` methods move from respective modules to `metrics.classification`.
 - Model Registry: The dafault table/stage created by the Registry now uses "_SYSTEM_" as a prefix.
 - Model Registry: `get_model_history()` method as been enhanced to include the history of model deployment.
```

## Comparing `snowflake_ml_python-1.0.1.dist-info/RECORD` & `snowflake_ml_python-1.0.2.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,246 +1,246 @@
 snowflake/ml/_internal/env.py,sha256=kCrJTRnqQ97VGUVI1cWUPD8HuBWeL5vOOtwUR0NB9Mg,161
 snowflake/ml/_internal/env_utils.py,sha256=tL-5IswRvbcuAZHvi1tIgGuuwg6_I0losgJSdAjfZPQ,14126
-snowflake/ml/_internal/file_utils.py,sha256=y69XEpwGGI1vZwlcnhiLp37T3yPyG_pCK0Nhj1v0a_I,7535
+snowflake/ml/_internal/file_utils.py,sha256=ue1mqkjz2sxipycEfLAxkYEX34SwHJKbnkEjWgSd4c0,6353
 snowflake/ml/_internal/init_utils.py,sha256=U-oPOtyVf22hCwDH_CH2uDr9yuN6Mr3kwQ_yRAs1mcM,2696
 snowflake/ml/_internal/telemetry.py,sha256=CPcC6ZBbIVVkX6Ny3f4-EZ8s3A7O9u_S85H-qxJ6X4M,20238
 snowflake/ml/_internal/type_utils.py,sha256=0AjimiQoAPHGnpLV_zCR6vlMR5lJ8CkZkKFwiUHYDCo,2168
 snowflake/ml/_internal/utils/formatting.py,sha256=pz3dFq11BzeHVcZugrU5lQOmPeBKmfkggEsTnDm8ggw,3678
-snowflake/ml/_internal/utils/identifier.py,sha256=sV_E4qYN1G-lyTX_H_s3ABHtIzq7nHJUmvTY1byPDY4,5358
+snowflake/ml/_internal/utils/identifier.py,sha256=zA2Eoc_p8u4kphGuVUbaYt1Fl6xSTjIYu6Qu8BrDZ1c,7703
 snowflake/ml/_internal/utils/import_utils.py,sha256=eexwIe7auT17s4aVxAns7se0_K15rcq3O17MkIvDpPI,2068
 snowflake/ml/_internal/utils/parallelize.py,sha256=zYtkYBq2_N7R49AvSzJynmvixNhUw3YBBZQ3uxVtTEA,4550
 snowflake/ml/_internal/utils/pkg_version_utils.py,sha256=AMR97AZCOr26Je2Q4fIePJRMf7cASr910R5-wr7ANpM,3722
 snowflake/ml/_internal/utils/query_result_checker.py,sha256=IrzUJ4fJvxjJ5ma-6mejWHpxoEtwnMKo9XTJ-YsECnk,12205
 snowflake/ml/_internal/utils/temp_file_utils.py,sha256=77k4ZAZJfyJBMw0IOfn4aItW2mUFGIl_3RgCNS_U4f4,1400
 snowflake/ml/_internal/utils/uri.py,sha256=wi5LTs306Prcs8tL1CR19b2nUto8U2FLlOyVQrUQcn0,1841
 snowflake/ml/fileset/fileset.py,sha256=hwKtNENBiNpEeHKyNra2QM11TYklzjyB_PtIQ8x5r_g,26746
 snowflake/ml/fileset/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
 snowflake/ml/fileset/parquet_parser.py,sha256=yTJdYFTzaTPsgb1rGMj_jv_wDjmuwJZzbVRRmk--yA8,5915
 snowflake/ml/fileset/sfcfs.py,sha256=YWL2D8P-3KcSoGmz6_nvMjQgRNTKzXbwGRhIZYYVZQo,11536
 snowflake/ml/fileset/stage_fs.py,sha256=deFiXBXqab_v2WG6-A0BaepWvNxh4afpDsGbYh0jNWA,14859
 snowflake/ml/fileset/tf_dataset.py,sha256=MrFtGiFu1FX3MSjAjWnZcEa5Ow4fsAHlUXW-BLqFWus,3462
 snowflake/ml/fileset/torch_datapipe.py,sha256=kjfUmAqEQ55Gd1nMUFP-3crp1XG46oJ4E74Euk4HEW8,2386
-snowflake/ml/model/_core_requirements.py,sha256=cQ0hiZzfNRKk-h2BFLeTUdYfT79wLGTM1SCFpm2X_iA,223
-snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=vwnty3j78w3diMyzzpOpnCq-4SZIzvE5bWK64ulAI_Y,8959
-snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=kFGKZbp_aTyumznxq9WRIYeR7_yev5NLiWtZC5cfcwI,2292
+snowflake/ml/model/_core_requirements.py,sha256=6HGtzvyZVGSIMYkJQ-J4TSyWwPt69uXnPXj7A4Nm34Q,197
+snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=AUv7H3qQVCkaevgEMENugBYW-_eL1r21vnleM7UezbQ,7962
+snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=qaGEbWhJCpdLse0KGw6kIS6gGD8iSA4j4By1wc-Lh2Y,2369
 snowflake/ml/model/_deployer.py,sha256=c08kn3R6krNV0RaPGhFjQJAWxJ1zsM3kFMJ7VQ0O4OI,9548
 snowflake/ml/model/_env.py,sha256=7vJHt77WusrMDDeKSRTyE-X9P1QICg-q68fxSx8scvg,4488
-snowflake/ml/model/_handlers/_base.py,sha256=f8V8uPwqVko4h9tkn6vBkgfGYL0CP38GN8WQXJZq-mw,2189
+snowflake/ml/model/_handlers/_base.py,sha256=JUPnwTCGgMkKzqVns2zeVCF4-MtxnVKDieqNZR1X8sc,2299
 snowflake/ml/model/_handlers/custom.py,sha256=Hjf_bg6LxhQWctkg6h35Knnu7-FHo2HWZLrPHRsEtWM,6084
-snowflake/ml/model/_handlers/sklearn.py,sha256=nnbDE8EpbRieg_f1FL-zay837wmpUkbyOxX2IVfIIPE,7484
+snowflake/ml/model/_handlers/sklearn.py,sha256=OrNHd6_k7l8AbqpUCKcVeK1-ypwQUybDjYQr6IYtmBc,7558
 snowflake/ml/model/_handlers/snowmlmodel.py,sha256=P35oabm3ERwGjnrREVi35a1JS1o9wdTzFJLThHt_uT8,7711
-snowflake/ml/model/_handlers/xgboost.py,sha256=WyrPHmErMDhW4C1haa3Vf9ftDV6L-LvkfrILbzk1lRw,7170
-snowflake/ml/model/_model.py,sha256=8ttJ40S5Vswyymx6SvpblQA9YUqe6EXyA426ZroWBZ8,26376
+snowflake/ml/model/_handlers/xgboost.py,sha256=8WLW_tKDB7t0AjFCy8DzpCat7ojRK61h0AMFKRF0mlg,7204
+snowflake/ml/model/_model.py,sha256=wBcwYjjmTlGhJcOilndqeZALsqfqR3cU30fF7ciTDm4,26448
 snowflake/ml/model/_model_handler.py,sha256=a1upCULZlNuxUiFoJbK85nERGkA2VkEsn5-IIZn7pro,2101
-snowflake/ml/model/_model_meta.py,sha256=hMXJWqWutBZ-QyIWREcFQeogMryrziRr-Zr86ji0SxA,16992
+snowflake/ml/model/_model_meta.py,sha256=FRhp90-SxVcE-_FxNZ39M_Bqycyu5h_LiNoMb61Ia_8,17684
 snowflake/ml/model/custom_model.py,sha256=8qEHi8myHcp02jcpFbG9Kqscn9YRv3QnzehCrTSI8ds,8016
 snowflake/ml/model/model_signature.py,sha256=Q_n1mcetW5btVYCS4VWMef29TshctoZSPC8Gk3Xqv2U,43624
-snowflake/ml/model/type_hints.py,sha256=-ZkPHuXKb_0v7AEcLMnmvbHFITrnSz2xxKFIbWln6CQ,4698
+snowflake/ml/model/type_hints.py,sha256=Vlpk52yXo2WcBKVdhoJM0gjnj20Tr6vwb3AOM3n35g4,4405
 snowflake/ml/modeling/calibration/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=rSuydzSthj5Am6vZNLWUGqjOe9SyeYlvxeVO3QKAoFE,53589
+snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=Fh6Yq3jvpDvnQvtN9UPPo6c1p8266OwqQ77aT5ZhQGo,54140
 snowflake/ml/modeling/cluster/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=S_0JgrVVO9TY9CjhNMcfDkBegbC9fwmtHOG5rFa-q18,51516
-snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=f5zah7n4GYbhKMPk6D8clHLzcBX4gxfPGAp2XVoP20U,53529
-snowflake/ml/modeling/cluster/birch.py,sha256=9m4D_x7kaE9fCirrM4LNi1an_QEuHGXM70PjVvqGJhg,51354
-snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=RWMJBqWxaGvdHVXRJGiaoLd5o0T6pb1kHvyfJrT9hhI,53736
-snowflake/ml/modeling/cluster/dbscan.py,sha256=GlVERP86VAwVce2cscy9gMxBYgXFkF9WtFTjksp3mZQ,51695
-snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=jPGZwz64gj42HJtanVk80kP8ukDtK9ElNpJ9N2Vgw4Y,54069
-snowflake/ml/modeling/cluster/k_means.py,sha256=DpSU-dO5AQF5YD6O9YYrUz2Mxwjw3qbQsUYWacHAcsc,53323
-snowflake/ml/modeling/cluster/mean_shift.py,sha256=ambjEjhmCCV2eHVguSBy62Ee60RxtYwllQ3Ohf8RSSA,51897
-snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=3QouCN6ZKFwt2KS1fAOUiR_mTMVf4E9Av4TYq7bA3VE,54598
-snowflake/ml/modeling/cluster/optics.py,sha256=7LL9YLuX7lSNyPtNjnG6nub7FIhE7gYlwiM6RaVNpiQ,55029
-snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=6mK-gqeTNNfmX1l0o694BNScbheFa2OiRRz8yEmK0pY,52087
-snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=a3fWLymIHgF9ZOvaW-gj4yPtj2JlOD7FufYbvCm0Hdk,55025
-snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=Biopk-f0hyW4bklNM5S6TRmmnb3dv3GRxsT935_Kxso,51217
+snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=2Yco1Od7Yy0Av_4DW84VFJLs96rPJCy8xz8CMEH_O4A,52067
+snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=-U-ZBqQpURTwAT45rTECN-udcTRdq9iWAHyla3ZRgxo,54080
+snowflake/ml/modeling/cluster/birch.py,sha256=_vm0DzphdPTij-tnBNszMhjO2ryIcWTKYWU_NdS0tUE,51905
+snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=Ae2AOU1qp2J0BszoKH3OC13ua9ut3xc4DdJ_DjFNf9A,54287
+snowflake/ml/modeling/cluster/dbscan.py,sha256=n5lG9ZKq4wEnD-4-HQDFaXj7-_lk9yOAPJPEEHbLawY,52246
+snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=tK0owQ9esTy_VVSuybejL5gEsGofar70CIVrE5GI5lk,54620
+snowflake/ml/modeling/cluster/k_means.py,sha256=Z0QFBDwU-aHt-ua8Cs91revqgFvMgg5tFMh3--lKpXg,53874
+snowflake/ml/modeling/cluster/mean_shift.py,sha256=VIe24mRUvAk35fNISI0JBYpRjIcmsv6XjRr4uvzq8Ic,52448
+snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=-FNLzyqaYZmQ6Hh5dtL7_CW_9G9MH-0ZHjrwsPIphVc,55149
+snowflake/ml/modeling/cluster/optics.py,sha256=c97yzv0KWqoOJyqk4RLggsdTqBfDl_M6WLbAMd7Wuak,55580
+snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=9-9qPdHANEnO7bwIm50M-Y_veYjmWXhrP63k9417Nao,52638
+snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=moI4NNxtXhSFze2pAmlwPVGxMbylrFJkL9w2BChFbpw,55576
+snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=vtJmPSB5RmPhBQgXWf2FqviSWsrVT9X06FMeFLbN1IY,51768
 snowflake/ml/modeling/compose/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/compose/column_transformer.py,sha256=1ho7HRAFcnmI9JYmpqVvmGOAyn0crSd0vJOVf9c4iqc,53800
-snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=VPaAQYF9aKXRct3ND5VIF2jN12fHnBNQC8JcEyJo5jc,51385
+snowflake/ml/modeling/compose/column_transformer.py,sha256=6LdwUi3XcdikyqmZhQo8Q7NIAIzGg_pu2AXe77DAGh0,54351
+snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=fq48CubtxA_SbKtlxvF1ksdTIJAta3ArzaYLPWPkmok,51936
 snowflake/ml/modeling/covariance/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=o5DNXmVgfHLarDmHbrVUW-t4vW_YwKkTrvKqj2f7rLE,51357
-snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=L8vfUoeaqK9RnNBe6lp2MDqozkX3GRynPOjXADeUzj4,49633
-snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=nRV_TWhvaiv9vsI0dCDnhoEUcbI2daZqfw78RJsg2Cc,50907
-snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=skNA0mVG2YuROZPvMfRyuzELCX0oZ2bhMddI-TDjUAQ,52371
-snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=xoOFic2TH9Oso5akgMAM39cnotzhpDpwfM7-hOBtOqs,49835
-snowflake/ml/modeling/covariance/min_cov_det.py,sha256=_hR6evVYLEW4drbqXCbJE1c2G-Q1rhAib8iOxY8cjRQ,50598
-snowflake/ml/modeling/covariance/oas.py,sha256=vXs2qJBO4DI1kLQ8xrWjelSA_G-PdzqeSTu1qn3XrHE,49524
-snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=L60jrpfgypCiCL5PSyHf4FujszUT3FSBvRtDNlH5IN4,49810
+snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=kbJ0rgwMD-FBtGox7MG3ZaGkVzqOU9GGa7h1otgsgJg,51908
+snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=RMAb0G8lhSfHDkXP3lfzWFyLMJbGT9RXs99hINq2n8Y,50184
+snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=ruc4U-p-0vyC3m_2DGoXwbm8RJnnysVucv7TiLY1wAU,51458
+snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=Z8os4_OgmL-7SEad1JVbU_l_uQTPN2etjFqTTF6o-Ko,52922
+snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=Lvld0ax1YowkWBiDQs8Jmn6vfwevg4ciM_r-DaAFPEA,50386
+snowflake/ml/modeling/covariance/min_cov_det.py,sha256=iHBPAUBZOJMrFwuJMbpG-D6aBfZknWKz2IqJZ0xBJNU,51149
+snowflake/ml/modeling/covariance/oas.py,sha256=7-KZ1KSN4Y4Gt_FAg03XE9vH6LC3gBnK2yt75PfKLRE,50075
+snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=l9afcrkEqgM2zbZuv8-Wdu_LwK6eHCxiteOyzfzFfys,50361
 snowflake/ml/modeling/decomposition/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=o_xej8vC3YfXoAWLz7wZq6r9GZ6vQdmSi01EE1JaIKg,54625
-snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=vCahfyRIkm7HI-jTbF63aYlNhyZgznTpwz0tCPmamPE,51997
-snowflake/ml/modeling/decomposition/fast_ica.py,sha256=jTfdav8omBRqDMx1uL_CHdpP09NlE8ovJ8k3j7TwuV8,52459
-snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=_Pvp0CWFRMSCdpHBsicgQdBJNzcjQDGl3Kin3Ol5fGs,50794
-snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=XY0fPRm9HIqg-z1YPFsg3vPEBZ-n0iTbCPUD088HwSs,54825
-snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=uHAc3BCYvpBid-fmFeQgGc7qZ8aHOwS17zocg6KyphU,55808
-snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=sR50mqcRPg8NCoVAN3HjdImXMSN9WX7GpcDosKHWQaI,53125
-snowflake/ml/modeling/decomposition/pca.py,sha256=YqVI7VfaB6lopOaEQDsNOOULtxenkgDkQDWkpg9pPr0,53669
-snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=TNw287AwitPtIX502qIr6HdDBId4Lz8GPqony40Mn_k,51990
-snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=decwb2Tn4oBlK6HJN13vpPzX60dO8fEjcEI1cWkr5Mk,51562
+snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=TgyT361-kmtHHJ015svz_d_NsOShWtLT_Hqa2oSgsuE,55176
+snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=TTlDQkaAUo2bWwZM-5UmH8zWltGPVj8ZWHAb3LRlLks,52548
+snowflake/ml/modeling/decomposition/fast_ica.py,sha256=T9_bqJwvIe-HWebGkDwHhBAKip13QlZS8zp_H3wTJ_Y,53010
+snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=m3qmci-Q8tu6BDBybHt5gTGc91oa4tzKYieuTUH5VqA,51345
+snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=7R008lOnVyiOOE_vGBSZfSkYPnHBPeJ_IEZy2yqo8_8,55376
+snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=sdGzUim1jbwQoMbqgmK4Yt2ujnrSNovPcmwMRr5kUuY,56359
+snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=S0ypJAaODsN7QIpoxaITjLt8PujLR7duy6JRw2b-kq4,53676
+snowflake/ml/modeling/decomposition/pca.py,sha256=zDw-12Xl0RVPlv1DRNej1wVZHVg4UbULL1-59XJI2JM,54220
+snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=yt9_-fHwmJly0bmrP4zF99nAFXNdOghC1tvlqe0uQ-s,52541
+snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=gyPfuP13xKVqxWxVRmfSPBsByDVip-TUmYX3JsZA82U,52113
 snowflake/ml/modeling/discriminant_analysis/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=ZObd4DKs3iKx3wY0_ReCPsjtTJVOV1c5Ha_kCuYbHXk,53810
-snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=LcLUZr7PEuZLJ_0I_iNQ1sCxImDOdC3c3spVWyub_n0,51875
+snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=pFO9WhFbvZxtNLT-qwMu25gFOwHUlqSzXk2SUWiYFbQ,54361
+snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=dYkvp3HMhJA5BuhkLMbNCm-QqPspZwQAUfZdP1appaU,52426
 snowflake/ml/modeling/ensemble/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=hdAoWWAS_Ud0fftyVrCFNryZ_ITUT7PZjTSy-b1AK70,52828
-snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=KOD5LtcTOdAsRYCdkJexComyx7r5sybeUPcZ7Q40x_E,51727
-snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=rinV_okW-9VYqW-Zf8LLw8cSGYdYUSUgTWhLxrLVZAQ,53752
-snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=beubMYXp2wFQW_1MIkiA9j-6cJYQWaeG6mOci_RnW3s,52996
-snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=j6Gge1zaYbzCVUa-8RXIn1iUxOmaUk-7dagrKCssRaM,58538
-snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=XbJyhzeYUV__huUtmZkd9qFPNSoOA_QsA7ypgpP-RZE,57149
-snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=jxeGLs61h0hHOgUjC7LykwHmnEWWyS-hTNkhmiZ8y-I,60147
-snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=tYE2Z7NmuA1LkvftHfi7KMeuKHUh66p-7h5mIAfc98s,59731
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=PVClNfIodzL1tUGMaW-bemOu8svJW2KKFjmK0Ag9r5Y,59797
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=LcojSXaMd-QAjcLW8Wh6_XQO9pDxToQSE3kAy8Z-SDA,58119
-snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=ANxeR-5QzTry4Xn2GsAjn3IvdGSTy561KZHBJ7DTk1w,52773
-snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=aoVqyJp10b7fWXVHS5OTMwLv--w2s7GtCcbaa0WUdWY,58493
-snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=8Fy6qDn56DErIyBEZFyWZKAdf0i4znICsj8Mb3GgIYg,57092
-snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=wBNwc3ifQG8pzwp7CvF74vaTPCm9ExkuPzBzcgbziOs,52680
-snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=VqyEnIoSnfIb3DJPfcCNxcLEFEhClYdUfs4mFgifGVA,52255
-snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=1ct1H15p-WWAuQ_aZCngs4tSXFFtIA7SCPyP4NNBr70,50790
+snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=wI-ZlS9RxmrbdrlVhnOysyNi_HI6F3lJ0l4CXCnnBTc,53379
+snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=T2hy8Vvh8q8S2FMdwlr6gUHpasrY5hts1zqzR1ylJZU,52278
+snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=uKoQc6Wgt4vLDPHGngumU0mdOnuT9B2b7oKuR3M4Ews,54303
+snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=brIXqQWVAZOk71nLIV21GYoW9SnnmosGJRG477wa1RQ,53547
+snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=aBemY9GMjkvYlAahwx2eXaxy59WcHywUNFDvfkVN3z0,59089
+snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=6CbTrwPsrZ4Ln6H7PGd7_JFepir6lWshjKjTpw-GhV8,57700
+snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=SC00Sy4eHVXziKjoxgmK6yTNHYdZae1oPI1JIbPZd8k,60698
+snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=y9KHokK5HsfmQwoRYsNUcPQAcpGeAVYdLa9ydwmuy1w,60282
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=yjlgQA8MTp8lxwY_BnnLOZfGWK2zvZKUcz6nuGSdBUs,60348
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=lAFA9_pmShlfPaa-Pw8gKTUnrO2hcydhElUSprIxHgc,58670
+snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=RkFtwkAmIq5NAv-peoS4TuszU2MCW4wcTfyjaENyJn8,53324
+snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=831nscGhY0toqmPYdq5aSvdVdOqJ1DVcxcq7Fr4Ht8k,59044
+snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=KTZVh4R1zLdILlKQD3WPo4hFP2DINNgvuPQrgZGnoI8,57643
+snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=xjeZPZIN0AOcXSg14lQOrB60Ah8E24ODQJpcOF2aS5U,53231
+snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=W-8z2EwwmgqHNm72aZCO_qeiB4PTGt9e5ok5T_hGTFc,52806
+snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=bnwzchGtAvU4VzgwvePCd0luAQrUYqAC2DRVbQyNZ7c,51341
 snowflake/ml/modeling/feature_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=CpeKqY8m-KH3G7QttqeQEfuO3Xv-W1Qog48fug1TGIE,50303
-snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=S3qkzuT8vPnkbdnB_20VEa1r0bWLtLYa1_8i6Kl5l_I,50001
-snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=pmsUKVbfoHxyi_O7KUsLy34A8ugF5q4QP4OPUL5GlVQ,49995
-snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=le_joOpW3y6QshA7Dof1ygCJYgt-IiV3nUKN0G4nINg,50003
-snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=yZDhMBT_Ak2Pg3Au8dnQ7BDhXF_SE3VTLQG1AG1ISxo,50080
-snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=mygBmBRADxDv0BbVz8aaKMHvNSk84o_fa7beAX7viik,50100
-snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=H99sj8xzYGU9dSLQ2hsqanQ4Uxbqrcrkpr9LQg5JZo8,52753
-snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=QpfxYlln8JPaD0KSX69IOERpF7EXz2u19aHfXJbmQjg,49732
+snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=2Zpf6cTF5Q1Qxmp4VhDU-Bw1NhcApNbr2zUvXu1cfLI,50854
+snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=zit_0nZp5bPXaac18IWC-6_tI8791ayR-ENTH-fdHZA,50552
+snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=tzP0ae_1EFjaKG5AV4RzzfSJkKiiK7EA0fyro9GAb8o,50546
+snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=MZpzDF57EWhQYNUIHB6q22RbUNcKhhhDIs6SfNXcl64,50554
+snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=b4jGwVHFlYG1MBh-TKqVAvLv3Gjnb7mHeP2rAC7NvKI,50631
+snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=5CbzG9cHWj1IeGQGPWsomLXAY3IfO9q-IYJnLycVPs8,50651
+snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=YEzSgY5_WX96DBXNQAu_u4F_PjEysmw9HScHvkqlZaI,53304
+snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=4YDgM5XGpigBA0wOIdmkp1Qpd-i6gyQsjZs2tepIM14,50283
 snowflake/ml/modeling/framework/_utils.py,sha256=So72kQZXXP0U9D47rXx0U5mxbkkRujKwmCh-f2OVf3E,9110
 snowflake/ml/modeling/framework/base.py,sha256=hxRwBOKQtQFVZtHopgj_bgBUIU5TK9vJj4p4ZDynFWc,21900
 snowflake/ml/modeling/gaussian_process/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=kbdnvnfAoPKi41cDIL9k_hXU51fvqCcE0qJsaShf6JA,55298
-snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=zGJO9qVS06wUfxZxaT1-nQQKxnd0sgzouym8Dc4sx34,53990
+snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=1SCwt_cR_NSOl8K38GDHgbZZ9urFfOcKeW4Iq5qe5Go,55849
+snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=bPuQg6EA5zfEylQgC7jnMsWKozVzDiR5PlyeM1zywBI,54541
 snowflake/ml/modeling/impute/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/impute/iterative_imputer.py,sha256=4yipO_6LFf-B8pVZz2IK3GDy-_OIpipiz4MrT98_Zz0,55853
-snowflake/ml/modeling/impute/knn_imputer.py,sha256=uiO_2pndsh7dKlDgKwsocP1IeB0suIBVcBk20pUEvGU,52075
-snowflake/ml/modeling/impute/missing_indicator.py,sha256=cYsxmqai9MVM2vc2fvSZm1ePjVfkdGvZ3D3UT9cxU-c,50872
+snowflake/ml/modeling/impute/iterative_imputer.py,sha256=lEPVoxZaWd1NXp2WeiaWJduLeIVmzVK7WF3LdHGMpXI,56404
+snowflake/ml/modeling/impute/knn_imputer.py,sha256=QR_-MNV6v2XC7BMgNxICUMjgOVwLTq6kZdFIPeIFRnk,52626
+snowflake/ml/modeling/impute/missing_indicator.py,sha256=VwmisPgtu_uUA_5_5i1fbDVlXzhUxUcJSfJrygG9q0g,51423
 snowflake/ml/modeling/impute/simple_imputer.py,sha256=AuqGFxRvVEuIdhTNhmk6T0Uz5K-k1RCKCTnQFCNQxWA,18118
 snowflake/ml/modeling/kernel_approximation/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=vEhkR9AYDI5G3L9_cSL1doBJR2sxOtgRFDBX1Bq3BAI,49816
-snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=-AkVbSpFaqdfaVLnbJQioVBjTbcfEX9gP2KrZX1H0w8,51689
-snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=jPtz-zwTaoRoSWOKH3RbRiAGsq5Ab_y0WTUKv28MVac,50843
-snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=aa_qp-H6bWcUXrPvm-XIxZJYaUvQxpgWfxsDhJRuhHc,50272
-snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=I8wu5J9Lx94gO9RuGF_zBt2Jaxd1WPzVsepr9epR0SA,50271
+snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=VqNhJGP4xuqMmHOCLwAMivpcvGs_MBSavRGi84mXuYc,50367
+snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=wE8KrYZOBHDqFdZMVdxLj_B_PVEjrivZDxHrJr_Ow7Q,52240
+snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=iGlxvqAjZDgJ5_C8LGcip7Tp_9Ug1WAEr4a5KfjN-Po,51394
+snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=_5PE-_HRtbdi4Nd_cOu5vJIkZXVx5o4dMQDCXIoDpOo,50823
+snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=uZvRARvLG1NI_VPFMS25o_IboKxA8UtgMXaNZ8niSC8,50822
 snowflake/ml/modeling/kernel_ridge/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=palzG-S69KMN1bAcrj_u3kfgIKaZFhRZBxs3Z5HbVFk,51789
+snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=K5rwm_PEU_U8nFGfEeUIWx0q4z0sGC8P_D-3PZkXZZo,52340
 snowflake/ml/modeling/lightgbm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=GtmSfEQFYrGhmiFcFL6ny0jhabQ5fwhM6uOOM0pzAyw,51311
-snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=ESMXLdw0c4ZRzb2_fWRB3vcQpiUVHf4jKdTOAmHIeiU,50822
+snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=p8WVRLf-_VnwpqLxW6dnVrj-2wxPm43TwyYnpvsaS-o,51862
+snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=HgV0YR1OoahM9qdBIslU8JTV3c3811npxITla-3WVog,51373
 snowflake/ml/modeling/linear_model/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/linear_model/ard_regression.py,sha256=cCuh9k0VuPjr8veelH3z-ld4R3pW6zL-uCHddLs7VTA,51537
-snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=E2bNIDTRRYe600W-FnJRt9ZfF3XJHBdKDNgsvW5cIdM,51850
-snowflake/ml/modeling/linear_model/elastic_net.py,sha256=oH0BQ30iCXEenf9a79g3WOsnKfDBzU1KU_YC-3-ZRZM,52734
-snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=arHRxIjksdg2Vt58t6asSI50Rx-rvcTmGosDq642yeA,53992
-snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=OmRqJRNFL2Lg8sYg5LaFjEkXFGCjqEcxBsThMHxojes,51790
-snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=OMW3vODTBsKiGwQuQ9EQpcLAWX8cpBC6vvsGTJPPHcA,50978
-snowflake/ml/modeling/linear_model/lars.py,sha256=1a51Qlp_lwTA3mkPhtAfCBqVp4Ofe3hVasQZzJ8zJ04,52275
-snowflake/ml/modeling/linear_model/lars_cv.py,sha256=CWMO6-oYseqlD2tuogok3xnfNsKNpS9TZN5DMIWcYcc,52482
-snowflake/ml/modeling/linear_model/lasso.py,sha256=ul_y0WW_n-auKTxpnrl4XO9dX8slCKI72VfSrzPQpxs,52374
-snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=5BqgXEPYdfKPgql8jY6vxEO6j5NkJPWEmfuTR7hiAzE,53149
-snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=BNsAXRJemcfimA1xtmb0Ii5Tf4pHPmjVHCPMLz-wRdM,53378
-snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=0EIvy68z3YSemKwnSeuLu1L8T5M3TA0jCFRZ6Obvbq4,53324
-snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=7KPoskNoiLgGsWygBwQFvJpEMofKILFHi-mrH-nkWbo,52669
-snowflake/ml/modeling/linear_model/linear_regression.py,sha256=UYEJXKVoRaxVGcqHEMImZw1vOW6_CqNYUmhHmdFeQ-s,50504
-snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=2tlMiwnuEJJyJptJ7Qttc62BVshefNGTuDu1mbUbisA,56755
-snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=EN9fyy904SCu6CcQ6E0oY4X0bIl1SVIsf7N08FLjLUA,57775
-snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=4kROq8sjpTBXTDNXh8he-mrcW_I9G2vOnk9cZT7Qxwk,51960
-snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=Sc0akFkSIVPTlLwrogALpbZDFdUgSUDEn7LW3pWpO5Q,53588
-snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=zWSHdg-9nFy17DG2i9THRkN2qGe1p56P8W0HDdOv3Ns,51542
-snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=5kOlXjV-VleR9JF95ZPkKn9A-JdvsXJCzAl4aRFMr4Y,52794
-snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=jdJl4I0bvQH0MaFkOpelGBiHa2kzFSmpXhZ0Ef-8IK8,51069
-snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=y0e5ZFhrbFWuFvQoJr2boyWEMw8gNQyJYKpndSwVRMI,54420
-snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=EzLiGB3ssXfr6SncL-sds2cJVv33mpRgPFerWuXM6qE,53495
-snowflake/ml/modeling/linear_model/perceptron.py,sha256=4is4haTNnI4F95kHeGqo0lNkxvkhdVDdwasnZ6_gO70,53925
-snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=cqP3Zla7fGh0HTuKCC-R4x4w6yZHJTuOGGS-QM-oi1A,51821
-snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=U-SKY1IXuf5C4K9pDibcK2j24osPc5rfjHRuFz8Du00,55295
-snowflake/ml/modeling/linear_model/ridge.py,sha256=YnLDe4RU_2YyBZyOhvJ1iQ7VWC4EKELLyFOZxC34GTI,53355
-snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=re4lZV9cO6p3nrGK_q3hIUaWeHAFZST-p_46qZvqiXA,53673
-snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=86Me4jzRBSXgpTPrlYgFkbxn5FPsc-nuh7OeiWLPeWk,52212
-snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=n1m67C1-KeKEjmCB1GRiYuN3xejSlvJAWHtwSut0qXs,52988
-snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=1TS5Tvs8Nl-8ThXorV3zLhYgFWttA5UqDv3mmxFOhno,59341
-snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=x4o2cQGEvBG04AfYyyRw1YIfBK9zd7HovxAX5KXQBzY,53955
-snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=_dsQajzzs9hNK1gfiS1oa6nbPdUUzkff99LHWosMKsg,56810
-snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=BGTpGlLGYMq5DlA3CCW71xI7jJexUPFKGoyXxY00qN4,52243
-snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=Lh7HgW294gp2oqlndaZZdq0YMY3WwVVlmoOROJzH_CA,53214
+snowflake/ml/modeling/linear_model/ard_regression.py,sha256=dggAjhWDP60kbKiOAJojAQXtg3Wo9Dm0bYVtYrggnXk,52088
+snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=ibrYuablj-vfo82qLxX_CMye93HcI3N1CGxJXscLI2Y,52401
+snowflake/ml/modeling/linear_model/elastic_net.py,sha256=DYLFUV5om3RAGM9ksDQXUKsilxZtjm03a5fyW55VQqg,53285
+snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=lyioafMUyXvkYg7irIKNvNpM9O4EDnZaDZ-Ke7TNkDU,54543
+snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=GSfFfwvgD9_FnEm4kvSCeNqRz0opwS90cO8vFPkKz_A,52341
+snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=bcFYgj993IaUgcJLFhglTK2yI7z5b9fMu2d4H2Gd4ag,51529
+snowflake/ml/modeling/linear_model/lars.py,sha256=mtwe3OdWTqhHOfRrjfNABKz4UMUuDOJQ-dSlzF34q7Y,52826
+snowflake/ml/modeling/linear_model/lars_cv.py,sha256=SRwQj4xDW0iOzxeitBEcvN80LIhnpVfcCmb5DbMNLH0,53033
+snowflake/ml/modeling/linear_model/lasso.py,sha256=3iLGhBze1F7t-BfN5-Vylr9hkVDAaLac-_vuscKd2Ys,52925
+snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=bx1ujpMUGVpqKiXQCmdA9Zn8jOExB55yArAI6TCq82Y,53700
+snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=lVZrQFB1Aglg2knrP85nPsohPPy-KFNMoU9gf7ttnMw,53929
+snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=FRRH0rb-MeC2SCuzvbh8vNdVP9_LDxzv9ROW2D6Xu3U,53875
+snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=bpzkYt94SMLgdlQXc92ecGF-j2y2LjlyKcZYUMqsNX8,53220
+snowflake/ml/modeling/linear_model/linear_regression.py,sha256=fPJuWbciS2kVXduXxGcuEdyiV2_2QZaHpdp7EYijtg8,51055
+snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=zxGgjDlyEzMoffE4L-55CZXa7BB3wnRKg9thAdg2snQ,57306
+snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=nDyKdYdCRUQ_9hgVTvSPmy3ABUEaOgcuv4jbNhj-GNw,58326
+snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=CGOn8vgTrLABtCCyGbndbCNUVzub-tlup6OZ_Il22qc,52511
+snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=yow9Tmcfgl1uVBcDbJ44nbu2RKP0hWIrgW3fmYvvgF0,54139
+snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=onLqMztO07DHdhweQ0Lr3huLRLDblFzMjYH1h_679tI,52093
+snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=B-kNPTuK9OcNVsg39taYRJiHchzE4rzkJLYAQiCVfEQ,53345
+snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=iDBwlSFe3A3rTnYCoMhQtPDt1luK9oZ2kXQjeE8ZrXY,51620
+snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=ZVRUYCkTqnyxVh5I28Lxir-wbillRSZErEfuk0zCl38,54971
+snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=77JuwJf8YLQXhbMOkBozT1cZgpjklzxQ7LG11P3esA4,54046
+snowflake/ml/modeling/linear_model/perceptron.py,sha256=npzkWzJhgzBvl6YDCu3ouxRhcH8NM9mY3D6WmvTfS8E,54476
+snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=G_9YM6RWmGptMNvOsz1yaU_MoyjYLiPLyuWDmFOHnn0,52372
+snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=Eooggs-DyzKWikkF_-oJrkTJVqBGbcTnEGuIGQHMhPc,55846
+snowflake/ml/modeling/linear_model/ridge.py,sha256=JeTgVVpjBJdoaAFIoSMCc1dp1t0ylPCdKUZuwt21AIA,53906
+snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=RVvzPvAiuM0-4neplIU9O6zP-ORDF3NfMGilwbFprGc,54224
+snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=H_kE-ZRnaqhTVbIYdqSX2MB-K4tGStNjll-FuruM9Sc,52763
+snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=IgH-7EtxjFfTI7Wih80NIk9EdZTug5nWvTQUBxe4XCY,53539
+snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=Fx68BRRQ7nXhEi7UCd1osFvtebWs0ZsY90wI9G2jKI0,59892
+snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=URNcQGuY6xEazEbL5SxOLE-msOVHR1HwAHvu82ylJQk,54506
+snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=DJqvXt6gYgvlWn4i2ilrlT7N4GPdEiWbprOhOH6T3fo,57361
+snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=jSoeUvjm-Iih83byRAP6Non9_d7HiiOgSi-fjuD8lRI,52794
+snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=FH1yc3D1_dZ-NeVXtoDhQZDL9fC47nz9Qt9eDzqT4ok,53765
 snowflake/ml/modeling/manifold/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/manifold/isomap.py,sha256=QfApbKZorEoMc9-D69_NM9DdGfSeJFm4Y3oro7-Ikao,52609
-snowflake/ml/modeling/manifold/mds.py,sha256=vLf1MKWbZTQk2dJiy8u_F2Iv017Yc_GapogfHWjqT8g,51827
-snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=YZkNEATa94lPBstItdrNeD5grwiAqW1g3_B0Lq3qsEw,52598
-snowflake/ml/modeling/manifold/tsne.py,sha256=lD3MMOxNxMQdjiYZY3j19CKKlSbr-6Bszkv6hA-w5ZY,55870
+snowflake/ml/modeling/manifold/isomap.py,sha256=Y9I0ZBAtgF2wdChAday86N4FqSnACAlVBZ8zUuy55XM,53160
+snowflake/ml/modeling/manifold/mds.py,sha256=8PxHji0zRLZEzKm6crf7zhPe0lHy0PPx6b4a4wVsBFU,52378
+snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=aJSl5NC_ygakrOhsT_-dlKxd2ELuOLASJ2suwaf0kdU,53149
+snowflake/ml/modeling/manifold/tsne.py,sha256=gwEXvNWqzPxBLX5ID9BVHJylNE35nMmRlM8a1Q4iToE,56421
 snowflake/ml/modeling/metrics/__init__.py,sha256=wp2LehkoLtyt4u_HBhglrKrV6E-dKt5vr-0N3MkJFaY,304
-snowflake/ml/modeling/metrics/classification.py,sha256=JPeFqd1B1D0LikIlJIRIxu4Oq1ezXD7pdnoYslwjHnI,39902
+snowflake/ml/modeling/metrics/classification.py,sha256=ZtTQ3ziMMglimNW1hG7oGDhAW5a6HBXOfQq8g3iptC8,40077
 snowflake/ml/modeling/metrics/correlation.py,sha256=4cjKDl07C3PGcx_VPwOqSFYjuBEA266btKuw9wd5D7w,4921
 snowflake/ml/modeling/metrics/covariance.py,sha256=hS_yILgo3OUjBVrPCL-NXR7cSyPjXOFftXlZJ1xaLus,4757
 snowflake/ml/modeling/metrics/metrics_utils.py,sha256=jvjOabIwGi02I1aEiSo_3NfgXLAIU7ggShQXDAAjCFs,12037
 snowflake/ml/modeling/metrics/ranking.py,sha256=KzRbI1bZf3G1U3wlSnvpX1GMTkddfGwy9y2gopxoW6E,15397
-snowflake/ml/modeling/metrics/regression.py,sha256=jDSH6vqD5FqqaVpKBWK3ljMUljXtc1pXD6-Fs_zBQww,1880
+snowflake/ml/modeling/metrics/regression.py,sha256=yqTiBnbFc1GtBR4LJfUiEGE8Pv3uNT2ZuFiaEyzxyhM,23144
 snowflake/ml/modeling/mixture/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=2_dQsMOQ6j94M6ZhHi7KyP-gsR2Ot2oBlf7Y52_jHas,56514
-snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=odZ9SUdNwLj6E0eZrn2m9611eCjYFZVZUCCLVxsCa4A,54516
+snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=Q9-C2JHJtOFuI70qqMe4jXpL3xqZakD90d9gsdTMDzE,57065
+snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=LkqMA3cxhIoHnbJeYbcECNUIbuxTLZnfkFQ_08IaQoY,55067
 snowflake/ml/modeling/model_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=xfE50IO0dqVc1IkZiynh8xbj18vGaHhgMipKqgNJeZ0,57065
-snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=n86LvX_HjE4B8N3AmqWGs707JGDnG5ecHbMdBultlPY,57909
+snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=1LuLtTFpd0Qeuv91xsMvY3uO2Dj8ePEYH5NjWc4id-M,57616
+snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=iLacSg4kt-NyJE4pRFX4F5nW8iZlRieXQhsgcrmUAjs,58460
 snowflake/ml/modeling/multiclass/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=m864OqcqOo--S5cbUQn9y72xe38XjbfAgXO_MB1g3lo,50497
-snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=ZIBYOb8tcifpwFvwUy3J1eM6XOPeRs8pXWs83gWACsw,51425
-snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=Vlq7-d77BOB8I_PwQzllHhiP8yp7eUp6OZg-pVaZ5DQ,50755
+snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=m49bTRLKzJHPj3_4hmPEdWPeHk_JADgnmdxTJz2t4Gs,51048
+snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=c8uxnEPim_ez3Fe3pAMqdy9sHUJGBpm4iPvZS7yi2xQ,51976
+snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=jUc8xYU1rW3T21IJDITK8Ju04ZA702CyMEb7Yhj5MtY,51306
 snowflake/ml/modeling/naive_bayes/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=1Fyb_kiTw-s3ThrLNwwp4X6KKdWNXBok9vNlu9_uQ-I,51082
-snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=iWgawrka8FMyw8F7DewQefbh99Okr9ndk9-m_bwix1Y,51403
-snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=xre1yFs4ZnU1coeGK_bSGXvDdgTPUvZbhECdwsPAgIM,51090
-snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=CQORxYivsdOvmWJxu_owMBGpNiMookneMVSMp5-8zik,50230
-snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=H4b_EtLTaC5v9bPh0QQG2Z_7AI1m6hW0cVesEOj3buk,50847
+snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=r-jo5zsBrHEJTJF_yqxVMTkcJ_UyHt03Vl5Jg9pdAwY,51633
+snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=rbzfzn1CbhWP0NUb_HuuJPqDXoGOh0q-U6pv1bQXJTQ,51954
+snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=sKFvVuVtZ5YkM9sn9hhSiJYPhu1XgfIpFlWCUZL6keo,51641
+snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=IJ_cLm2e1j9fIIJEuEqs73ErG5bTQadTyoXRPQdLSiA,50781
+snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=nw33nrEnVLho0AnOaP2X2wxQ9Sf7OFgzec1KIY153do,51398
 snowflake/ml/modeling/neighbors/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=ThFxBWquZeEYkWEuTySqWKHcD0Ayi9JMNFfSgMv2pPY,53634
-snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=rTZojmIiz0OA6qH4CYWHw5R5tkEY1qcRXdNA8gvDKMc,53116
-snowflake/ml/modeling/neighbors/kernel_density.py,sha256=ylRMdgAYn4QfVTZydGRsoIWLEOpu3jo68K9QvVtS024,51593
-snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=-BsaDjDDYkagMaCBLjiCFAbwU7_DLaH9MNgstE19ZUU,53874
-snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=dl4hqd_aU7E0lyQ12SMAesdOZlABgwbJEcT0NM_cvws,50397
-snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=1UrxBSafivNGN4mPkLMxPi_eGaHGYXezr0W4t3sDTMk,52306
-snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=-nq9fOxZko93m_6Ajk56qPkxgNVdEH__8bgpAN3Wa84,53782
-snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=ml2k9gMCfRbuZKnGhQuV_ZeWCjtRxwz0pkauvMB1CCM,54263
-snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=H8IjYbVS4ElBKr-7w4aL_tkvwTEwMYewfJpGYCkNNLI,53149
+snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=7slt_NxYYctTAtwfieDxAYnYl9ckZs4Q1UOizvdEBaE,54185
+snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=17ofYkVFAIgbvXGMpNDg7ygvdBMXtxIDNp6QeJygdTI,53667
+snowflake/ml/modeling/neighbors/kernel_density.py,sha256=g1reFsR1aWBrsbSCzwGzifS4rCqKJY3H5MqRbnqjp9s,52144
+snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=DhTrzk-WnYYXWCov8H-Bn3g8D-FoxvFkH7O_XbHHOwA,54425
+snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=0PAR1wLl8qwFTT4x44FLqOA5pj1x4twKe1ca0wkMkGQ,50948
+snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=qTNrjb-vPzYQDs-CDhGlRPLe_bYhrJ8EfQEc2RaT96k,52857
+snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=b5aaw6oF-754dVsGkizkJHgtZCetKuDxb118ke8KzQQ,54333
+snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=j5mHbaSuJqLVfR6vdNmGWXut1UNAh5IxSgiO4HgVvGY,54814
+snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=GOxeGcAVyAWCRgxJvRO25guzDin5-tryWV2vuABel0U,53700
 snowflake/ml/modeling/neural_network/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=bUiDQEw26KsXLZuchtFbzxh490wiwn2J01V-JUOAlfU,50798
-snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=DvgZ2YPerZtjAQh3ypq9ecrkUem7Ye9Lt7-O5s_JyWk,58296
-snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=VAmyYFWkPZHJP_4dHbZuA-iNWQLSiDPJTdvZk_gJ-Z8,57573
+snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=joVaKo1byI-5MRmb0MarM7JBFn6Hg4jyqd37nu06Zhg,51349
+snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=ToJzxQly6m2g_WNHxXq0jJdM6tupA9w4m-L0Gvf6GW0,58847
+snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=Xi_YG8Ce0vVkIsHN1Zk82txQKDG-iHYFi-D2d_20J-U,58124
 snowflake/ml/modeling/pipeline/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/pipeline/pipeline.py,sha256=kVfKdC7zSmuCorPppIsS20yyH-fghYOUsHXjzgvbEaE,22173
+snowflake/ml/modeling/pipeline/pipeline.py,sha256=kIvKahAyF7zQoT8eYVm9dJPafYLybGZ8ELaxrBIkQ34,23381
 snowflake/ml/modeling/preprocessing/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/preprocessing/binarizer.py,sha256=IoGdiZwqsLYRSkifmxzfCqCeOy5ir5Gq_ls_gsPu54I,6092
 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=upW9qxntwE0vZ8foc2J3BlVdKy61M7JBspZkKqAyKW0,20422
 snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=r3S_-G5OIqjeBttyIicSar_4FNO68MOvRSyAi_6gzeA,6285
 snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=O2dXkX6PPJZaVbS7jIpC4DOfqUt85YFaDA-rLXz6pEc,8491
 snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=1LDaOp-OJU-79B36ZxBhAMQe5AXDEU5f71PNVXwtLXU,10716
 snowflake/ml/modeling/preprocessing/normalizer.py,sha256=0pbgiOGqwC4Pv9MKnYfo_0vIUmBdyLFoPSd_Sr7Og4U,5951
-snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=xEhHia5Gqr4RGq0WAAmrU8ci0vY1xNeqOEkV0z9vnBo,67083
+snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=ubZCjUhPdkqn_w4nuIpgozawjcV3HvnkqiKMYqo3ljA,66998
 snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=uryEQmMp45tHuuHI7k-D4CY9JCkFYJUuP6hWZcODoAQ,27848
-snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=HfCRSatprXHA0b2AA2BZa5orXJnvix8uvQGyR3gUmyM,50932
+snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=aD_FCZlwUnGtAM5eexF8eGUqcOw6pC9TrSv_qadJ-n8,51483
 snowflake/ml/modeling/preprocessing/robust_scaler.py,sha256=JGgkPZfgezS4X8YECSjeWDQIoLbU98j43qbwqP2RzZE,11981
 snowflake/ml/modeling/preprocessing/standard_scaler.py,sha256=hu2VnATyizCz-QKv7aaGdATeU8Fyug8MeNxau3-CllQ,10672
 snowflake/ml/modeling/semi_supervised/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=I5ggE5jcbywFT_eV2aGKCXP66Gk7pAuDp0-RK1DLTx0,51269
-snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=hd-HWf4B5oGCMMwso3NZK5wGmWC6y8XltfAvw4MOT_g,51633
+snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=06dQseRmK25t9TjC06hHtG0Yx-w1QAgdg-iqofaR6FA,51820
+snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=4Y8eBB-lX8Jqo6aHSCDiAr6I392OtVknE0dis3qEtFQ,52184
 snowflake/ml/modeling/svm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/svm/linear_svc.py,sha256=UAwHLZhZ21Trzv784V8CoRLsGN7tmX2KxGFBYnlHjMU,53811
-snowflake/ml/modeling/svm/linear_svr.py,sha256=KoFme6nQVa_ttso4EScQdEqe4llk65F80--r2oD1zn0,52226
-snowflake/ml/modeling/svm/nu_svc.py,sha256=5FlnNUOPqwVPQbMQDWBqCyMVipBft3oWhlhfs5E0XjE,54525
-snowflake/ml/modeling/svm/nu_svr.py,sha256=QWwfmZYXWiwrCVZ9J2hBALUtn8LAM-cVw9OKsMPtHc0,51601
-snowflake/ml/modeling/svm/svc.py,sha256=S-9k38YwDYFXnwrQPZkGMGFK11N7qVwpI_E6bP1RwKc,54688
-snowflake/ml/modeling/svm/svr.py,sha256=S3b7yWZG7c-REbbAt-HDHlxCEDqf3J3gSdFWYY3mJwA,51804
+snowflake/ml/modeling/svm/linear_svc.py,sha256=LLj2LrHZjcFf4A8V0MYLrQz9iWRkxE58XKoaSwMzyA0,54362
+snowflake/ml/modeling/svm/linear_svr.py,sha256=9YPXR-D_nsv8FJaZBuUACgqofeHuaQCJVLl4gKbH6V8,52777
+snowflake/ml/modeling/svm/nu_svc.py,sha256=2IlDpoENw5Q6OtKC7lK2ZLrHwVZBoSYfuF2eal18HbE,55076
+snowflake/ml/modeling/svm/nu_svr.py,sha256=zLfLSZATC_IikRI5lh3ySu_o0Rq5NmJCUeEB_0TcY-I,52152
+snowflake/ml/modeling/svm/svc.py,sha256=0ptSWOd54T5A8tO_3X8cIpthu-cDWELcZlGh4fiAVYI,55239
+snowflake/ml/modeling/svm/svr.py,sha256=XDymWoig3szXbPIiYyCs0sM-Q7lcCcZaD6f9-CE4lrg,52355
 snowflake/ml/modeling/tree/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=QjVTJ5M1zDyRc6hLfwS4gG2RnhQFdUTrpDNVeEXzR1Y,56887
-snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=AmtFiREgYRafCuma2w2hAPIOQLFQeBQErhRtvZ6mqGU,55583
-snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=QY5DWzpHVvuMzU4nXchPSzxVCxVbFNsAv8oW5-hR2vQ,56250
-snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=o7LKwLdVx_qLL_5PiPj4ODoji3GOVWxZfZsYyMAlOIw,54955
+snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=Rej8fjFzDE4osX0waJdHjsw0DZ3Z6yg3VcfmdsA-V4Y,57438
+snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=kylQASQOkEtMWgGaTMO6vUqTImgdBhTEC5EbfZYl5i4,56134
+snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=aWV05GlwO653ndAmn3AtBmQA1R3KeVFjL3l2a8L2srQ,56801
+snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=zTo1vpjNvj65GiWCOPCv7iH93Jj8p66QPUZ0CMP0qN8,55506
 snowflake/ml/modeling/xgboost/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=ABpZxBV5odwIaLuPcd9DbqIoqIXy37NKaHFGnyAI0zM,60657
-snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=tlIeNuUth21wXyJlIhqhOm1h8SWHIIYo3YzEAYSIbhg,60163
-snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=KTTL52LCK9TW4dKDWbYdAKbMOTry0kJGfzutWsZPDHY,60821
-snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=spFAmtcP9PMG6tLQPvQuquEMQ1FJvwKLgPgQFEjx7Ls,60354
+snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=KqK_zKRnWoTWqW82Un7CYGEgbHsloRwwOUbvhOk9SJQ,61208
+snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=4BrG1tjYvMk3wX3aK47PDiTmeMrCSB60Ke4BWud3vuk,60714
+snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=BQel82uG1f3o1XoODm6TIMQD3zRYSuXoD2G6oGSlX_A,61372
+snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=3EYUqQgFSFkiWRxmIYAw-Cav2CJl2pEejChVqAGgLDY,60905
 snowflake/ml/registry/_schema.py,sha256=7NezDozAqdbOjB9dYHSQQpxapSTKuXqnGrl394bDohc,1381
-snowflake/ml/registry/model_registry.py,sha256=hnOc0oNnxm9ODbcdzYNSNYfOq-gToeXEY_JkZ6573J8,85339
+snowflake/ml/registry/model_registry.py,sha256=jkeGWntSSKYRQZqoIX3_qAbAveMwoJhYzF_jgug3bxA,84697
 snowflake/ml/utils/connection_params.py,sha256=W_MwEw1xUARgrDehP_Kz5dmqt1sBXct80xQ7N56qFCc,6138
 snowflake/ml/utils/sparse.py,sha256=1mI2lOm-nMQEwNfbDtHpkJ4SDkKKqsRFyGwSQJJZAiE,3893
-snowflake/ml/version.py,sha256=tevytQN3Q9r0hFbXhoTW1fyqa59CqJHog4ENpyz0XDc,16
-snowflake_ml_python-1.0.1.dist-info/METADATA,sha256=RwVimIZaEu28C-UaEo68GU9azg5QXAXsPi6nrU_Bx-k,10425
-snowflake_ml_python-1.0.1.dist-info/RECORD,,
-snowflake_ml_python-1.0.1.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
+snowflake/ml/version.py,sha256=_MuTm0ZX7Fno8rwBkHygvwK4Mr2oy6nYbO-KOyECohQ,16
+snowflake_ml_python-1.0.2.dist-info/METADATA,sha256=5k_a2EQPGGkZZNUpSFLXSJ_WCEddktODr-aYUhz6gCc,11756
+snowflake_ml_python-1.0.2.dist-info/RECORD,,
+snowflake_ml_python-1.0.2.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
```

