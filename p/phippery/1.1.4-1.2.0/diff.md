# Comparing `tmp/phippery-1.1.4-py3-none-any.whl.zip` & `tmp/phippery-1.2.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,18 @@
-Zip file size: 34666 bytes, number of entries: 17
--rw-r--r--  2.0 unx      133 b- defN 22-Oct-28 22:34 phippery/__init__.py
--rw-r--r--  2.0 unx    11952 b- defN 22-Oct-28 22:34 phippery/cli.py
--rw-r--r--  2.0 unx     3355 b- defN 22-Oct-28 22:34 phippery/eigen.py
--rw-r--r--  2.0 unx     8810 b- defN 22-Oct-28 22:34 phippery/escprof.py
--rw-r--r--  2.0 unx     3294 b- defN 22-Oct-28 22:34 phippery/gampois.py
--rw-r--r--  2.0 unx    10511 b- defN 22-Oct-28 22:34 phippery/modeling.py
--rw-r--r--  2.0 unx     3506 b- defN 22-Oct-28 22:34 phippery/negbinom.py
--rw-r--r--  2.0 unx    32503 b- defN 22-Oct-28 22:34 phippery/normalize.py
--rw-r--r--  2.0 unx     5037 b- defN 22-Oct-28 22:34 phippery/string.py
--rw-r--r--  2.0 unx    28960 b- defN 22-Oct-28 22:34 phippery/utils.py
--rw-r--r--  2.0 unx     4782 b- defN 22-Oct-28 22:34 phippery/zscore.py
--rw-r--r--  2.0 unx     1095 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     2315 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       46 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1320 b- defN 22-Oct-28 22:35 phippery-1.1.4.dist-info/RECORD
-17 files, 117720 bytes uncompressed, 32534 bytes compressed:  72.4%
+Zip file size: 32351 bytes, number of entries: 16
+-rw-r--r--  2.0 unx      166 b- defN 23-Jun-23 18:01 phippery/__init__.py
+-rw-r--r--  2.0 unx    11952 b- defN 23-Jun-23 18:01 phippery/cli.py
+-rw-r--r--  2.0 unx     3355 b- defN 23-Jun-23 18:01 phippery/eigen.py
+-rw-r--r--  2.0 unx     8810 b- defN 23-Jun-23 18:01 phippery/escprof.py
+-rw-r--r--  2.0 unx     3294 b- defN 23-Jun-23 18:01 phippery/gampois.py
+-rw-r--r--  2.0 unx     6626 b- defN 23-Jun-23 18:01 phippery/modeling.py
+-rw-r--r--  2.0 unx    32628 b- defN 23-Jun-23 18:01 phippery/normalize.py
+-rw-r--r--  2.0 unx     5037 b- defN 23-Jun-23 18:01 phippery/string.py
+-rw-r--r--  2.0 unx    28957 b- defN 23-Jun-23 18:01 phippery/utils.py
+-rw-r--r--  2.0 unx     4782 b- defN 23-Jun-23 18:01 phippery/zscore.py
+-rw-r--r--  2.0 unx     1095 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2315 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       46 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        9 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1242 b- defN 23-Jun-23 18:01 phippery-1.2.0.dist-info/RECORD
+16 files, 110406 bytes uncompressed, 30335 bytes compressed:  72.5%
```

## zipnote {}

```diff
@@ -12,41 +12,38 @@
 
 Filename: phippery/gampois.py
 Comment: 
 
 Filename: phippery/modeling.py
 Comment: 
 
-Filename: phippery/negbinom.py
-Comment: 
-
 Filename: phippery/normalize.py
 Comment: 
 
 Filename: phippery/string.py
 Comment: 
 
 Filename: phippery/utils.py
 Comment: 
 
 Filename: phippery/zscore.py
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/LICENSE
+Filename: phippery-1.2.0.dist-info/LICENSE
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/METADATA
+Filename: phippery-1.2.0.dist-info/METADATA
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/WHEEL
+Filename: phippery-1.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/entry_points.txt
+Filename: phippery-1.2.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/top_level.txt
+Filename: phippery-1.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: phippery-1.1.4.dist-info/RECORD
+Filename: phippery-1.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## phippery/__init__.py

```diff
@@ -1,8 +1,11 @@
 # __init__.py
 
 
-__version__ = "1.1.4"
+__version__ = "1.2.0"
 
 from phippery.utils import *
-from phippery.normalize import *
-from phippery.modeling import *
+import phippery.normalize
+import phippery.modeling
+import phippery.eigen
+import phippery.escprof
+
```

## phippery/modeling.py

```diff
@@ -12,46 +12,44 @@
 import pandas as pd
 import itertools
 import copy
 import scipy.stats as st
 from phippery.gampois import fit_gamma
 from phippery.gampois import gamma_poisson_posterior_rates
 from phippery.gampois import mlxp_gamma_poisson
-from phippery.negbinom import fit_neg_binom
-from phippery.negbinom import mlxp_neg_binom
 from phippery.zscore import zscore_pids_binning
 from phippery.zscore import compute_zscore
 
 
 def gamma_poisson_model(
     ds,
     starting_alpha=0.8,
     starting_beta=0.1,
     trim_percentile=99.9,
     data_table="size_factors",
     inplace=True,
     new_table_name="gamma_poisson_mlxp",
 ):
-    """Fit a Gamma distribution to determine Poisson rates
+    r"""Fit a Gamma distribution to determine Poisson rates
     per peptide for the non-specific binding background and estimate the
     :math:`-\log_{10}(p)` value, or *mlxp*,
     for each sample-peptide enrichment in the dataset provided.
     We use the following parameterization of the Gamma distribution:
 
     .. math::
-        f(x) = \\frac{\\beta^\\alpha}{\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}
+        f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}
 
     The fit is performed on the distribution of peptide average counts to
-    obtain :math:`\\alpha` and :math:`\\beta`. If there are :math:`n`
+    obtain :math:`\alpha` and :math:`\beta`. If there are :math:`n`
     samples involved in the fit, and for a given peptide with counts
     :math:`x_1, x_2, \ldots, x_n`, the background Poisson distribution
     is determined by the rate,
 
     .. math::
-        \lambda = \\frac{\\alpha + \sum^n_{k=1} x_k}{\\beta + n}
+        \lambda = \frac{\alpha + \sum^n_{k=1} x_k}{\beta + n}
 
     Note
     ----
     much of this source code is derived from
     https://github.com/lasersonlab/phip-stat
     and written by Uri Laserson.
 
@@ -117,134 +115,32 @@
         return (alpha, beta)
     else:
         ds_copy = copy.deepcopy(ds)
         ds_copy[new_table_name] = xr.DataArray(counts)
         return (alpha, beta), ds_copy
 
 
-def neg_binom_model(
-    ds,
-    beads_ds,
-    nb_p=2,
-    trim_percentile=100.0,
-    outlier_reject_scale=10.0,
-    data_table="size_factors",
-    inplace=True,
-    new_table_name="neg_binom_mlxp",
-):
-    r"""Fit a negative binomial distribution per peptide and estimate the
-    :math:`-\log_{10}(p)` value, or *mlxp*,
-    for each sample-peptide enrichment in the dataset provided.
-
-    The fit is performed with statsmodels but the mlxp evaluation is done
-    with SciPy. The function returns values corresponding to the SciPy
-    parameterization with 'size' (or number of successes), :math:`n`
-    and 'probability' (of a single success), :math:`p`,
-
-    .. math::
-        f(k) = \binom{k + n - 1}{n - 1}p^n(1-p)^k
-
-    Parameters
-    ----------
-    ds : xarray.DataSet
-        The dataset containing samples to estimate significance on.
-
-    beads_ds : xarray.DataSet
-        The dataset containing beads only control samples
-        for which the distribution will be fit.
-
-    nb_p : int
-        The negative binomial type parameter (either 1 or 2), which determines the
-        relationship between variance and mean 
-        (see `statsmodels documentation <https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.NegativeBinomial.html>`_ for details)
-
-    outlier_reject_scale : float
-        Extreme outliers are defined as being more than a multiple of interquartile range (IQR)
-        above the 75th percentile. (i.e. for outlier_reject_scale = 10, extreme outliers are
-        those lying at least 10*IQR above the 75th percentile). Extreme outliers are removed
-        from the fit.
-
-    data_table : str
-        The name of the enrichment layer you would like to fit mlxp to.
-
-    new_table_name : str
-        The name of the new layer you would like to append to the dataset.
-
-    inplace : bool
-        If True, then this function
-        appends a dataArray to ds which is indexed with the same coordinate dimensions as
-        'data_table'. If False, a copy of ds is returned with the appended dataArray
-
-    Returns
-    -------
-    tuple :
-        The size and probability parameters of the fitted negative binomial distribution.
-        If inplace is false, a copy of the new dataset is returned first.
-    """
-    #'nb_p' determines the relationship between mean and variance. Valid values
-    # are 1 and 2 (sometimes called Type-1 and Type-2 Negative Binominal, respectively)
-    # : 
-    # If 'inplace' parameter is True, then this function
-    # appends a dataArray to ds which is indexed with the same coordinate dimensions as
-    #'data_table'. If False, a copy of ds is returned with the appended dataArray
-
-    if data_table not in ds:
-        raise KeyError(f"{data_table} is not included in dataset.")
-
-    beads_counts = copy.deepcopy(beads_ds[f"{data_table}"].to_pandas())
-    upper_bound = st.scoreatpercentile(beads_counts.values, trim_percentile)
-    trimmed_data = np.ma.masked_greater(beads_counts.values, upper_bound)
-
-    nb_mu = []
-    nb_alpha = []
-    nb_var = []
-    nb_size = []
-    nb_prob = []
-
-    for i in range(beads_counts.shape[0]):
-        (mu, alpha, var, size, prob) = fit_neg_binom(
-            trimmed_data[i].compressed(), nb_p, outlier_reject_scale
-        )
-        nb_mu.append(mu)
-        nb_alpha.append(alpha)
-        nb_var.append(var)
-        nb_size.append(size)
-        nb_prob.append(prob)
-
-    counts = copy.deepcopy(ds[f"{data_table}"].to_pandas())
-    counts = counts.round(2)
-    counts.loc[:, :] = mlxp_neg_binom(counts, nb_size, nb_prob)
-
-    if inplace:
-        ds[new_table_name] = xr.DataArray(counts)
-        return (nb_size, nb_prob)
-    else:
-        ds_copy = copy.deepcopy(ds)
-        ds_copy[new_table_name] = xr.DataArray(counts)
-        return (nb_size, nb_prob), ds_copy
-
-
 def zscore(
     ds,
     beads_ds,
     data_table="cpm",
     min_Npeptides_per_bin=300,
     lower_quantile_limit=0.05,
     upper_quantile_limit=0.95,
     inplace=True,
     new_table_name="zscore",
 ):
-    """Calculate a Z-score of empirical enrichment relative to
+    r"""Calculate a Z-score of empirical enrichment relative to
     expected background mean CPM (:math:`\mu`) and stddev CPM (:math:`\sigma`)
     from beads-only samples,
     for each sample-peptide enrichment in the dataset provided.
     For a peptide with CPM :math:`n`, the Z-score is,
 
     .. math::
-        z = \\frac{n - \mu}{\sigma}
+        z = \frac{n - \mu}{\sigma}
 
     Note
     ----
     This implementation follows the method described in the
     supplement to DOI:10.1126/science.aay6485.
 
     Parameters
```

## phippery/normalize.py

```diff
@@ -174,15 +174,15 @@
         1, ds_lib_counts_mean_sum / sum(ds_bead_counts_mean)
     )
     pseudo_sample_freq = pseudo_sample / sum(pseudo_sample)
     pseudo_lib_counts_freq = pseudo_lib_counts / sum(pseudo_lib_counts)
     pseudo_bead_enrichment = pseudo_sample_freq / pseudo_lib_counts_freq
 
     # compute all sample standardized enrichment
-    for sample_id, sample in counts.iteritems():
+    for sample_id, sample in counts.items():
         pseudo_sample = sample + max(1, sum(sample) / ds_lib_counts_mean_sum)
         pseudo_lib_counts = ds_lib_counts_mean + max(
             1, ds_lib_counts_mean_sum / sum(sample)
         )
         pseudo_sample_freq = pseudo_sample / sum(pseudo_sample)
         pseudo_lib_counts_freq = pseudo_lib_counts / sum(pseudo_lib_counts)
         sample_enrichment = pseudo_sample_freq / pseudo_lib_counts_freq
@@ -262,15 +262,15 @@
     enrichments = copy.deepcopy(counts)
 
     # find controls and average all
     lib_counts_mean = lib_counts.mean(axis=1)
     lib_counts_mean_sum = sum(lib_counts_mean)
 
     # compute all sample standardized enrichment
-    for sample_id, sample in enrichments.iteritems():
+    for sample_id, sample in enrichments.items():
 
         pseudo_sample = sample + max(1, sum(sample) / lib_counts_mean_sum)
         pseudo_lib_control = lib_counts_mean + max(1, lib_counts_mean_sum / sum(sample))
         pseudo_sample_freq = pseudo_sample / sum(pseudo_sample)
         pseudo_lib_control_freq = pseudo_lib_control / sum(pseudo_lib_control)
         sample_enrichment = pseudo_sample_freq / pseudo_lib_control_freq
         enrichments.loc[:, sample_id] = sample_enrichment
@@ -709,14 +709,19 @@
         )
     diff_sel = np.array([np.log2(v / base) for v in all_other_values])
     return diff_sel * scalar
 
 
 def size_factors(ds, inplace=True, data_table="counts", new_table_name="size_factors"):
     r"""
+    Warning
+    -------
+    This method is deprecated. It is currently maintained only for reproducibility of previous results.
+   
+
     Compute size factors from 
     `Anders and Huber 2010 
     <https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-10-r106>`_.
 
     Concretely, given a Matrix of enrichments, :math:`X_{i,j}`, in the
     `phippery` dataset with shape (peptides, samples). Consider a *pseudo-reference sample*
     where the count of each peptide is the geometric mean of counts for that peptide across
```

## phippery/utils.py

```diff
@@ -1,8 +1,8 @@
-"""
+r"""
 =================
 Utils
 =================
 
 Utilities for building, indexing, and manipulating
 and xarray dataset topology 
 specific to most **phippery** functions provided in this package
@@ -708,15 +708,15 @@
     You could achieve the same functionality by listing features you know to be
     homogeneous in the 'by' parameter.
     """
 
     # Find out which collapse features are shared within groups
     collapsed_sample_metadata = defaultdict(list)
     for i, (group, group_df) in enumerate(df.groupby(by)):
-        for column, value in group_df.iteritems():
+        for column, value in group_df.items():
             v = value.values
             if np.all(v == v[0]) or np.all([n != n for n in v]):
                 collapsed_sample_metadata[column].append(v[0])
 
     # Throw out features that are not shared between groups
     to_throw = [
         key for key, value in collapsed_sample_metadata.items() if len(value) < i + 1
```

## Comparing `phippery-1.1.4.dist-info/LICENSE` & `phippery-1.2.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `phippery-1.1.4.dist-info/METADATA` & `phippery-1.2.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: phippery
-Version: 1.1.4
+Version: 1.2.0
 Summary: Tools for analyzing PhIP-Seq Data.
 Author-email: Jared Galloway <jgallowa@fredhutch.org>, Kevin Sung <ksung@fredhutch.org>
 Project-URL: documentation, https://matsengrp.github.io/phippery/
 Project-URL: repository, https://github.com/matsengrp/phippery
 Keywords: phippery,phipseq,phage,modeling,phip
 Classifier: License :: OSI Approved :: GNU Affero General Public License v3
 Classifier: Programming Language :: Python
```

